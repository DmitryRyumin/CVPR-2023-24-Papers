[
  {
    "title": "Pix2map: Cross-Modal Retrieval for Inferring Street Maps From Images",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": "https://pix2map.github.io/",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Pix2map_Cross-Modal_Retrieval_for_Inferring_Street_Maps_From_Images_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2301.04224",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "18VtggvpynY",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Audio-Visual Grouping Network for Sound Localization From Mixtures",
    "base_url": null,
    "title_page": null,
    "repo": "stoneMo/AVGN",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Mo_Audio-Visual_Grouping_Network_for_Sound_Localization_From_Mixtures_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.17056",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "xMA2vOlpaHY",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Learning Semantic Relationship Among Instances for Image-Text Matching",
    "base_url": null,
    "title_page": null,
    "repo": "CrossmodalGroup/HREM",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Fu_Learning_Semantic_Relationship_Among_Instances_for_Image-Text_Matching_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Identity-Preserving Talking Face Generation With Landmark and Appearance Priors",
    "base_url": null,
    "title_page": null,
    "repo": "Weizhi-Zhong/IP_LAP",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Zhong_Identity-Preserving_Talking_Face_Generation_With_Landmark_and_Appearance_Priors_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2305.08293",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "jpap8rLXh94",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "ImageBind: One Embedding Space To Bind Them All <br/> <img alt=\"CVPR - Highlight\" src=\"https://img.shields.io/badge/CVPR-Highlight-FFFF00\"/>",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": "https://facebookresearch.github.io/ImageBind",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2305.05665",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "X5JCEzhdkW4",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Learning To Dub Movies via Hierarchical Prosody Models",
    "base_url": null,
    "title_page": null,
    "repo": "GalaxyCong/HPMDubbing",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Cong_Learning_To_Dub_Movies_via_Hierarchical_Prosody_Models_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.04054",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "OmniMAE: Single Model Masked Pretraining on Images and Videos",
    "base_url": null,
    "title_page": null,
    "repo": "facebookresearch/omnivore",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Girdhar_OmniMAE_Single_Model_Masked_Pretraining_on_Images_and_Videos_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2206.08356",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "CNVid-3.5M: Build, Filter, and Pre-Train the Large-Scale Public Chinese Video-Text Dataset",
    "base_url": null,
    "title_page": null,
    "repo": "CNVid/CNVid-3.5M",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Gan_CNVid-3.5M_Build_Filter_and_Pre-Train_the_Large-Scale_Public_Chinese_Video-Text_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "ywJfAg4wvr0",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Egocentric Audio-Visual Object Localization",
    "base_url": null,
    "title_page": null,
    "repo": "WikiChao/Ego-AV-Loc",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Egocentric_Audio-Visual_Object_Localization_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.13471",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "0-_XJJ1JLmM",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Learning Visual Representations via Language-Guided Sampling",
    "base_url": null,
    "title_page": null,
    "repo": "mbanani/lgssl",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Banani_Learning_Visual_Representations_via_Language-Guided_Sampling_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2302.12248",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "95I8DmUoJ2s",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Unite and Conquer: Plug &amp; Play Multi-Modal Synthesis Using Diffusion Models",
    "base_url": null,
    "title_page": null,
    "repo": "Nithin-GK/UniteandConquer",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Nair_Unite_and_Conquer_Plug__Play_Multi-Modal_Synthesis_Using_Diffusion_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.00793",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "iQuery: Instruments As Queries for Audio-Visual Sound Separation",
    "base_url": null,
    "title_page": null,
    "repo": "JiabenChen/iQuery",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_iQuery_Instruments_As_Queries_for_Audio-Visual_Sound_Separation_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.03814",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "EZ9CgknV9Z4",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Diverse Embedding Expansion Network and Low-Light Cross-Modality Benchmark for Visible-Infrared Person Re-Identification",
    "base_url": null,
    "title_page": null,
    "repo": "ZYK100/LLCM",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Diverse_Embedding_Expansion_Network_and_Low-Light_Cross-Modality_Benchmark_for_Visible-Infrared_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.14481",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "oMIRqc-Fq5c",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "PiMAE: Point Cloud and Image Interactive Masked Autoencoders for 3D Object Detection",
    "base_url": null,
    "title_page": null,
    "repo": "BLVLab/PiMAE",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_PiMAE_Point_Cloud_and_Image_Interactive_Masked_Autoencoders_for_3D_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.08129",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "rcs8DYAwugQ",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners",
    "base_url": null,
    "title_page": null,
    "repo": "ZrrSkywalker/CaFo",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Prompt_Generate_Then_Cache_Cascade_of_Foundation_Models_Makes_Strong_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.02151",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Non-Contrastive Learning Meets Language-Image Pre-Training",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Zhou_Non-Contrastive_Learning_Meets_Language-Image_Pre-Training_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2210.09304",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Highly Confident Local Structure Based Consensus Graph Learning for Incomplete Multi-View Clustering",
    "base_url": null,
    "title_page": null,
    "repo": "ckghostwj/cvpr2023_code",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Wen_Highly_Confident_Local_Structure_Based_Consensus_Graph_Learning_for_Incomplete_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "8yaouwWf4ko",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Vision Transformers Are Parameter-Efficient Audio-Visual Learners",
    "base_url": null,
    "title_page": null,
    "repo": "GenjiB/LAVISH",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_Vision_Transformers_Are_Parameter-Efficient_Audio-Visual_Learners_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.07983",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Teaching Structured Vision &amp; Language Concepts to Vision &amp; Language Models",
    "base_url": null,
    "title_page": null,
    "repo": "SivanDoveh/TSVLC",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Doveh_Teaching_Structured_Vision__Language_Concepts_to_Vision__Language_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2211.11733",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "s_rr1RbX1Iw",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Data-Free Sketch-Based Image Retrieval",
    "base_url": null,
    "title_page": null,
    "repo": "abhrac/data-free-sbir",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Chaudhuri_Data-Free_Sketch-Based_Image_Retrieval_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.07775",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Align and Attend: Multimodal Summarization With Dual Contrastive Losses",
    "base_url": null,
    "title_page": null,
    "repo": "boheumd/A2Summ",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/He_Align_and_Attend_Multimodal_Summarization_With_Dual_Contrastive_Losses_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.07284",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "7EfdJx6G9rI",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Efficient Multimodal Fusion via Interactive Prompting",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Efficient_Multimodal_Fusion_via_Interactive_Prompting_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2304.06306",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "MK_HJkgGnJ4",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Multimodal Prompting With Missing Modalities for Visual Recognition",
    "base_url": null,
    "title_page": null,
    "repo": "YiLunLee/missing_aware_prompts",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Lee_Multimodal_Prompting_With_Missing_Modalities_for_Visual_Recognition_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.03369",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Learning Instance-Level Representation for Large-Scale Multi-Modal Pretraining in E-Commerce",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Jin_Learning_Instance-Level_Representation_for_Large-Scale_Multi-Modal_Pretraining_in_E-Commerce_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2304.02853",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "What Happened 3 Seconds Ago? Inferring the Past With Thermal Imaging",
    "base_url": null,
    "title_page": null,
    "repo": "ZitianTang/Thermal-IM",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Tang_What_Happened_3_Seconds_Ago_Inferring_the_Past_With_Thermal_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2304.13651",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "RKptl-mUYQw",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "MMANet: Margin-Aware Distillation and Modality-Aware Regularization for Incomplete Multimodal Learning",
    "base_url": null,
    "title_page": null,
    "repo": "shicaiwei123/MMANet",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Wei_MMANet_Margin-Aware_Distillation_and_Modality-Aware_Regularization_for_Incomplete_Multimodal_Learning_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2304.08028",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Multi-Modal Learning With Missing Modality via Shared-Specific Feature Modelling",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Multi-Modal_Learning_With_Missing_Modality_via_Shared-Specific_Feature_Modelling_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2307.14126",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "wkYFpWOZaeg",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "The ObjectFolder Benchmark: Multisensory Learning With Neural and Real Objects",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": "https://objectfolder.stanford.edu/",
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_The_ObjectFolder_Benchmark_Multisensory_Learning_With_Neural_and_Real_Objects_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "VhXDempUYgE",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Position-Guided Text Prompt for Vision-Language Pre-Training",
    "base_url": null,
    "title_page": null,
    "repo": "sail-sg/ptp",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Position-Guided_Text_Prompt_for_Vision-Language_Pre-Training_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.09737",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "6FNXUSMefIQ",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Conditional Generation of Audio From Video via Foley Analogies",
    "base_url": null,
    "title_page": null,
    "repo": "XYPB/CondFoleyGen",
    "web_page": null,
    "github_page": "https://xypb.github.io/CondFoleyGen/",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Du_Conditional_Generation_of_Audio_From_Video_via_Foley_Analogies_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2304.08490",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "OSAN: A One-Stage Alignment Network To Unify Multimodal Alignment and Unsupervised Domain Adaptation",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_OSAN_A_One-Stage_Alignment_Network_To_Unify_Multimodal_Alignment_and_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "84H9dQZD3DY",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Self-Supervised Video Forensics by Audio-Visual Anomaly Detection <br/> <img alt=\"CVPR - Highlight\" src=\"https://img.shields.io/badge/CVPR-Highlight-FFFF00\"/>",
    "base_url": null,
    "title_page": null,
    "repo": "cfeng16/audio-visual-forensics",
    "web_page": null,
    "github_page": "https://cfeng16.github.io/audio-visual-forensics/",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2301.01767",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding",
    "base_url": null,
    "title_page": null,
    "repo": "salesforce/ULIP",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_ULIP_Learning_a_Unified_Representation_of_Language_Images_and_Point_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.05171",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "dgFYBVmeilk",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "AVFormer: Injecting Vision Into Frozen Speech Models for Zero-Shot AV-ASR",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Seo_AVFormer_Injecting_Vision_Into_Frozen_Speech_Models_for_Zero-Shot_AV-ASR_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.16501",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Watch or Listen: Robust Audio-Visual Speech Recognition With Visual Corruption Modeling and Reliability Scoring",
    "base_url": null,
    "title_page": null,
    "repo": "joannahong/AV-RelScore",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Hong_Watch_or_Listen_Robust_Audio-Visual_Speech_Recognition_With_Visual_Corruption_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.08536",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "SceneTrilogy: On Human Scene-Sketch and Its Complementarity With Photo and Text",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": "https://www.pinakinathc.me/scenetrilogy/",
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Chowdhury_SceneTrilogy_On_Human_Scene-Sketch_and_Its_Complementarity_With_Photo_and_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2204.11964",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Exploring and Exploiting Uncertainty for Incomplete Multi-View Classification",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Xie_Exploring_and_Exploiting_Uncertainty_for_Incomplete_Multi-View_Classification_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2304.05165",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "d124-SGH6bw",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "EXIF As Language: Learning Cross-Modal Associations Between Images and Camera Metadata <br/> <img alt=\"CVPR - Highlight\" src=\"https://img.shields.io/badge/CVPR-Highlight-FFFF00\"/>",
    "base_url": null,
    "title_page": null,
    "repo": "hellomuffin/exif-as-language",
    "web_page": null,
    "github_page": "https://hellomuffin.github.io/exif-as-language/",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Zheng_EXIF_As_Language_Learning_Cross-Modal_Associations_Between_Images_and_Camera_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2301.04647",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "dPHdNgxUXd4",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens",
    "base_url": null,
    "title_page": null,
    "repo": "yuxiaochen1103/FDT",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Revisiting_Multimodal_Representation_in_Contrastive_Learning_From_Patch_and_Token_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.14865",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "PT1InnOlfmg",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "RONO: Robust Discriminative Learning With Noisy Labels for 2D-3D Cross-Modal Retrieval",
    "base_url": null,
    "title_page": null,
    "repo": "penghu-cs/RONO",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Feng_RONO_Robust_Discriminative_Learning_With_Noisy_Labels_for_2D-3D_Cross-Modal_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "CASP-Net: Rethinking Video Saliency Prediction From an Audio-Visual Consistency Perceptual Perspective",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": "https://junwenxiong.github.io/CASP-Net/index.html",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Xiong_CASP-Net_Rethinking_Video_Saliency_Prediction_From_an_Audio-Visual_Consistency_Perceptual_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.06357",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "keeAINxmhAM",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Learning Audio-Visual Source Localization via False Negative Aware Contrastive Learning",
    "base_url": null,
    "title_page": null,
    "repo": "OpenNLPLab/FNAC_AVL",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Sun_Learning_Audio-Visual_Source_Localization_via_False_Negative_Aware_Contrastive_Learning_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.11302",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "LZprwXIn-2U",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "ReVISE: Self-Supervised Speech Resynthesis With Visual Input for Universal and Generalized Speech Regeneration",
    "base_url": null,
    "title_page": null,
    "repo": "facebookresearch/av_hubert",
    "web_page": null,
    "github_page": "https://wnhsu.github.io/ReVISE/",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Hsu_ReVISE_Self-Supervised_Speech_Resynthesis_With_Visual_Input_for_Universal_and_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.11377",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Look, Radiate, and Learn: Self-Supervised Localisation via Radio-Visual Correspondence",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Alloulah_Look_Radiate_and_Learn_Self-Supervised_Localisation_via_Radio-Visual_Correspondence_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2206.06424",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "NUV0hm03slY",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Learning Emotion Representations From Verbal and Nonverbal Communication",
    "base_url": null,
    "title_page": null,
    "repo": "Xeaver/EmotionCLIP",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Learning_Emotion_Representations_From_Verbal_and_Nonverbal_Communication_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2305.13500",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "9Mcw_xf3Dk4",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Enhanced Multimodal Representation Learning With Cross-Modal KD",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Enhanced_Multimodal_Representation_Learning_With_Cross-Modal_KD_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2306.07646",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "ZTELXRN5W0M",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "MELTR: Meta Loss Transformer for Learning To Fine-Tune Video Foundation Models",
    "base_url": null,
    "title_page": null,
    "repo": "mlvlab/MELTR",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Ko_MELTR_Meta_Loss_Transformer_for_Learning_To_Fine-Tune_Video_Foundation_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.13009",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "nDIKwYRf-NE",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Multilateral Semantic Relations Modeling for Image Text Retrieval",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Multilateral_Semantic_Relations_Modeling_for_Image_Text_Retrieval_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "GeoVLN: Learning Geometry-Enhanced Visual Representation With Slot Attention for Vision-and-Language Navigation",
    "base_url": null,
    "title_page": null,
    "repo": "jingyanghuo/GeoVLN",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Huo_GeoVLN_Learning_Geometry-Enhanced_Visual_Representation_With_Slot_Attention_for_Vision-and-Language_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2305.17102",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "i-WjmcNMnQI",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Noisy Correspondence Learning With Meta Similarity Correction",
    "base_url": null,
    "title_page": null,
    "repo": "hhc1997/MSCN",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Han_Noisy_Correspondence_Learning_With_Meta_Similarity_Correction_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2304.06275",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "DspzqtqDgmo",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Improving Cross-Modal Retrieval With Set of Diverse Embeddings <br/> <img alt=\"CVPR - Highlight\" src=\"https://img.shields.io/badge/CVPR-Highlight-FFFF00\"/>",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": "https://cvlab.postech.ac.kr/research/DivE/",
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Kim_Improving_Cross-Modal_Retrieval_With_Set_of_Diverse_Embeddings_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2211.16761",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment",
    "base_url": null,
    "title_page": null,
    "repo": "postech-ami/Sound2Scene",
    "web_page": null,
    "github_page": "https://sound2scene.github.io/",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Sung-Bin_Sound_to_Visual_Scene_Generation_by_Audio-to-Visual_Latent_Alignment_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.17490",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "XETgkM22n6s",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "MaPLe: Multi-Modal Prompt Learning",
    "base_url": null,
    "title_page": null,
    "repo": "muzairkhattak/multimodal-prompt-learning",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Khattak_MaPLe_Multi-Modal_Prompt_Learning_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2210.03117",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "fmULeaqAzfg",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Fine-Grained Image-Text Matching by Cross-Modal Hard Aligning Network",
    "base_url": null,
    "title_page": null,
    "repo": "ppanzx/CHAN",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Pan_Fine-Grained_Image-Text_Matching_by_Cross-Modal_Hard_Aligning_Network_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "DwvSv_FATjg",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Towards Modality-Agnostic Person Re-Identification With Descriptive Query",
    "base_url": null,
    "title_page": null,
    "repo": "ccq195/UNIReID",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Towards_Modality-Agnostic_Person_Re-Identification_With_Descriptive_Query_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "_nQANSa7GyA",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Physics-Driven Diffusion Models for Impact Sound Synthesis From Videos",
    "base_url": null,
    "title_page": null,
    "repo": "sukun1045/video-physics-sound-diffusion",
    "web_page": null,
    "github_page": "https://sukun1045.github.io/video-physics-sound-diffusion/",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Su_Physics-Driven_Diffusion_Models_for_Impact_Sound_Synthesis_From_Videos_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.16897",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "8IDssk5bWmg",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "FashionSAP: Symbols and Attributes Prompt for Fine-Grained Fashion Vision-Language Pre-Training",
    "base_url": null,
    "title_page": null,
    "repo": "hssip/FashionSAP",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Han_FashionSAP_Symbols_and_Attributes_Prompt_for_Fine-Grained_Fashion_Vision-Language_Pre-Training_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2304.05051",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "MAP: Multimodal Uncertainty-Aware Vision-Language Pre-Training Model",
    "base_url": null,
    "title_page": null,
    "repo": "IIGROUP/MAP",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Ji_MAP_Multimodal_Uncertainty-Aware_Vision-Language_Pre-Training_Model_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2210.05335",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "XZqZDqe9MYo",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Egocentric Auditory Attention Localization in Conversations",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": "https://fkryan.github.io/saal",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Ryan_Egocentric_Auditory_Attention_Localization_in_Conversations_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.16024",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Improving Zero-Shot Generalization and Robustness of Multi-Modal Models",
    "base_url": null,
    "title_page": null,
    "repo": "gyhandy/Hierarchy-CLIP",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Ge_Improving_Zero-Shot_Generalization_and_Robustness_of_Multi-Modal_Models_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.01758",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "6nirYCh2xA0",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Understanding and Constructing Latent Modality Structures in Multi-Modal Representation Learning",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Jiang_Understanding_and_Constructing_Latent_Modality_Structures_in_Multi-Modal_Representation_Learning_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.05952",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "kEDr2zmhe3Q",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles <br/> <img alt=\"CVPR - Highlight\" src=\"https://img.shields.io/badge/CVPR-Highlight-FFFF00\"/>",
    "base_url": null,
    "title_page": null,
    "repo": "pleaseconnectwifi/DANCE",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Ye_Improving_Commonsense_in_Vision-Language_Models_via_Knowledge_Graph_Riddles_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2211.16504",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "GCFAgg: Global and Cross-View Feature Aggregation for Multi-View Clustering",
    "base_url": null,
    "title_page": null,
    "repo": "Galaxy922/GCFAggMVC",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Yan_GCFAgg_Global_and_Cross-View_Feature_Aggregation_for_Multi-View_Clustering_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2305.06799",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "BiCro: Noisy Correspondence Rectification for Multi-Modality Data via Bi-Directional Cross-Modal Similarity Consistency",
    "base_url": null,
    "title_page": null,
    "repo": "xu5zhao/BiCro",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_BiCro_Noisy_Correspondence_Rectification_for_Multi-Modality_Data_via_Bi-Directional_Cross-Modal_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.12419",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "DisCo-CLIP: A Distributed Contrastive Loss for Memory Efficient CLIP Training <br/> <img alt=\"CVPR - Highlight\" src=\"https://img.shields.io/badge/CVPR-Highlight-FFFF00\"/>",
    "base_url": null,
    "title_page": null,
    "repo": "IDEA-Research/DisCo-CLIP",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_DisCo-CLIP_A_Distributed_Contrastive_Loss_for_Memory_Efficient_CLIP_Training_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2304.08480",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "1MTUKnqRwC0",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Referring Image Matting",
    "base_url": null,
    "title_page": null,
    "repo": "JizhiziLi/RIM",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Referring_Image_Matting_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2206.05149",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "0d9J5DD7vAY",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Leveraging per Image-Token Consistency for Vision-Language Pre-Training",
    "base_url": null,
    "title_page": null,
    "repo": "gyhdog99/epic",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Gou_Leveraging_per_Image-Token_Consistency_for_Vision-Language_Pre-Training_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2211.15398",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Seeing What You Miss: Vision-Language Pre-Training With Semantic Completion Learning",
    "base_url": null,
    "title_page": null,
    "repo": "iigroup/scl",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Ji_Seeing_What_You_Miss_Vision-Language_Pre-Training_With_Semantic_Completion_Learning_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2211.13437",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "KXqcGfozrQw",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Sample-Level Multi-View Graph Clustering",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Tan_Sample-Level_Multi-View_Graph_Clustering_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "SmallCap: Lightweight Image Captioning Prompted With Retrieval Augmentation",
    "base_url": null,
    "title_page": null,
    "repo": "RitaRamo/smallcap",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Ramos_SmallCap_Lightweight_Image_Captioning_Prompted_With_Retrieval_Augmentation_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2209.15323",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "On the Effects of Self-Supervision and Contrastive Alignment in Deep Multi-View Clustering <br/> <img alt=\"CVPR - Highlight\" src=\"https://img.shields.io/badge/CVPR-Highlight-FFFF00\"/>",
    "base_url": null,
    "title_page": null,
    "repo": "DanielTrosten/DeepMVC",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Trosten_On_the_Effects_of_Self-Supervision_and_Contrastive_Alignment_in_Deep_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.09877",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "YRnOo3qs28A",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "SmartBrush: Text and Shape Guided Object Inpainting With Diffusion Model",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Xie_SmartBrush_Text_and_Shape_Guided_Object_Inpainting_With_Diffusion_Model_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.05034",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "kzrfcKi-XCI",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Novel-View Acoustic Synthesis",
    "base_url": null,
    "title_page": null,
    "repo": "facebookresearch/novel-view-acoustic-synthesis",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Novel-View_Acoustic_Synthesis_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2301.08730",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "5X8JdLYUA8w",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "MAGVLT: Masked Generative Vision-and-Language Transformer",
    "base_url": null,
    "title_page": null,
    "repo": "kakaobrain/magvlt",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Kim_MAGVLT_Masked_Generative_Vision-and-Language_Transformer_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.12208",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "8dpW-V7N3-0",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Reproducible Scaling Laws for Contrastive Language-Image Learning",
    "base_url": null,
    "title_page": null,
    "repo": "LAION-AI/scaling-laws-openclip",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.07143",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "QFNt52y6IK4",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "PMR: Prototypical Modal Rebalance for Multimodal Learning",
    "base_url": null,
    "title_page": null,
    "repo": "fanyunfeng-bit/Modal-Imbalance-PMR",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Fan_PMR_Prototypical_Modal_Rebalance_for_Multimodal_Learning_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2211.07089",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Language-Guided Music Recommendation for Video via Prompt Analogies <br/> <img alt=\"CVPR - Highlight\" src=\"https://img.shields.io/badge/CVPR-Highlight-FFFF00\"/>",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": "https://www.danielbmckee.com/language-guided-music-for-video/",
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/McKee_Language-Guided_Music_Recommendation_for_Video_via_Prompt_Analogies_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2306.09327",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "z6DBTAiLKzY",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "RA-CLIP: Retrieval Augmented Contrastive Language-Image Pre-Training",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Xie_RA-CLIP_Retrieval_Augmented_Contrastive_Language-Image_Pre-Training_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "MMG-Ego4D: Multimodal Generalization in Egocentric Action Recognition",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Gong_MMG-Ego4D_Multimodal_Generalization_in_Egocentric_Action_Recognition_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2305.07214",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "rZoMIkIk6Gw",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Open Vocabulary Semantic Segmentation With Patch Aligned Contrastive Learning <br/> <img alt=\"CVPR - Highlight\" src=\"https://img.shields.io/badge/CVPR-Highlight-FFFF00\"/>",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Mukhoti_Open_Vocabulary_Semantic_Segmentation_With_Patch_Aligned_Contrastive_Learning_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.04994",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "PRISE: Demystifying Deep Lucas-Kanade With Strongly Star-Convex Constraints for Multimodel Image Alignment <br/> <img alt=\"CVPR - Highlight\" src=\"https://img.shields.io/badge/CVPR-Highlight-FFFF00\"/>",
    "base_url": null,
    "title_page": null,
    "repo": "Zhang-VISLab/CVPR2023-PRISE",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_PRISE_Demystifying_Deep_Lucas-Kanade_With_Strongly_Star-Convex_Constraints_for_Multimodel_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.11526",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Masked Autoencoding Does Not Help Natural Language Supervision at Scale",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Weers_Masked_Autoencoding_Does_Not_Help_Natural_Language_Supervision_at_Scale_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2301.07836",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "CLIPPO: Image-and-Language Understanding From Pixels Only",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": "https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/clippo/README.md",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Tschannen_CLIPPO_Image-and-Language_Understanding_From_Pixels_Only_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.08045",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "3tTbm4_EsFU",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Chat2Map: Efficient Scene Mapping From Multi-Ego Conversations",
    "base_url": null,
    "title_page": null,
    "repo": "facebookresearch/chat2map-official",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Majumder_Chat2Map_Efficient_Scene_Mapping_From_Multi-Ego_Conversations_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2301.02184",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Critical Learning Periods for Multisensory Integration in Deep Networks <br/> <img alt=\"CVPR - Highlight\" src=\"https://img.shields.io/badge/CVPR-Highlight-FFFF00\"/>",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Kleinman_Critical_Learning_Periods_for_Multisensory_Integration_in_Deep_Networks_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2210.04643",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "CLIPPING: Distilling CLIP-Based Models With a Student Base for Video-Language Retrieval",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Pei_CLIPPING_Distilling_CLIP-Based_Models_With_a_Student_Base_for_Video-Language_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "NUWA-LIP: Language-Guided Image Inpainting With Defect-Free VQGAN",
    "base_url": null,
    "title_page": null,
    "repo": "kodenii/NUWA-LIP",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Ni_NUWA-LIP_Language-Guided_Image_Inpainting_With_Defect-Free_VQGAN_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2202.05009",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "WINNER: Weakly-Supervised hIerarchical decompositioN and aligNment for Spatio-tEmporal Video gRounding",
    "base_url": null,
    "title_page": null,
    "repo": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Li_WINNER_Weakly-Supervised_hIerarchical_decompositioN_and_aligNment_for_Spatio-tEmporal_Video_gRounding_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  },
  {
    "title": "Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation",
    "base_url": null,
    "title_page": null,
    "repo": "feiyuchen7/M3NET",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "zenodo": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Multivariate_Multi-Frequency_and_Multimodal_Rethinking_Graph_Neural_Networks_for_Emotion_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multi-Modal Learning"
  }
]