[
  {
    "title": "Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": "https://antoyang.github.io/vid2seq.html",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Vid2Seq_Large-Scale_Pretraining_of_a_Visual_Language_Model_for_Dense_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2302.14115",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "hXP-2fYzq4g",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Open-Vocabulary Panoptic Segmentation With Text-to-Image Diffusion Models",
    "base_url": null,
    "title_page": null,
    "github": "NVlabs/ODISE",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Open-Vocabulary_Panoptic_Segmentation_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.04803",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "eW2vF8o_7p0",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Iterative Proposal Refinement for Weakly-Supervised Video Grounding",
    "base_url": null,
    "title_page": null,
    "github": "ttengwang/Awesome_Long_Term_Video_Understanding",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Iterative_Proposal_Refinement_for_Weakly-Supervised_Video_Grounding_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "BbGvHI_pVXk",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "MetaCLUE: Towards Comprehensive Visual Metaphors Research",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": "https://metaclue.github.io/",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com/content/CVPR2023/papers/Akula_MetaCLUE_Towards_Comprehensive_Visual_Metaphors_Research_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.09898",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "V3TmeNETL-o",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "PolyFormer: Referring Image Segmentation As Sequential Polygon Generation",
    "base_url": null,
    "title_page": null,
    "github": "amazon-science/polygon-transformer",
    "web_page": null,
    "github_page": "https://polyformer.github.io/",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_PolyFormer_Referring_Image_Segmentation_As_Sequential_Polygon_Generation_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2302.07387",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "6LNrqoxQR1M",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "GeneCIS: A Benchmark for General Conditional Image Similarity",
    "base_url": null,
    "title_page": null,
    "github": "facebookresearch/genecis",
    "web_page": null,
    "github_page": "https://sgvaze.github.io/genecis/",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com/content/CVPR2023/papers/Vaze_GeneCIS_A_Benchmark_for_General_Conditional_Image_Similarity_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2306.07969",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "wu3U2iNGIUw",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks",
    "base_url": null,
    "title_page": null,
    "github": "BrandonHanx/FAME-ViL",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com/content/CVPR2023/papers/Han_FAME-ViL_Multi-Tasking_Vision-Language_Model_for_Heterogeneous_Fashion_Tasks_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.02483",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Generative Bias for Robust Visual Question Answering",
    "base_url": null,
    "title_page": null,
    "github": "chojw/genb",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Generative_Bias_for_Robust_Visual_Question_Answering_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2208.00690",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Advancing Visual Grounding With Scene Knowledge: Benchmark and Method",
    "base_url": null,
    "title_page": null,
    "github": "zhjohnchan/SK-VG",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Advancing_Visual_Grounding_With_Scene_Knowledge_Benchmark_and_Method_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2307.11558",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "DmmPiseO59o",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Gloss Attention for Gloss-Free Sign Language Translation",
    "base_url": null,
    "title_page": null,
    "github": "YinAoXiong/GASLT",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_Gloss_Attention_for_Gloss-Free_Sign_Language_Translation_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2307.07361",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "NEoWvxkJXfU",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "You Can Ground Earlier Than See: An Effective and Efficient Pipeline for Temporal Sentence Grounding in Compressed Videos",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_You_Can_Ground_Earlier_Than_See_An_Effective_and_Efficient_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.07863",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Generalized Decoding for Pixel, Image, and Language",
    "base_url": null,
    "title_page": null,
    "github": "microsoft/X-Decoder",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zou_Generalized_Decoding_for_Pixel_Image_and_Language_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.11270",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "wYp6vmyolqE",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Accelerating Vision-Language Pretraining With Free Language Modeling",
    "base_url": null,
    "title_page": null,
    "github": "TencentARC/FLM",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Accelerating_Vision-Language_Pretraining_With_Free_Language_Modeling_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.14038",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "WbH_5DH_jfY",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "GRES: Generalized Referring Expression Segmentation",
    "base_url": null,
    "title_page": null,
    "github": "henghuiding/ReLA",
    "web_page": null,
    "github_page": "https://henghuiding.github.io/GRES/",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_GRES_Generalized_Referring_Expression_Segmentation_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2306.00968",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "eWjAgYUU6Do",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "BUFFER: Balancing Accuracy, Efficiency, and Generalizability in Point Cloud Registration",
    "base_url": null,
    "title_page": null,
    "github": "The-Learning-And-Vision-Atelier-LAVA/BUFFER",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ao_BUFFER_Balancing_Accuracy_Efficiency_and_Generalizability_in_Point_Cloud_Registration_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "STmAkRWuSiY",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "RGB No More: Minimally-Decoded JPEG Vision Transformers",
    "base_url": null,
    "title_page": null,
    "github": "JeongsooP/RGB-no-more",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com/content/CVPR2023/papers/Park_RGB_No_More_Minimally-Decoded_JPEG_Vision_Transformers_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2211.16421",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Scaling Language-Image Pre-Training via Masking",
    "base_url": null,
    "title_page": null,
    "github": "facebookresearch/flip",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Scaling_Language-Image_Pre-Training_via_Masking_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.00794",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "coOHTSMWhb8",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding",
    "base_url": null,
    "title_page": null,
    "github": "yanmin-wu/EDA",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_EDA_Explicit_Text-Decoupling_and_Dense_Alignment_for_3D_Visual_Grounding_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2209.14941",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "YBpPqYU07Es",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "RefTeacher: A Strong Baseline for Semi-Supervised Referring Expression Comprehension",
    "base_url": null,
    "title_page": null,
    "github": "Disguiser15/RefTeacher",
    "web_page": null,
    "github_page": "https://refteacher.github.io/",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Sun_RefTeacher_A_Strong_Baseline_for_Semi-Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Mobile User Interface Element Detection via Adaptively Prompt Tuning",
    "base_url": null,
    "title_page": null,
    "github": "antmachineintelligence/MUI-zh",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Gu_Mobile_User_Interface_Element_Detection_via_Adaptively_Prompt_Tuning_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2305.09699",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "dMC26H1DQWw",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Tell Me What Happened: Unifying Text-Guided Video Completion via Multimodal Masked Video Generation",
    "base_url": null,
    "title_page": null,
    "github": "tsujuifu/pytorch_tvc",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Fu_Tell_Me_What_Happened_Unifying_Text-Guided_Video_Completion_via_Multimodal_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2211.12824",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "dnBzUfsf9Cc",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Meta Compositional Referring Expression Segmentation",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Meta_Compositional_Referring_Expression_Segmentation_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2304.04415",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "VindLU: A Recipe for Effective Video-and-Language Pretraining",
    "base_url": null,
    "title_page": null,
    "github": "klauscc/VindLU",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Cheng_VindLU_A_Recipe_for_Effective_Video-and-Language_Pretraining_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.05051",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "9koWpSPcYBQ",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "GIVL: Improving Geographical Inclusivity of Vision-Language Models With Pre-Training Methods",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Yin_GIVL_Improving_Geographical_Inclusivity_of_Vision-Language_Models_With_Pre-Training_Methods_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2301.01893",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "LAVENDER: Unifying Video-Language Understanding As Masked Language Modeling",
    "base_url": null,
    "title_page": null,
    "github": "microsoft/LAVENDER",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Li_LAVENDER_Unifying_Video-Language_Understanding_As_Masked_Language_Modeling_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2206.07160",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "f8scI82_caE",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "An Empirical Study of End-to-End Video-Language Transformers With Masked Visual Modeling",
    "base_url": null,
    "title_page": null,
    "github": "tsujuifu/pytorch_empirical-mvm",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Fu_An_Empirical_Study_of_End-to-End_Video-Language_Transformers_With_Masked_Visual_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2209.01540",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "T1qTkcMCq1k",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations",
    "base_url": null,
    "title_page": null,
    "github": "joyhsu0504/NS3D",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Hsu_NS3D_Neuro-Symbolic_Grounding_of_3D_Objects_and_Relations_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.13483",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "FmWgRR3NKIg",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Clover: Towards a Unified Video-Language Alignment and Fusion Model",
    "base_url": null,
    "title_page": null,
    "github": "LeeYN-43/Clover",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Clover_Towards_a_Unified_Video-Language_Alignment_and_Fusion_Model_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2207.07885",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Dual Alignment Unsupervised Domain Adaptation for Video-Text Retrieval",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Hao_Dual_Alignment_Unsupervised_Domain_Adaptation_for_Video-Text_Retrieval_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "FuwfdNEpfsM",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Task Residual for Tuning Vision-Language Models",
    "base_url": null,
    "title_page": null,
    "github": "geekyutao/TaskRes",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_Task_Residual_for_Tuning_Vision-Language_Models_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2211.10277",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "VbkT-AS3ZRA",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": "https://bluestyle97.github.io/dream3d/",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Dream3D_Zero-Shot_Text-to-3D_Synthesis_Using_3D_Shape_Prior_and_Text-to-Image_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.14704",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "rjomF5oe4_M",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "End-to-End 3D Dense Captioning With Vote2Cap-DETR",
    "base_url": null,
    "title_page": null,
    "github": "ch3cook-fdu/Vote2Cap-DETR",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_End-to-End_3D_Dense_Captioning_With_Vote2Cap-DETR_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2301.02508",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "azR_OvPWYfo",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Luo_Towards_Generalisable_Video_Moment_Retrieval_Visual-Dynamic_Injection_to_Image-Text_Pre-Training_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.00040",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Adaptive Zone-Aware Hierarchical Planner for Vision-Language Navigation",
    "base_url": null,
    "title_page": null,
    "github": "chengaopro/azhp",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Exploring the Effect of Primitives for Compositional Generalization in Vision-and-Language",
    "base_url": null,
    "title_page": null,
    "github": "NeverMoreLCH/SSL2CG",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Exploring_the_Effect_of_Primitives_for_Compositional_Generalization_in_Vision-and-Language_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Referring Multi-Object Tracking",
    "base_url": null,
    "title_page": null,
    "github": "wudongming97/RMOT",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Referring_Multi-Object_Tracking_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.03366",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "oXY0s8P5C1g",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Mofusion: A Framework for Denoising-Diffusion-Based Motion Synthesis",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": "https://vcai.mpi-inf.mpg.de/projects/MoFusion/",
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Dabral_Mofusion_A_Framework_for_Denoising-Diffusion-Based_Motion_Synthesis_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.04495",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "Woo73Gy2jUg",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "MIST: Multi-Modal Iterative Spatial-Temporal Transformer for Long-Form Video Question Answering",
    "base_url": null,
    "title_page": null,
    "github": "showlab/mist",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_MIST_Multi-Modal_Iterative_Spatial-Temporal_Transformer_for_Long-Form_Video_Question_Answering_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.09522",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "Tov7aR_Dnr0",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Learning To Segment Every Referring Object Point by Point",
    "base_url": null,
    "title_page": null,
    "github": "qumengxue/Partial-RES",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Qu_Learning_To_Segment_Every_Referring_Object_Point_by_Point_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "XRKem03hl_k",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Contrastive Grouping With Transformer for Referring Image Segmentation",
    "base_url": null,
    "title_page": null,
    "github": "SooLab/CGFormer",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Tang_Contrastive_Grouping_With_Transformer_for_Referring_Image_Segmentation_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Prototype-Based Embedding Network for Scene Graph Generation",
    "base_url": null,
    "title_page": null,
    "github": "VL-Group/PENET",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Zheng_Prototype-Based_Embedding_Network_for_Scene_Graph_Generation_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.07096",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "pMBYBYlmkNQ",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Collaborative Static and Dynamic Vision-Language Streams for Spatio-Temporal Video Grounding",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_Collaborative_Static_and_Dynamic_Vision-Language_Streams_for_Spatio-Temporal_Video_Grounding_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "NmfykPpl1vE",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "S<sup>3</sup>C: Semi-Supervised VQA Natural Language Explanation via Self-Critical Learning",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Suo_S3C_Semi-Supervised_VQA_Natural_Language_Explanation_via_Self-Critical_Learning_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2309.02155",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "7p_rkzsaXnk",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Prompting Large Language Models With Answer Heuristics for Knowledge-Based Visual Question Answering",
    "base_url": null,
    "title_page": null,
    "github": "MILVLG/prophet",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Shao_Prompting_Large_Language_Models_With_Answer_Heuristics_for_Knowledge-Based_Visual_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.01903",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "IjRpn5rDLyo",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "HAAV: Hierarchical Aggregation of Augmented Views for Image Captioning",
    "base_url": null,
    "title_page": null,
    "github": "GT-RIPL/HAAV",
    "web_page": "https://sites.google.com/view/chiawen-kuo/home/haav",
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Kuo_HAAV_Hierarchical_Aggregation_of_Augmented_Views_for_Image_Captioning_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2305.16295",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "s9nq8T51Gs0",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Zero-Shot Referring Image Segmentation With Global-Local Context Features",
    "base_url": null,
    "title_page": null,
    "github": "Seonghoon-Yu/Zero-shot-RIS",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_Zero-Shot_Referring_Image_Segmentation_With_Global-Local_Context_Features_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.17811",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "X_37jodjz2Y",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Hierarchical Semantic Correspondence Networks for Video Paragraph Grounding",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Tan_Hierarchical_Semantic_Correspondence_Networks_for_Video_Paragraph_Grounding_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "fkGopOsJAF8",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",
    "base_url": null,
    "title_page": null,
    "github": "facebookresearch/diht",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Radenovic_Filtering_Distillation_and_Hard_Negatives_for_Vision-Language_Pre-Training_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2301.02280",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "qvLxglovwDU",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Probabilistic Prompt Learning for Dense Prediction",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Kwon_Probabilistic_Prompt_Learning_for_Dense_Prediction_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2304.00779",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "6PMEDyNa2xQ",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-Training via Word-Region Alignment",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Yao_DetCLIPv2_Scalable_Open-Vocabulary_Object_Detection_Pre-Training_via_Word-Region_Alignment_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2304.04514",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "LIUcWY84Egg",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "All in One: Exploring Unified Video-Language Pre-Training",
    "base_url": null,
    "title_page": null,
    "github": "showlab/all-in-one",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_All_in_One_Exploring_Unified_Video-Language_Pre-Training_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2203.07303",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "sH6tdF8OTPM",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding",
    "base_url": null,
    "title_page": null,
    "github": "TAU-VAILab/isbertblind",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Alper_Is_BERT_Blind_Exploring_the_Effect_of_Vision-and-Language_Pretraining_on_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.12513",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "iZcA0Q07naY",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Divide and Conquer: Answering Questions With Object Factorization and Compositional Reasoning",
    "base_url": null,
    "title_page": null,
    "github": "szzexpoi/POEM",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Divide_and_Conquer_Answering_Questions_With_Object_Factorization_and_Compositional_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.10482",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "WtBq2BKnRd0",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "ConZIC: Controllable Zero-Shot Image Captioning by Sampling-Based Polishing",
    "base_url": null,
    "title_page": null,
    "github": "joeyz0z/ConZIC",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Zeng_ConZIC_Controllable_Zero-Shot_Image_Captioning_by_Sampling-Based_Polishing_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.02437",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "sHbE88HsvNg",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "RefCLIP: A Universal Teacher for Weakly Supervised Referring Expression Comprehension",
    "base_url": null,
    "title_page": null,
    "github": "kingthreestones/RefCLIP",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Jin_RefCLIP_A_Universal_Teacher_for_Weakly_Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation",
    "base_url": null,
    "title_page": null,
    "github": "XiangyangLi20/KERM",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Li_KERM_Knowledge_Enhanced_Reasoning_for_Vision-and-Language_Navigation_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.15796",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "Afm4rymYtMM",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "ANetQA: A Large-Scale Benchmark for Fine-Grained Compositional Reasoning Over Untrimmed Videos",
    "base_url": null,
    "title_page": null,
    "github": "MILVLG/anetqa-code",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_ANetQA_A_Large-Scale_Benchmark_for_Fine-Grained_Compositional_Reasoning_Over_Untrimmed_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2305.02519",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "hVvfP_xCUGM",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "ViLEM: Visual-Language Error Modeling for Image-Text Retrieval",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_ViLEM_Visual-Language_Error_Modeling_for_Image-Text_Retrieval_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Multi-Modal Representation Learning With Text-Driven Soft Masks",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Park_Multi-Modal_Representation_Learning_With_Text-Driven_Soft_Masks_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2304.00719",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Meta-Personalizing Vision-Language Models To Find Named Instances in Video",
    "base_url": null,
    "title_page": null,
    "github": "danielchyeh/this-is-my",
    "web_page": null,
    "github_page": "https://danielchyeh.github.io/metaper/",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Yeh_Meta-Personalizing_Vision-Language_Models_To_Find_Named_Instances_in_Video_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2306.10169",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "DnOOThEGZmU",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "ReCo: Region-Controlled Text-to-Image Generation",
    "base_url": null,
    "title_page": null,
    "github": "microsoft/ReCo",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_ReCo_Region-Controlled_Text-to-Image_Generation_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2211.15518",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "8X0zvl05mtU",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Are Deep Neural Networks SMARTer Than Second Graders?",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": "https://smartdataset.github.io/smart101/",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Cherian_Are_Deep_Neural_Networks_SMARTer_Than_Second_Graders_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2212.09993",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "nLtlC1fs5vY",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Graph Representation for Order-Aware Visual Transformation",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Qiu_Graph_Representation_for_Order-Aware_Visual_Transformation_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "3D Concept Learning and Reasoning From Multi-View Images",
    "base_url": null,
    "title_page": null,
    "github": "evelinehong/3D-CLR-Official",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Hong_3D_Concept_Learning_and_Reasoning_From_Multi-View_Images_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.11327",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Text With Knowledge Graph Augmented Transformer for Video Captioning",
    "base_url": null,
    "title_page": null,
    "github": "nightonion/textkg",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Gu_Text_With_Knowledge_Graph_Augmented_Transformer_for_Video_Captioning_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.12423",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "GCxQ8pKalcM",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Crossing the Gap: Domain Generalization for Image Captioning",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Ren_Crossing_the_Gap_Domain_Generalization_for_Image_Captioning_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "KdZTnBMaQ5c",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering",
    "base_url": null,
    "title_page": null,
    "github": "jingjing12110/MixPHM",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Jiang_MixPHM_Redundancy-Aware_Parameter-Efficient_Tuning_for_Low-Resource_Visual_Question_Answering_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.01239",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "NMnrcXqYaZo",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "VQACL: A Novel Visual Question Answering Continual Learning Setting",
    "base_url": null,
    "title_page": null,
    "github": "zhangxi1997/VQACL",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_VQACL_A_Novel_Visual_Question_Answering_Continual_Learning_Setting_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "zlkJEQEjTKc",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Improving Selective Visual Question Answering by Learning From Your Peers",
    "base_url": null,
    "title_page": null,
    "github": "facebookresearch/selective-vqa_ood",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Dancette_Improving_Selective_Visual_Question_Answering_by_Learning_From_Your_Peers_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2306.08751",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "High-Fidelity 3D Face Generation From Natural Language Descriptions",
    "base_url": null,
    "title_page": null,
    "github": "zhuhao-nju/describe3d",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_High-Fidelity_3D_Face_Generation_From_Natural_Language_Descriptions_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2305.03302",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Language-Guided Audio-Visual Source Separation via Trimodal Consistency",
    "base_url": null,
    "title_page": null,
    "github": "rxtan2/AVSeT",
    "web_page": null,
    "github_page": "https://cs-people.bu.edu/rxtan/projects/VAST/",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Tan_Language-Guided_Audio-Visual_Source_Separation_via_Trimodal_Consistency_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.16342",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "tjrmTEOJ7Wk",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Test of Time: Instilling Video-Language Models With a Sense of Time",
    "base_url": null,
    "title_page": null,
    "github": "bpiyush/TestOfTime",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Bagad_Test_of_Time_Instilling_Video-Language_Models_With_a_Sense_of_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2301.02074",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "RTRRdrA5H88",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Learning Situation Hyper-Graphs for Video Question Answering",
    "base_url": null,
    "title_page": null,
    "github": "aurooj/SHG-VQA",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Urooj_Learning_Situation_Hyper-Graphs_for_Video_Question_Answering_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2304.08682",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "lxkFfadg_78",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Pic2Word: Mapping Pictures to Words for Zero-Shot Composed Image Retrieval",
    "base_url": null,
    "title_page": null,
    "github": "google-research/composed_image_retrieval",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Saito_Pic2Word_Mapping_Pictures_to_Words_for_Zero-Shot_Composed_Image_Retrieval_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2302.03084",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "m-Z72lBPeYc",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Fine-Grained Audible Video Description",
    "base_url": null,
    "title_page": null,
    "github": "OpenNLPLab/FAVDBench",
    "web_page": "http://www.avlbench.opennlplab.cn/papers/favd",
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Shen_Fine-Grained_Audible_Video_Description_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2303.15616",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "sIuYDu6t9hk",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Discovering the Real Association: Multimodal Causal Reasoning in Video Question Answering",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Zang_Discovering_the_Real_Association_Multimodal_Causal_Reasoning_in_Video_Question_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "A-Cap: Anticipation Captioning With Commonsense Knowledge",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Vo_A-Cap_Anticipation_Captioning_With_Commonsense_Knowledge_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2304.06602",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "kJtIXqXsDHQ",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Cross-Domain Image Captioning With Discriminative Finetuning",
    "base_url": null,
    "title_page": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Dessi_Cross-Domain_Image_Captioning_With_Discriminative_Finetuning_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2304.01662",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Improving Visual Grounding by Encouraging Consistent Gradient-Based Explanations",
    "base_url": null,
    "title_page": null,
    "github": "uvavision/AMC-grounding",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_Improving_Visual_Grounding_by_Encouraging_Consistent_Gradient-Based_Explanations_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2206.15462",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "meZu2mKfhUM",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training",
    "base_url": null,
    "title_page": null,
    "github": "gicheonkang/gst-visdial",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Kang_The_Dialog_Must_Go_On_Improving_Visual_Dialog_via_Generative_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2205.12502",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "SrOzZRyqezs",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Q: How To Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!",
    "base_url": null,
    "title_page": null,
    "github": "codezakh/SelTDA",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Khan_Q_How_To_Specialize_Large_Vision-Language_Models_to_Data-Scarce_VQA_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2306.03932",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Open-Vocabulary Semantic Segmentation With Mask-Adapted CLIP",
    "base_url": null,
    "title_page": null,
    "github": "facebookresearch/ov-seg",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Liang_Open-Vocabulary_Semantic_Segmentation_With_Mask-Adapted_CLIP_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2210.04150",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "xIUSG0pLNyo",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Similarity Maps for Self-Training Weakly-Supervised Phrase Grounding",
    "base_url": null,
    "title_page": null,
    "github": "talshaharabany/Similarity-Maps-for-Self-Training-Weakly-Supervised-Phrase-Grounding",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Shaharabany_Similarity_Maps_for_Self-Training_Weakly-Supervised_Phrase_Grounding_CVPR_2023_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": "0wg84aYN9U4",
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  },
  {
    "title": "Language Adaptive Weight Generation for Multi-Task Visual Grounding",
    "base_url": null,
    "title_page": null,
    "github": "Charles-Xie/awesome-described-object-detection",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "https://openaccess.thecvf.com//content/CVPR2023/papers/Su_Language_Adaptive_Weight_Generation_for_Multi-Task_Visual_Grounding_CVPR_2023_paper.pdf",
    "paper_arxiv_id": "2306.04652",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Vision, Language, and Reasoning"
  }
]