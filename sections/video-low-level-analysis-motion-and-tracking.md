# CVPR-2023-Papers

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/computational-imaging.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/vision-applications-and-systems.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Video: Low-Level Analysis, Motion, and Tracking

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Uncovering the Missing Pattern: Unified Framework Towards Trajectory Imputation and Prediction | [![GitHub](https://img.shields.io/github/stars/colorfulfuture/GC-VRNN)](https://github.com/colorfulfuture/GC-VRNN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Uncovering_the_Missing_Pattern_Unified_Framework_Towards_Trajectory_Imputation_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16005-b31b1b.svg)](http://arxiv.org/abs/2303.16005) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=fneLvMJIALo) |
| Tracking Multiple Deformable Objects in Egocentric Videos | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mingzhenhuang.com/projects/detracker.html) <br /> [![GitHub](https://img.shields.io/github/stars/Mingzhen-Huang/DETracker)](https://github.com/Mingzhen-Huang/DETracker) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Tracking_Multiple_Deformable_Objects_in_Egocentric_Videos_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Tracking through Containers and Occluders in the Wild | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://tcow.cs.columbia.edu/) <br /> [![GitHub](https://img.shields.io/github/stars/basilevh/tcow)](https://github.com/basilevh/tcow) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Van_Hoorick_Tracking_Through_Containers_and_Occluders_in_the_Wild_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.03052-b31b1b.svg)](http://arxiv.org/abs/2305.03052) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WODiwfq9d2g) |
| TarViS: A Unified Approach for Target-based Video Segmentation | [![GitHub](https://img.shields.io/github/stars/Ali2500/TarViS)](https://github.com/Ali2500/TarViS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Athar_TarViS_A_Unified_Approach_for_Target-Based_Video_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.02657-b31b1b.svg)](http://arxiv.org/abs/2301.02657) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qecFRmSYq40) |
| VideoTrack: Learning to Track Objects via Video Transformer | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_VideoTrack_Learning_To_Track_Objects_via_Video_Transformer_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=v-SvFZ0FrF8) |
| ARKitTrack: A New Diverse Dataset for Tracking using Mobile RGB-D Data | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://arkittrack.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/lawrence-cj/ARKitTrack)](https://github.com/lawrence-cj/ARKitTrack) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_ARKitTrack_A_New_Diverse_Dataset_for_Tracking_Using_Mobile_RGB-D_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13885-b31b1b.svg)](http://arxiv.org/abs/2303.13885) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=r02f6egcpdw) |
| A Dynamic Multi-Scale Voxel Flow Network for Video Prediction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://huxiaotaostasy.github.io/DMVFN/) <br /> [![GitHub](https://img.shields.io/github/stars/megvii-research/CVPR2023-DMVFN)](https://github.com/megvii-research/CVPR2023-DMVFN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_A_Dynamic_Multi-Scale_Voxel_Flow_Network_for_Video_Prediction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09875-b31b1b.svg)](http://arxiv.org/abs/2303.09875) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rlghCGbAqUo) |
| Representation Learning for Visual Object Tracking by Masked Appearance Transfer | [![GitHub](https://img.shields.io/github/stars/difhnp/MAT)](https://github.com/difhnp/MAT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Representation_Learning_for_Visual_Object_Tracking_by_Masked_Appearance_Transfer_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| EqMotion: Equivariant Multi-Agent Motion Prediction with Invariant Interaction Reasoning | [![GitHub](https://img.shields.io/github/stars/MediaBrain-SJTU/EqMotion)](https://github.com/MediaBrain-SJTU/EqMotion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_EqMotion_Equivariant_Multi-Agent_Motion_Prediction_With_Invariant_Interaction_Reasoning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10876-b31b1b.svg)](http://arxiv.org/abs/2303.10876) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ROactuGU1YA) |
| Semi-Supervised Video Inpainting with Cycle Consistency Constraints | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Semi-Supervised_Video_Inpainting_With_Cycle_Consistency_Constraints_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.06807-b31b1b.svg)](http://arxiv.org/abs/2208.06807) | :heavy_minus_sign: |
| Generalized Relation Modeling for Transformer Tracking | [![GitHub](https://img.shields.io/github/stars/Little-Podi/GRM)](https://github.com/Little-Podi/GRM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Generalized_Relation_Modeling_for_Transformer_Tracking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16580-b31b1b.svg)](http://arxiv.org/abs/2303.16580) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bQKN3HV-8XI) |
| Breaking the ``Object`` in Video Object Segmentation | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.vostdataset.org/) <br /> [![GitHub](https://img.shields.io/github/stars/TRI-ML/VOST)](https://github.com/TRI-ML/VOST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tokmakov_Breaking_the_Object_in_Video_Object_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.06200-b31b1b.svg)](http://arxiv.org/abs/2212.06200) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=SBdA6HCXf_M) |
| Unifying Short and Long-Term Tracking with Graph Hierarchies | [![GitHub](https://img.shields.io/github/stars/dvl-tum/SUSHI)](https://github.com/dvl-tum/SUSHI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cetintas_Unifying_Short_and_Long-Term_Tracking_With_Graph_Hierarchies_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03038-b31b1b.svg)](http://arxiv.org/abs/2212.03038) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Q1TiZukGQYQ) |
| Simple Cues Lead to a Strong Multi-Object Tracker | [![GitHub](https://img.shields.io/github/stars/dvl-tum/GHOST)](https://github.com/dvl-tum/GHOST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Seidenschwarz_Simple_Cues_Lead_to_a_Strong_Multi-Object_Tracker_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.04656-b31b1b.svg)](http://arxiv.org/abs/2206.04656) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3gozhzOHwE0) |
| Unified Mask Embedding and Correspondence Learning for Self-Supervised Video Segmentation | [![GitHub](https://img.shields.io/github/stars/0liliulei/Mask-VOS)](https://github.com/0liliulei/Mask-VOS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Unified_Mask_Embedding_and_Correspondence_Learning_for_Self-Supervised_Video_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10100-b31b1b.svg)](http://arxiv.org/abs/2303.10100) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LJj4frqBgqY) |
| MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors | [![GitHub](https://img.shields.io/github/stars/megvii-research/MOTRv2)](https://github.com/megvii-research/MOTRv2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_MOTRv2_Bootstrapping_End-to-End_Multi-Object_Tracking_by_Pretrained_Object_Detectors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.09791-b31b1b.svg)](http://arxiv.org/abs/2211.09791) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7WnQgQLQLE4) |
| SeqTrack: Sequence to Sequence Learning for Visual Object Tracking | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](microsoft/VideoX/tree/master/SeqTrack) <br /> [![GitHub](https://img.shields.io/github/stars/microsoft/VideoX)](https://github.com/microsoft/VideoX) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_SeqTrack_Sequence_to_Sequence_Learning_for_Visual_Object_Tracking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.14394-b31b1b.svg)](http://arxiv.org/abs/2304.14394) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jb_zZqrEcJA) |
| Joint Visual Grounding and Tracking with Natural Language Specification | [![GitHub](https://img.shields.io/github/stars/lizhou-cs/JointNLT)](https://github.com/lizhou-cs/JointNLT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Joint_Visual_Grounding_and_Tracking_With_Natural_Language_Specification_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12027-b31b1b.svg)](http://arxiv.org/abs/2303.12027) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kUN7tchiG2Q) |
| Boosting Video Object Segmentation via Space-Time Correspondence Learning | [![GitHub](https://img.shields.io/github/stars/wenguanwang/VOS_Correspondence)](https://github.com/wenguanwang/VOS_Correspondence) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Boosting_Video_Object_Segmentation_via_Space-Time_Correspondence_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06211-b31b1b.svg)](http://arxiv.org/abs/2304.06211) | :heavy_minus_sign: |
| Visual Prompt Multi-Modal Tracking | [![GitHub](https://img.shields.io/github/stars/jiawen-zhu/ViPT)](https://github.com/jiawen-zhu/ViPT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Visual_Prompt_Multi-Modal_Tracking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10826-b31b1b.svg)](http://arxiv.org/abs/2303.10826) | :heavy_minus_sign: |
| OVTrack: Open-Vocabulary Multiple Object Tracking | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.vis.xyz/pub/ovtrack/) <br /> [![GitHub](https://img.shields.io/github/stars/SysCV/ovtrack)](https://github.com/SysCV/ovtrack) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_OVTrack_Open-Vocabulary_Multiple_Object_Tracking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08408-b31b1b.svg)](http://arxiv.org/abs/2304.08408) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tMQ_sh0JbpY) |
| TransFlow: Transformer as Flow Learner | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_TransFlow_Transformer_As_Flow_Learner_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.11523-b31b1b.svg)](http://arxiv.org/abs/2304.11523) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xbnyj9wspqA) |
| Focus on Details: Online Multi-Object Tracking with Diverse Fine-grained Representation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Focus_on_Details_Online_Multi-Object_Tracking_With_Diverse_Fine-Grained_Representation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.14589-b31b1b.svg)](http://arxiv.org/abs/2302.14589) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=j4TJJEsqllM) |
| Autoregressive Visual Tracking | [![GitHub](https://img.shields.io/github/stars/MIV-XJTU/ARTrack)](https://github.com/MIV-XJTU/ARTrack) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Autoregressive_Visual_Tracking_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=fOkpG5SNaX0) |
| Bootstrapping Objectness from Videos by Relaxed Common Fate and Visual Grouping | [![GitHub](https://img.shields.io/github/stars/TonyLianLong/RCF-UnsupVideoSeg)](https://github.com/TonyLianLong/RCF-UnsupVideoSeg) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lian_Bootstrapping_Objectness_From_Videos_by_Relaxed_Common_Fate_and_Visual_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08025-b31b1b.svg)](http://arxiv.org/abs/2304.08025) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dyaDEvT4YkY) |
| Tangentially Elongated Gaussian Belief Propagation for Event-based Incremental Optical Flow Estimation | [![GitHub](https://img.shields.io/github/stars/DensoITLab/tegbp)](https://github.com/DensoITLab/tegbp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Nagata_Tangentially_Elongated_Gaussian_Belief_Propagation_for_Event-Based_Incremental_Optical_Flow_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rClkk5MY33A) |
| Bridging Search Region Interaction with Template for RGB-T Tracking |  |  |  |
| Efficient RGB-T Tracking via Cross-Modality Distillation |  |  |  |
| MotionTrack: Learning Robust Short-Term and Long-Term Motions for Multi-Object Tracking |  |  |  |
| Self-Supervised AutoFlow |  |  |  |
| UTM: A Unified Multiple Object Tracking Model with Identity-Aware Feature Enhancement |  |  |  |
| BiFormer: Learning Bilateral Motion Estimation via Bilateral Transformer for 4K Video Frame Interpolation |  |  |  |
| Spatial-then-Temporal Self-Supervised Learning for Video Correspondence |  |  |  |
| BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects |  |  |  |
| MED-VT: Multiscale Encoder-Decoder Video Transformer with Application to Object Segmentation |  |  |  |
| Context-Aware Relative Object Queries to Unify Video Instance and Panoptic Segmentation |  |  |  |
| Unsupervised Space-Time Network for Temporally-Consistent Segmentation of Multiple Motions |  |  |  |
| Resource-Efficient RGBD Aerial Tracking |  |  |  |
| MMVC: Learned Multi-Mode Video Compression with Block-based Prediction Mode Selection and Density-Adaptive Entropy Coding |  |  |  |
| Streaming Video Model |  |  |  |
| Weakly Supervised Class-Agnostic Motion Prediction for Autonomous Driving |  |  |  |
| LSTFE-Net: Long Short-Term Feature Enhancement Network for Video Small Object Detection |  |  |  |
| DistractFlow: Improving Optical Flow Estimation via Realistic Distractions and Pseudo-Labeling |  |  |  |
| SCOTCH and SODA: A Transformer Video Shadow Detection Framework |  |  |  |
| ZBS: Zero-Shot Background Subtraction via Instance-Level Background Modeling and Foreground Selection |  |  |  |
| Frame-Event Alignment and Fusion Network for High Frame Rate Tracking |  |  |  |
