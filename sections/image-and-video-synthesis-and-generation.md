# CVPR-2023-Papers

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/3d-from-multi-view-and-sensors.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/humans-face-body-pose-gesture-movement.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" />
    </a>
</div>

## Image and Video Synthesis and Generation

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Towards Universal Fake Image Detectors That Generalize Across Generative Models | [![GitHub](https://img.shields.io/github/stars/Yuheng-Li/UniversalFakeDetect)](https://github.com/Yuheng-Li/UniversalFakeDetect)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ojha_Towards_Universal_Fake_Image_Detectors_That_Generalize_Across_Generative_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.10174-b31b1b.svg)](http://arxiv.org/abs/2302.10174)|:heavy_minus_sign: |
| Implicit Diffusion Models for Continuous Super-Resolution | [![GitHub](https://img.shields.io/github/stars/Ree1s/IDM)](https://github.com/Ree1s/IDM)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_Implicit_Diffusion_Models_for_Continuous_Super-Resolution_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16491-b31b1b.svg)](http://arxiv.org/abs/2303.16491)| :heavy_minus_sign: | 
| High-Fidelity Guided Image Synthesis With Latent Diffusion Models |  [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://1jsingh.github.io/gradop) <br /> [![GitHub](https://img.shields.io/github/stars/1jsingh/GradOP-Guided-Image-Synthesis)](https://github.com/1jsingh/GradOP-Guided-Image-Synthesis) |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Singh_High-Fidelity_Guided_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.17084-b31b1b.svg)](http://arxiv.org/abs/2211.17084)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Yk83RPCOa2o) |
| DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields | [![GitHub](https://img.shields.io/github/stars/AIBluefisher/dbarf)](https://github.com/AIBluefisher/dbarf)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_DBARF_Deep_Bundle-Adjusting_Generalizable_Neural_Radiance_Fields_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14478-b31b1b.svg)](http://arxiv.org/abs/2303.14478)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wFPO403wtAg) | 
| Deep Arbitrary-Scale Image Super-Resolution via Scale-Equivariance Pursuit | [![GitHub](https://img.shields.io/github/stars/neuralchen/EQSR)](https://github.com/neuralchen/EQSR)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Deep_Arbitrary-Scale_Image_Super-Resolution_via_Scale-Equivariance_Pursuit_CVPR_2023_paper.pdf) |[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Pq9eI5kxqUE) |
| Balanced Spherical Grid for Egocentric View Synthesis | [![GitHub](https://img.shields.io/github/stars/changwoonchoi/EgoNeRF)](https://github.com/changwoonchoi/EgoNeRF)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Choi_Balanced_Spherical_Grid_for_Egocentric_View_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12408-b31b1b.svg)](http://arxiv.org/abs/2303.12408)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=D-lsBhVP8zw) |
| SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation | [![GitHub](https://img.shields.io/github/stars/yccyenchicheng/SDFusion)](https://github.com/yccyenchicheng/SDFusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cheng_SDFusion_Multimodal_3D_Shape_Completion_Reconstruction_and_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04493-b31b1b.svg)](http://arxiv.org/abs/2212.04493) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6EvHJRlUMFQ) |
| DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation <br/> [![CVPR - Award](https://img.shields.io/badge/CVPR-Award-294A7C)]() | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dreambooth.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.12242-b31b1b.svg)](http://arxiv.org/abs/2208.12242) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=82x4XTSFwBQ) |
| Self-Guided Diffusion Models | [![GitHub](https://img.shields.io/github/stars/dongzhuoyao/self-guided-diffusion-models)](https://github.com/dongzhuoyao/self-guided-diffusion-models) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hu_Self-Guided_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.06462-b31b1b.svg)](http://arxiv.org/abs/2210.06462) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zwkn640t-u8) |
| Multi-Concept Customization of Text-to-Image Diffusion | [![GitHub](https://img.shields.io/github/stars/adobe-research/custom-diffusion)](https://github.com/adobe-research/custom-diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04488-b31b1b.svg)](http://arxiv.org/abs/2212.04488) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WWNA_IPLO84) |
| 3D-Aware Conditional Image Synthesis | [![GitHub](https://img.shields.io/github/stars/dunbar12138/pix2pix3D)](https://github.com/dunbar12138/pix2pix3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Deng_3D-Aware_Conditional_Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.08509-b31b1b.svg)](http://arxiv.org/abs/2302.08509)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dDRoI1gjbzk) |
| QuantArt: Quantizing Image Style Transfer Towards High Visual Fidelity | [![GitHub](https://img.shields.io/github/stars/siyuhuang/QuantArt)](https://github.com/siyuhuang/QuantArt) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_QuantArt_Quantizing_Image_Style_Transfer_Towards_High_Visual_Fidelity_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.10431-b31b1b.svg)](http://arxiv.org/abs/2212.10431) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WWNA_IPLO84) |
| SceneComposer: Any-Level Semantic Image Synthesis <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() |  [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zengyu.me/scenec/) <br /> [![GitHub](https://img.shields.io/github/stars/zengxianyu/scenec)](https://github.com/zengxianyu/scenec)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zeng_SceneComposer_Any-Level_Semantic_Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11742-b31b1b.svg)](http://arxiv.org/abs/2211.11742) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ky0LWZ_USRA) |
| DiffCollage: Parallel Generation of Large Content with Diffusion Models |  |  |  |
| Putting People in Their Place: Affordance-Aware Human Insertion into Scenes |  |  |  |
| Hybrid Neural Rendering for Large-Scale Scenes with Motion Blur |  |  |  |
| Binary Latent Diffusion |  |  |  |
| StyleRes: Transforming the Residuals for Real Image Editing with StyleGAN |  |  |  |
| KD-DLGAN: Data Limited Image Generation via Knowledge Distillation |  |  |  |
| <i>SeaThru</i>-NeRF: Neural Radiance Fields in Scattering Media |  |  |  |
| PointAvatar: Deformable Point-based Head Avatars from Videos |  |  |  |
| 3DAvatarGAN: Bridging Domains for Personalized Editable Avatars |  |  |  |
| Neural Preset for Color Style Transfer |  |  |  |
| Zero-Shot Generative Model Adaptation via Image-Specific Prompt Learning |  |  |  |
| DyNCA: Real-Time Dynamic Texture Synthesis using Neural Cellular Automata |  |  |  |
| Exploring Incompatible Knowledge Transfer in Few-Shot Image Generation |  |  |  |
| HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising |  |  |  |
| Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization |  |  |  |
| RiDDLE: Reversible and Diversified De-Identification with Latent Encryptor |  |  |  |
| LayoutDiffusion: Controllable Diffusion Model for Layout-to-Image Generation |  |  |  |
| LipFormer: High-Fidelity and Generalizable Talking Face Generation with A Pre-learned Facial Codebook |  |  |  |
| Not All Image Regions Matter: Masked Vector Quantization for Autoregressive Image Generation |  |  |  |
| GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis |  |  |  |
| High-Fidelity Generalized Emotional Talking Face Generation with Multi-Modal Emotion Space Learning |  |  |  |
| Consistent View Synthesis with Pose-guided Diffusion Models |  |  |  |
| StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-based Generator |  |  |  |
| Imagic: Text-based Real Image Editing with Diffusion Models |  |  |  |
| Large-Capacity and Flexible Video Steganography via Invertible Neural Network |  |  |  |
| Quantitative Manipulation of Custom Attributes on 3D-Aware Image Synthesis |  |  |  |
| Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent Portrait Synthesis from Monocular Image |  |  |  |
| CF-Font: Content Fusion for Few-Shot Font Generation |  |  |  |
| One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural Radiance Field |  |  |  |
| Unsupervised Domain Adaption with Pixel-Level Discriminator for Image-Aware Layout Generation |  |  |  |
| Diffusion Probabilistic Model Made Slim | :heavy_minus_sign: | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_Diffusion_Probabilistic_Model_Made_Slim_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.17106-b31b1b.svg)](http://arxiv.org/abs/2211.17106) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=82N6FsRUfr4) |
| Collaborative Diffusion for Multi-Modal Face Generation and Editing |  |  |  |
| High-Fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors |  |  |  |
| Network-Free, Unsupervised Semantic Segmentation with Synthetic Images |  |  |  |
| Visual Prompt Tuning for Generative Transfer Learning |  |  |  |
| Specialist Diffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models to Learn Any Unseen Style |  |  |  |
| Catch Missing Details: Image Reconstruction with Frequency Augmented Variational Autoencoder |  |  |  |
| Towards Bridging the Performance Gaps of Joint Energy-based Models |  |  |  |
| GLeaD: Improving GANs with a Generator-Leading Task |  |  |  |
| Structural Multiplane Image: Bridging Neural View Synthesis and 3D Reconstruction |  |  |  |
| SPARF: Neural Radiance Fields from Sparse and Noisy Poses |  |  |  |
| DeltaEdit: Exploring Text-Free Training for Text-Driven Image Manipulation |  |  |  |
| Inferring and Leveraging Parts from Object Shape for Improving Semantic Image Synthesis |  |  |  |
| VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation |  |  |  |
| MaskSketch: Unpaired Structure-guided Masked Image Generation |  |  |  |
| Affordance Diffusion: Synthesizing Hand-Object Interactions |  |  |  |
| Interactive Cartoonization with Controllable Perceptual Factors |  |  |  |
| MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation |  |  |  |
| Paint by Example: Exemplar-based Image Editing with Diffusion Models |  |  |  |
| GLIGEN: Open-Set Grounded Text-to-Image Generation |  |  |  |
| L-CoIns: Language-based Colorization with Instance Awareness |  |  |  |
| DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation |  |  |  |
| Evading DeepFake Detectors via Adversarial Statistical Consistency |  |  |  |
| GlassesGAN: Eyewear Personalization using Synthetic Appearance Discovery and Targeted Subspace Modeling |  |  |  |
| GP-VTON: Towards General Purpose Virtual Try-on via Collaborative Local-Flow Global-Parsing Learning |  |  |  |
| Where is My Spot? Few-Shot Image Generation via Latent Subspace Optimization |  |  |  |
| Regularized Vector Quantization for Tokenized Image Synthesis |  |  |  |
| EDICT: Exact Diffusion Inversion via Coupled Transformations |  |  |  |
| Scaling up GANs for Text-to-Image Synthesis |  |  |  |
| Shape-Aware Text-Driven Layered Video Editing |  |  |  |
| A Unified Pyramid Recurrent Network for Video Frame Interpolation |  |  |  |
| TAPS3D: Text-guided 3D Textured Shape Generation from Pseudo Supervision |  |  |  |
| Fine-grained Face Swapping via Regional GAN Inversion |  |  |  |
| OTAvatar: One-Shot Talking Face Avatar with Controllable Tri-Plane Rendering |  |  |  |
| Deep Stereo Video Inpainting |  |  |  |
| StyleGAN Salon: Multi-View Latent Optimization for Pose-Invariant Hairstyle Transfer |  |  |  |
| Cross-GAN Auditing: Unsupervised Identification of Attribute Level Similarities and Differences between Pretrained Generative Models |  |  |  |
| Unsupervised Volumetric Animation |  |  |  |
| SINE: SINgle Image Editing with Text-to-Image Diffusion Models |  |  |  |
| Progressive Disentangled Representation Learning for Fine-grained Controllable Talking Head Synthesis |  |  |  |
| CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer |  |  |  |
| DeepVecFont-v2: Exploiting Transformers to Synthesize Vector Fonts with Higher Quality |  |  |  |
| LEMaRT: Label-Efficient Masked Region Transform for Image Harmonization |  |  |  |
| SINE: Semantic-Driven Image-based NeRF Editing with Prior-guided Editing Field |  |  |  |
| Exploring Intra-Class Variation Factors with Learnable Cluster Prompts for Semi-Supervised Image Synthesis |  |  |  |
| Image Cropping with Spatial-Aware Feature and Rank Consistency |  |  |  |
| <i>Picture that Sketch</i>: Photorealistic Image Generation from Abstract Sketches |  |  |  |
| MonoHuman: Animatable Human Neural Field from Monocular Video |  |  |  |
| PixHt-Lab: Pixel Height based Light Effect Generation for Image Compositing |  |  |  |
| Neural Pixel Composition for 3D-4D View Synthesis from Multi-Views |  |  |  |
| SpaText: Spatio-Textual Representation for Controllable Image Generation |  |  |  |
| Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation |  |  |  |
| MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation |  |  |  |
| Synthesizing Photorealistic Virtual Humans Through Cross-Modal Disentanglement |  |  |  |
| Video Probabilistic Diffusion Models in Projected Latent Space |  |  |  |
| Variational Distribution Learning for Unsupervised Text-to-Image Generation |  |  |  |
| Linking Garment with Person via Semantically Associated Landmarks for Virtual Try-On |  |  |  |
| UV Volumes for Real-Time Rendering of Editable Free-View Human Performance |  |  |  |
| Null-Text Inversion for Editing Real Images using Guided Diffusion Models |  |  |  |
| Polynomial Implicit Neural Representations for Large Diverse Datasets |  |  |  |
| Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation |  |  |  |
| Conditional Image-to-Video Generation with Latent Flow Diffusion Models |  |  |  |
| Local 3D Editing via 3D Distillation of CLIP Knowledge |  |  |  |
| Private Image Generation with Dual-Purpose Auxiliary Classifier |  |  |  |
| MAGVIT: Masked Generative Video Transformer |  |  |  |
| Dimensionality-Varying Diffusion Process |  |  |  |
| VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs |  |  |  |
| LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data |  |  |  |
| DATID-3D: Diversity-Preserved Domain Adaptation using Text-to-Image Diffusion for 3D Generative Model |  |  |  |
| Delving StyleGAN Inversion for Image Editing: A Foundation Latent Space Viewpoint |  |  |  |
| High-Fidelity and Freely Controllable Talking Head Video Generation |  |  |  |
| SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation |  |  |  |
| StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields |  |  |  |
| MOSO: Decomposing MOtion, Scene and Object for Video Prediction |  |  |  |
| Multi Domain Learning for Motion Magnification |  |  |  |
| GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields |  |  |  |
| Hierarchical B-frame Video Coding using Two-Layer CANF without Motion Coding |  |  |  |
| Blemish-Aware and Progressive Face Retouching with Limited Paired Data |  |  |  |
| Text-guided Unsupervised Latent Transformation for Multi-attribute Image Manipulation |  |  |  |
| NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models |  |  |  |
| Fix the Noise: Disentangling Source Feature for Controllable Domain Translation |  |  |  |
| Class-Balancing Diffusion Models |  |  |  |
| DPE: Disentanglement of Pose and Expression for General Video Portrait Editing |  |  |  |
| Inversion-based Style Transfer with Diffusion Models |  |  |  |
| Deep Curvilinear Editing: Commutative and Nonlinear Image Manipulation for Pretrained Deep Generative Model |  |  |  |
| FlowGrad: Controlling the Output of Generative ODEs with Gradients |  |  |  |
| Graph Transformer GANs for Graph-Constrained House Generation |  |  |  |
| Master: Meta Style Transformer for Controllable Zero-Shot and Few-Shot Artistic Style Transfer |  |  |  |
| Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars |  |  |  |
| Ham2Pose: Animating Sign Language Notation into Pose Sequences |  |  |  |
| Neural Transformation Fields for Arbitrary-Styled Font Generation |  |  |  |
| LayoutDM: Transformer-based Diffusion Model for Layout Generation |  |  |  |
| Removing Objects from Neural Radiance Fields |  |  |  |
| Person Image Synthesis via Denoising Diffusion Model |  |  |  |
| AdaptiveMix: Improving GAN Training via Feature Space Shrinkage |  |  |  |
| Learning Joint Latent Space EBM Prior Model for Multi-Layer Generator |  |  |  |
| 3D Neural Field Generation using Triplane Diffusion|  |  |  |
| OmniAvatar: Geometry-guided Controllable 3D Head Synthesis |  |  |  |
| RWSC-Fusion: Region-Wise Style-Controlled Fusion Network for the Prohibited X-ray Security Image Synthesis |  |  |  |
| ObjectStitch: Object Compositing with Diffusion Model |  |  |  |
| Persistent Nature: A Generative Model of Unbounded 3D Worlds |  |  |  |
| Masked and Adaptive Transformer for Exemplar based Image Translation |  |  |  |
| Spider GAN: Leveraging Friendly Neighbors to Accelerate GAN Training |  |  |  |
| Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild |  |  |  |
| Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models |  |  |  |
| All are Worth Words: A ViT Backbone for Diffusion Models |  |  |  |
| Few-Shot Semantic Image Synthesis with Class Affinity Transfer |  |  |  |
| Blowing in the Wind: CycleNet for Human Cinemagraphs from Still Images |  |  |  |
| StyleGene: Crossover and Mutation of Region-Level Facial Genes for Kinship Face Synthesis |  |  |  |
| MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs |  |  |  |
| MoStGAN-V: Video Generation with Temporal Motion Styles |  |  |  |
| Frame Interpolation Transformer and Uncertainty Guidance |  |  |  |
| Towards End-to-End Generative Modeling of Long Videos with Memory-Efficient Bidirectional Transformers |  |  |  |
| HOLODIFFUSION: Training a 3D Diffusion Model using 2D Images |  |  |  |
| Neural Texture Synthesis with Guided Correspondence |  |  |  |
| PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360&deg; |  |  |  |
| InstructPix2Pix: Learning to Follow Image Editing Instructions |  |  |  |
| Unpaired Image-to-Image Translation with Shortest Path Regularization |  |  |  |
| Freestyle Layout-to-Image Synthesis |  |  |  |
| On Distillation of Guided Diffusion Models |  |  |  |
| Single Image Backdoor Inversion via Robust Smoothed Classifiers |  |  |  |
| Make-a-Story: Visual Memory Conditioned Consistent Story Generation |  |  |  |
| Towards Practical Plug-and-Play Diffusion Models |  |  |  |
| Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis |  |  |  |
| Wavelet Diffusion Models are Fast and Scalable Image Generators |  |  |  |
| 3D GAN Inversion with Facial Symmetry Prior |  |  |  |
| Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert |  |  |  |
| PCT-Net: Full Resolution Image Harmonization Using Pixel-Wise Color Transformations |  |  |  |
| ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts |  |  |  |
| Video Compression with Entropy-Constrained Neural Representations |  |  |  |
| Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models |  |  |  |
| CoralStyleCLIP: Co-optimized Region and Layer Selection for Image Editing |  |  |  |
| Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding |  |  |  |
| Sequential Training of GANs Against GAN-classifiers Reveals Correlated ``Knowledge Gaps`` Present among Independently Trained GAN Instances |  |  |  |
| Attribute-Preserving Face Dataset Anonymization via Latent Code Optimization |  |  |  |
| Shifted Diffusion for Text-to-Image Generation |  |  |  |
| HandsOff: Labeled Dataset Generation with no Additional Human Annotations |  |  |  |
| Lookahead Diffusion Probabilistic Models for Refining Mean Estimation |  |  |  |
| Imagen Editor and EditBench: Advancing and Evaluating Text-guided Image Inpainting |  |  |  |
| Re-GAN: Data-Efficient GANs Training via Architectural Reconfiguration |  |  |  |
| BBDM: Image-to-Image Translation with Brownian Bridge Diffusion Models |  |  |  |
| VectorFusion: Text-to-SVG by Abstracting Pixel-based Diffusion Models |  |  |  |
