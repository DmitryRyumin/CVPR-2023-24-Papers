# CVPR-2023-Papers

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/3d-from-multi-view-and-sensors.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/humans-face-body-pose-gesture-movement.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" />
    </a>
</div>

## Image and Video Synthesis and Generation

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Towards Universal Fake Image Detectors That Generalize Across Generative Models | [![GitHub](https://img.shields.io/github/stars/Yuheng-Li/UniversalFakeDetect)](https://github.com/Yuheng-Li/UniversalFakeDetect)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ojha_Towards_Universal_Fake_Image_Detectors_That_Generalize_Across_Generative_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.10174-b31b1b.svg)](http://arxiv.org/abs/2302.10174)|:heavy_minus_sign: |
| Implicit Diffusion Models for Continuous Super-Resolution | [![GitHub](https://img.shields.io/github/stars/Ree1s/IDM)](https://github.com/Ree1s/IDM)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_Implicit_Diffusion_Models_for_Continuous_Super-Resolution_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16491-b31b1b.svg)](http://arxiv.org/abs/2303.16491)| :heavy_minus_sign: | 
| High-Fidelity Guided Image Synthesis With Latent Diffusion Models |  [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://1jsingh.github.io/gradop) <br /> [![GitHub](https://img.shields.io/github/stars/1jsingh/GradOP-Guided-Image-Synthesis)](https://github.com/1jsingh/GradOP-Guided-Image-Synthesis) |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Singh_High-Fidelity_Guided_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.17084-b31b1b.svg)](http://arxiv.org/abs/2211.17084)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Yk83RPCOa2o) |
| DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields | [![GitHub](https://img.shields.io/github/stars/AIBluefisher/dbarf)](https://github.com/AIBluefisher/dbarf)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_DBARF_Deep_Bundle-Adjusting_Generalizable_Neural_Radiance_Fields_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14478-b31b1b.svg)](http://arxiv.org/abs/2303.14478)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wFPO403wtAg) | 
| Deep Arbitrary-Scale Image Super-Resolution via Scale-Equivariance Pursuit | [![GitHub](https://img.shields.io/github/stars/neuralchen/EQSR)](https://github.com/neuralchen/EQSR)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Deep_Arbitrary-Scale_Image_Super-Resolution_via_Scale-Equivariance_Pursuit_CVPR_2023_paper.pdf) |[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Pq9eI5kxqUE) |
| Balanced Spherical Grid for Egocentric View Synthesis | [![GitHub](https://img.shields.io/github/stars/changwoonchoi/EgoNeRF)](https://github.com/changwoonchoi/EgoNeRF)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Choi_Balanced_Spherical_Grid_for_Egocentric_View_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12408-b31b1b.svg)](http://arxiv.org/abs/2303.12408)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=D-lsBhVP8zw) |
| SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation | [![GitHub](https://img.shields.io/github/stars/yccyenchicheng/SDFusion)](https://github.com/yccyenchicheng/SDFusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cheng_SDFusion_Multimodal_3D_Shape_Completion_Reconstruction_and_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04493-b31b1b.svg)](http://arxiv.org/abs/2212.04493) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6EvHJRlUMFQ) |
| DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation <br/> [![CVPR - Award](https://img.shields.io/badge/CVPR-Award-294A7C)]() | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dreambooth.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.12242-b31b1b.svg)](http://arxiv.org/abs/2208.12242) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=82x4XTSFwBQ) |
| Self-Guided Diffusion Models | [![GitHub](https://img.shields.io/github/stars/dongzhuoyao/self-guided-diffusion-models)](https://github.com/dongzhuoyao/self-guided-diffusion-models) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hu_Self-Guided_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.06462-b31b1b.svg)](http://arxiv.org/abs/2210.06462) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zwkn640t-u8) |
| Multi-Concept Customization of Text-to-Image Diffusion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://www.cs.cmu.edu/~custom-diffusion/dataset.html) <br /> [![GitHub](https://img.shields.io/github/stars/adobe-research/custom-diffusion)](https://github.com/adobe-research/custom-diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04488-b31b1b.svg)](http://arxiv.org/abs/2212.04488) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WWNA_IPLO84) |
| 3D-Aware Conditional Image Synthesis | [![GitHub](https://img.shields.io/github/stars/dunbar12138/pix2pix3D)](https://github.com/dunbar12138/pix2pix3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Deng_3D-Aware_Conditional_Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.08509-b31b1b.svg)](http://arxiv.org/abs/2302.08509)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dDRoI1gjbzk) |
| QuantArt: Quantizing Image Style Transfer Towards High Visual Fidelity | [![GitHub](https://img.shields.io/github/stars/siyuhuang/QuantArt)](https://github.com/siyuhuang/QuantArt) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_QuantArt_Quantizing_Image_Style_Transfer_Towards_High_Visual_Fidelity_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.10431-b31b1b.svg)](http://arxiv.org/abs/2212.10431) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WWNA_IPLO84) |
| SceneComposer: Any-Level Semantic Image Synthesis <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() |  [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zengyu.me/scenec/) <br /> [![GitHub](https://img.shields.io/github/stars/zengxianyu/scenec)](https://github.com/zengxianyu/scenec)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zeng_SceneComposer_Any-Level_Semantic_Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11742-b31b1b.svg)](http://arxiv.org/abs/2211.11742) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ky0LWZ_USRA) |
| DiffCollage: Parallel Generation of Large Content With Diffusion Models | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://research.nvidia.com/labs/dir/diffcollage/) |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_DiffCollage_Parallel_Generation_of_Large_Content_With_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17076-b31b1b.svg)](http://arxiv.org/abs/2303.17076) | :heavy_minus_sign: |
| Putting People in Their Place: Affordance-Aware Human Insertion Into Scenes | [![GitHub](https://img.shields.io/github/stars/adobe-research/affordance-insertion)](https://github.com/adobe-research/affordance-insertion)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kulal_Putting_People_in_Their_Place_Affordance-Aware_Human_Insertion_Into_Scenes_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.14406-b31b1b.svg)](http://arxiv.org/abs/2304.14406) | :heavy_minus_sign: |
| Hybrid Neural Rendering for Large-Scale Scenes With Motion Blur | [![GitHub](https://img.shields.io/github/stars/CVMI-Lab/HybridNeuralRendering)](https://github.com/CVMI-Lab/HybridNeuralRendering)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Dai_Hybrid_Neural_Rendering_for_Large-Scale_Scenes_With_Motion_Blur_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.12652-b31b1b.svg)](http://arxiv.org/abs/2304.12652) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hAhFfKRqDgE) |
| Binary Latent Diffusion | [![GitHub](https://img.shields.io/github/stars/ZeWang95/BinaryLatentDiffusion)](https://github.com/ZeWang95/BinaryLatentDiffusion)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Binary_Latent_Diffusion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04820-b31b1b.svg)](http://arxiv.org/abs/2304.04820) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/channel/UCzkUNNsV1TYuf6U_wGnMlnw) |
| StyleRes: Transforming the Residuals for Real Image Editing With StyleGAN | [![GitHub](https://img.shields.io/github/stars/hamzapehlivan/StyleRes)](https://github.com/hamzapehlivan/StyleRes)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Pehlivan_StyleRes_Transforming_the_Residuals_for_Real_Image_Editing_With_StyleGAN_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.14359-b31b1b.svg)](http://arxiv.org/abs/2212.14359) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=S9ZswKv8enw) |
| KD-DLGAN: Data Limited Image Generation via Knowledge Distillation | [![GitHub](https://img.shields.io/github/stars/cuikaiwen18/KD_DLGAN)](https://github.com/cuikaiwen18/KD_DLGAN)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cui_KD-DLGAN_Data_Limited_Image_Generation_via_Knowledge_Distillation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17158-b31b1b.svg)](https://arxiv.org/abs/2303.17158) | :heavy_minus_sign: |
| SeaThru-NeRF: Neural Radiance Fields in Scattering Media | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sea-thru-nerf.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/deborahLevy130/seathru_NeRF)](https://github.com/deborahLevy130/seathru_NeRF)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Levy_SeaThru-NeRF_Neural_Radiance_Fields_in_Scattering_Media_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.07743-b31b1b.svg)](https://arxiv.org/abs/2304.07743) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dXKCJS4cscg) |
| PointAvatar: Deformable Point-Based Head Avatars From Videos | [![GitHub](https://img.shields.io/github/stars/zhengyuf/PointAvatar)](https://github.com/zhengyuf/PointAvatar)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zheng_PointAvatar_Deformable_Point-Based_Head_Avatars_From_Videos_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08377-b31b1b.svg)](http://arxiv.org/abs/2212.08377) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wll_XtgpU7U) |
| 3DAvatarGAN: Bridging Domains for Personalized Editable Avatars | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rameenabdal.github.io/3DAvatarGAN/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Abdal_3DAvatarGAN_Bridging_Domains_for_Personalized_Editable_Avatars_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.02700-b31b1b.svg)](http://arxiv.org/abs/2301.02700) | :heavy_minus_sign: |
| Neural Preset for Color Style Transfer | [![GitHub](https://img.shields.io/github/stars/ZHKKKe/NeuralPreset)](https://github.com/ZHKKKe/NeuralPreset)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ke_Neural_Preset_for_Color_Style_Transfer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13511-b31b1b.svg)](http://arxiv.org/abs/2303.13511) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=x6fLAvTPesk) |
| Zero-Shot Generative Model Adaptation via Image-Specific Prompt Learning | [![GitHub](https://img.shields.io/github/stars/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation)](https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Guo_Zero-Shot_Generative_Model_Adaptation_via_Image-Specific_Prompt_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03119-b31b1b.svg)](http://arxiv.org/abs/2304.03119) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vw9-C3Sz5nM) |
| DyNCA: Real-Time Dynamic Texture Synthesis Using Neural Cellular Automata | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dynca.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/IVRL/DyNCA)](https://github.com/IVRL/DyNCA)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Pajouheshgar_DyNCA_Real-Time_Dynamic_Texture_Synthesis_Using_Neural_Cellular_Automata_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11417-b31b1b.svg)](http://arxiv.org/abs/2211.11417)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ELZC2mX5Z9U) |
| Exploring Incompatible Knowledge Transfer in Few-Shot Image Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yunqing-me.github.io/RICK/) <br /> [![GitHub](https://img.shields.io/github/stars/yunqing-me/RICK)](https://github.com/yunqing-me/RICK) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhao_Exploring_Incompatible_Knowledge_Transfer_in_Few-Shot_Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.07574-b31b1b.svg)](http://arxiv.org/abs/2304.07574) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=s14bA8filtw) |
| HouseDiffusion: Vector Floorplan Generation via a Diffusion Model With Discrete and Continuous Denoising | [![GitHub](https://img.shields.io/github/stars/aminshabani/house_diffusion)](https://github.com/aminshabani/house_diffusion)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shabani_HouseDiffusion_Vector_Floorplan_Generation_via_a_Diffusion_Model_With_Discrete_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13287-b31b1b.svg)](http://arxiv.org/abs/2211.13287) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ku6_gr94n5Q) |
| Towards Accurate Image Coding: Improved Autoregressive Image Generation With Dynamic Vector Quantization <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/CrossmodalGroup/DynamicVectorQuantization)](https://github.com/CrossmodalGroup/DynamicVectorQuantization) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Towards_Accurate_Image_Coding_Improved_Autoregressive_Image_Generation_With_Dynamic_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.11718-b31b1b.svg)](http://arxiv.org/abs/2305.11718) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ir60YW9JCjU) |
| RiDDLE: Reversible and Diversified De-Identification With Latent Encryptor | [![GitHub](https://img.shields.io/github/stars/ldz666666/RiDDLE)](https://github.com/ldz666666/RiDDLE)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_RiDDLE_Reversible_and_Diversified_De-Identification_With_Latent_Encryptor_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05171-b31b1b.svg)](http://arxiv.org/abs/2303.05171) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=gqs9Q6ReEn0) |
| LayoutDiffusion: Controllable Diffusion Model for Layout-to-Image Generation | [![GitHub](https://img.shields.io/github/stars/ZGCTroy/LayoutDiffusion)](https://github.com/ZGCTroy/LayoutDiffusion)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zheng_LayoutDiffusion_Controllable_Diffusion_Model_for_Layout-to-Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17189-b31b1b.svg)](http://arxiv.org/abs/2303.17189) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bJOpJnvhw3s) |
| LipFormer: High-Fidelity and Generalizable Talking Face Generation With a Pre-Learned Facial Codebook | [![GitHub](https://img.shields.io/github/stars/DaddyJin/awesome-faceReenactment)](https://github.com/DaddyJin/awesome-faceReenactment)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_LipFormer_High-Fidelity_and_Generalizable_Talking_Face_Generation_With_a_Pre-Learned_CVPR_2023_paper.pdf)  | :heavy_minus_sign: |
| Not All Image Regions Matter: Masked Vector Quantization for Autoregressive Image Generation | [![GitHub](https://img.shields.io/github/stars/CrossmodalGroup/MaskedVectorQuantization)](https://github.com/CrossmodalGroup/MaskedVectorQuantization)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Not_All_Image_Regions_Matter_Masked_Vector_Quantization_for_Autoregressive_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.13607-b31b1b.svg)](http://arxiv.org/abs/2305.13607) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=o2eyRscEejw) |
| GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis | [![GitHub](https://img.shields.io/github/stars/tobran/GALIP)](https://github.com/tobran/GALIP)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tao_GALIP_Generative_Adversarial_CLIPs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.12959-b31b1b.svg)](http://arxiv.org/abs/2301.12959)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lwhTDY4du_g) |
| High-Fidelity Generalized Emotional Talking Face Generation With Multi-Modal Emotion Space Learning | :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_High-Fidelity_Generalized_Emotional_Talking_Face_Generation_With_Multi-Modal_Emotion_Space_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.02572-b31b1b.svg)](http://arxiv.org/abs/2305.02572) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=eQG6ql83T0w) |
| Consistent View Synthesis With Pose-Guided Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://poseguided-diffusion.github.io/)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tseng_Consistent_View_Synthesis_With_Pose-Guided_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17598-b31b1b.svg)](http://arxiv.org/abs/2303.17598) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://youtu.be/eV1jwq14lE0) |
| StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-Based Generator | [![GitHub](https://img.shields.io/github/stars/guanjz20/StyleSync)](https://github.com/guanjz20/StyleSync)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Guan_StyleSync_High-Fidelity_Generalized_and_Personalized_Lip_Sync_in_Style-Based_Generator_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.05445-b31b1b.svg)](http://arxiv.org/abs/2305.05445) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=yAPDl2dVonY) |
| Imagic: Text-Based Real Image Editing With Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://poseguided-diffusion.github.io/)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kawar_Imagic_Text-Based_Real_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.09276-b31b1b.svg)](http://arxiv.org/abs/2210.09276) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lOZvBGz47wQ) |
| Large-Capacity and Flexible Video Steganography via Invertible Neural Network | [![GitHub](https://img.shields.io/github/stars/MC-E/LF-VSN)](https://github.com/MC-E/LF-VSN)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Mou_Large-Capacity_and_Flexible_Video_Steganography_via_Invertible_Neural_Network_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.12300-b31b1b.svg)](http://arxiv.org/abs/2304.12300) | :heavy_minus_sign: |
| Quantitative Manipulation of Custom Attributes on 3D-Aware Image Synthesis | :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Do_Quantitative_Manipulation_of_Custom_Attributes_on_3D-Aware_Image_Synthesis_CVPR_2023_paper.pdf)  | :heavy_minus_sign: |
| Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent Portrait Synthesis From Monocular Image | [![GitHub](https://img.shields.io/github/stars/YuDeng/GRAMInverter)](https://github.com/YuDeng/GRAMInverter)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Deng_Learning_Detailed_Radiance_Manifolds_for_High-Fidelity_and_3D-Consistent_Portrait_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13901-b31b1b.svg)](http://arxiv.org/abs/2211.13901) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nclBOg_CiJo) |
| CF-Font: Content Fusion for Few-Shot Font Generation | [![GitHub](https://img.shields.io/github/stars/wangchi95/CF-Font)](https://github.com/wangchi95/CF-Font)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_CF-Font_Content_Fusion_for_Few-Shot_Font_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14017-b31b1b.svg)](https://arxiv.org/abs/2303.14017) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=biwFd0K3X9o) |
| One-Shot High-Fidelity Talking-Head Synthesis With Deformable Neural Radiance Field | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.waytron.net/hidenerf/)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_One-Shot_High-Fidelity_Talking-Head_Synthesis_With_Deformable_Neural_Radiance_Field_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05097-b31b1b.svg)](http://arxiv.org/abs/2304.05097) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=opLdLY8_VYQ) |
| Unsupervised Domain Adaption With Pixel-Level Discriminator for Image-Aware Layout Generation | :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Unsupervised_Domain_Adaption_With_Pixel-Level_Discriminator_for_Image-Aware_Layout_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14377-b31b1b.svg)](http://arxiv.org/abs/2303.14377) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DBHFzw02T1I) |
| Diffusion Probabilistic Model Made Slim | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_Diffusion_Probabilistic_Model_Made_Slim_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.17106-b31b1b.svg)](http://arxiv.org/abs/2211.17106) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=82N6FsRUfr4) |
| Collaborative Diffusion for Multi-Modal Face Generation and Editing | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ziqihuangg.github.io/projects/collaborative-diffusion.html) <br/> [![GitHub](https://img.shields.io/github/stars/ziqihuangg/Collaborative-Diffusion)](https://github.com/ziqihuangg/Collaborative-Diffusion)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Collaborative_Diffusion_for_Multi-Modal_Face_Generation_and_Editing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10530-b31b1b.svg)](http://arxiv.org/abs/2304.10530)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=inLK4c8sNhc) |
| High-Fidelity Facial Avatar Reconstruction From Monocular Video With Generative Priors | [![GitHub](https://img.shields.io/github/stars/bbaaii/HFA-GP)](https://github.com/bbaaii/HFA-GP)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Bai_High-Fidelity_Facial_Avatar_Reconstruction_From_Monocular_Video_With_Generative_Priors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.15064-b31b1b.svg)](http://arxiv.org/abs/2211.15064)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZWdF8ASl0BQ) |
| Network-Free, Unsupervised Semantic Segmentation With Synthetic Images |  :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Feng_Network-Free_Unsupervised_Semantic_Segmentation_With_Synthetic_Images_CVPR_2023_paper.pdf) <br /> [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/network-free-unsupervised-semantic-segmentation-with-synthetic-images) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mfFZdvaF1Tw) |
| Visual Prompt Tuning for Generative Transfer Learning | [![GitHub](https://img.shields.io/github/stars/google-research/generative_transfer)](https://github.com/google-research/generative_transfer)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sohn_Visual_Prompt_Tuning_for_Generative_Transfer_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.00990-b31b1b.svg)](http://arxiv.org/abs/2210.00990) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kOza8-xop_g) |
| Specialist Diffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models To Learn Any Unseen Style|[![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://specialist-diffusion.github.io/) <br/> [![GitHub](https://img.shields.io/github/stars/Picsart-AI-Research/Specialist-Diffusion)](https://github.com/Picsart-AI-Research/Specialist-Diffusion)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lu_Specialist_Diffusion_Plug-and-Play_Sample-Efficient_Fine-Tuning_of_Text-to-Image_Diffusion_Models_To_CVPR_2023_paper.pdf)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5-hkImpVsNI) |
| Catch Missing Details: Image Reconstruction With Frequency Augmented Variational Autoencoder|[![GitHub](https://img.shields.io/github/stars/JiauZhang/FA-VAE)](https://github.com/JiauZhang/FA-VAE)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_Catch_Missing_Details_Image_Reconstruction_With_Frequency_Augmented_Variational_Autoencoder_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.02541-b31b1b.svg)](http://arxiv.org/abs/2305.02541) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wJ9U7tAnQEo) |
| Towards Bridging the Performance Gaps of Joint Energy-Based Models|[![GitHub](https://img.shields.io/github/stars/sndnyang/sadajem)](https://github.com/sndnyang/sadajem)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_Towards_Bridging_the_Performance_Gaps_of_Joint_Energy-Based_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.07959-b31b1b.svg)](http://arxiv.org/abs/2209.07959) | :heavy_minus_sign: |
| GLeaD: Improving GANs With a Generator-Leading Task|[![GitHub](https://img.shields.io/github/stars/EzioBy/glead)](https://github.com/EzioBy/glead)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Bai_GLeaD_Improving_GANs_With_a_Generator-Leading_Task_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03752-b31b1b.svg)](http://arxiv.org/abs/2212.03752) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bP8Iq_qLuU0) | 
| Structural Multiplane Image: Bridging Neural View Synthesis and 3D Reconstruction|[![GitHub](https://img.shields.io/github/stars/mf-zhang/Structural-MPI)](https://github.com/mf-zhang/Structural-MPI)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Structural_Multiplane_Image_Bridging_Neural_View_Synthesis_and_3D_Reconstruction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05937-b31b1b.svg)](http://arxiv.org/abs/2303.05937) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8Bbl8oZKAOs) |
| SPARF: Neural Radiance Fields From Sparse and Noisy Poses <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() |[![GitHub](https://img.shields.io/github/stars/google-research/sparf)](https://github.com/google-research/sparf)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Truong_SPARF_Neural_Radiance_Fields_From_Sparse_and_Noisy_Poses_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11738-b31b1b.svg)](http://arxiv.org/abs/2211.11738) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_s3_p2Brd_8) |
| DeltaEdit: Exploring Text-Free Training for Text-Driven Image Manipulation|[![GitHub](https://img.shields.io/github/stars/Yueming6568/DeltaEdit)](https://github.com/Yueming6568/DeltaEdit)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lyu_DeltaEdit_Exploring_Text-Free_Training_for_Text-Driven_Image_Manipulation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06285-b31b1b.svg)](http://arxiv.org/abs/2303.06285) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8cdZSbhDMIA) |
| Inferring and Leveraging Parts From Object Shape for Improving Semantic Image Synthesis|[![GitHub](https://img.shields.io/github/stars/csyxwei/iPOSE)](https://github.com/csyxwei/iPOSE)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wei_Inferring_and_Leveraging_Parts_From_Object_Shape_for_Improving_Semantic_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.19547-b31b1b.svg)](https://arxiv.org/abs/2305.19547) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=yVUmjQU9-v4) |
| VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation|:heavy_minus_sign:|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Luo_VideoFusion_Decomposed_Diffusion_Models_for_High-Quality_Video_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08320-b31b1b.svg)](http://arxiv.org/abs/2303.08320)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hxA0DTZScg0) |
| MaskSketch: Unpaired Structure-Guided Masked Image Generation <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() |:heavy_minus_sign:|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Bashkirova_MaskSketch_Unpaired_Structure-Guided_Masked_Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.05496-b31b1b.svg)](http://arxiv.org/abs/2302.05496) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2tBzEGASeo0) |
| Affordance Diffusion: Synthesizing Hand-Object Interactions|[![GitHub](https://img.shields.io/github/stars/NVlabs/affordance_diffusion)](https://github.com/NVlabs/affordance_diffusion)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ye_Affordance_Diffusion_Synthesizing_Hand-Object_Interactions_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12538-b31b1b.svg)](http://arxiv.org/abs/2303.12538) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=omhEoLzsopo) |
| Interactive Cartoonization With Controllable Perceptual Factors|:heavy_minus_sign:|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ahn_Interactive_Cartoonization_With_Controllable_Perceptual_Factors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09555-b31b1b.svg)](http://arxiv.org/abs/2212.09555) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=z8B2RiB4DyM) | |
| MetaPortrait: Identity-Preserving Talking Head Generation With Fast Personalized Adaptation | [![GitHub](https://img.shields.io/github/stars/Meta-Portrait/MetaPortrait)](https://github.com/Meta-Portrait/MetaPortrait)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_MetaPortrait_Identity-Preserving_Talking_Head_Generation_With_Fast_Personalized_Adaptation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08062-b31b1b.svg)](http://arxiv.org/abs/2212.08062) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DDEnjbCNNY4) |
| Paint by Example: Exemplar-Based Image Editing With Diffusion Models | [![GitHub](https://img.shields.io/github/stars/Fantasy-Studio/Paint-by-Example)](https://github.com/Fantasy-Studio/Paint-by-Example)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_Paint_by_Example_Exemplar-Based_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13227-b31b1b.svg)](http://arxiv.org/abs/2211.13227) | :heavy_minus_sign: |
| GLIGEN: Open-Set Grounded Text-to-Image Generation | [![GitHub](https://img.shields.io/github/stars/gligen/GLIGEN)](https://github.com/gligen/GLIGEN)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_GLIGEN_Open-Set_Grounded_Text-to-Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.07093-b31b1b.svg)](http://arxiv.org/abs/2301.07093) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-MCkU7IAGKs) |
| L-CoIns: Language-Based Colorization With Instance Awareness | [![GitHub](https://img.shields.io/github/stars/changzheng123/L-CoIns)](https://github.com/changzheng123/L-CoIns)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chang_L-CoIns_Language-Based_Colorization_With_Instance_Awareness_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation | [![GitHub](https://img.shields.io/github/stars/sstzal/DiffTalk)](https://github.com/sstzal/DiffTalk)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shen_DiffTalk_Crafting_Diffusion_Models_for_Generalized_Audio-Driven_Portraits_Animation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.03786-b31b1b.svg)](http://arxiv.org/abs/2301.03786) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tup5kbsOJXc) |
| Evading DeepFake Detectors via Adversarial Statistical Consistency |  :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hou_Evading_DeepFake_Detectors_via_Adversarial_Statistical_Consistency_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.11670-b31b1b.svg)](http://arxiv.org/abs/2304.11670) | :heavy_minus_sign: |
| GlassesGAN: Eyewear Personalization Using Synthetic Appearance Discovery and Targeted Subspace Modeling | :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Plesh_GlassesGAN_Eyewear_Personalization_Using_Synthetic_Appearance_Discovery_and_Targeted_Subspace_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.14145-b31b1b.svg)](http://arxiv.org/abs/2210.14145) |  [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=oMiV__LWV4A) |
| GP-VTON: Towards General Purpose Virtual Try-On via Collaborative Local-Flow Global-Parsing Learning | [![GitHub](https://img.shields.io/github/stars/xiezhy6/GP-VTON)](https://github.com/xiezhy6/GP-VTON)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xie_GP-VTON_Towards_General_Purpose_Virtual_Try-On_via_Collaborative_Local-Flow_Global-Parsing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13756-b31b1b.svg)](https://arxiv.org/abs/2303.13756) |  [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=b-FDMJ0jrw0) |
| Where Is My Spot? Few-Shot Image Generation via Latent Subspace Optimization | [![GitHub](https://img.shields.io/github/stars/chansey0529/LSO)](https://github.com/chansey0529/LSO) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zheng_Where_Is_My_Spot_Few-Shot_Image_Generation_via_Latent_Subspace_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Regularized Vector Quantization for Tokenized Image Synthesis |  :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Regularized_Vector_Quantization_for_Tokenized_Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06424-b31b1b.svg)](http://arxiv.org/abs/2303.06424) |  :heavy_minus_sign: |
| EDICT: Exact Diffusion Inversion via Coupled Transformations | [![GitHub](https://img.shields.io/github/stars/salesforce/EDICT)](https://github.com/salesforce/EDICT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wallace_EDICT_Exact_Diffusion_Inversion_via_Coupled_Transformations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12446-b31b1b.svg)](http://arxiv.org/abs/2211.12446) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2k6DiE_h1eY) |
| Scaling Up GANs for Text-to-Image Synthesis <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mingukkang.github.io/GigaGAN/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kang_Scaling_Up_GANs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05511-b31b1b.svg)](http://arxiv.org/abs/2303.05511) |  [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZjxtuDQkOPY) |
| Shape-Aware Text-Driven Layered Video Editing | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://text-video-edit.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lee_Shape-Aware_Text-Driven_Layered_Video_Editing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.13173-b31b1b.svg)](http://arxiv.org/abs/2301.13173) | :heavy_minus_sign: |
| A Unified Pyramid Recurrent Network for Video Frame Interpolation | [![GitHub](https://img.shields.io/github/stars/srcn-ivl/UPR-Net)](https://github.com/srcn-ivl/UPR-Net) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jin_A_Unified_Pyramid_Recurrent_Network_for_Video_Frame_Interpolation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.03456-b31b1b.svg)](http://arxiv.org/abs/2211.03456) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=clvrjUKgfhI) |
| TAPS3D: Text-Guided 3D Textured Shape Generation From Pseudo Supervision | [![GitHub](https://img.shields.io/github/stars/plusmultiply/TAPS3D)](https://github.com/plusmultiply/TAPS3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wei_TAPS3D_Text-Guided_3D_Textured_Shape_Generation_From_Pseudo_Supervision_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13273-b31b1b.svg)](http://arxiv.org/abs/2303.13273) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-eWBEwAkThA) |
| Fine-Grained Face Swapping via Regional GAN Inversion | [![GitHub](https://img.shields.io/github/stars/e4s2022/e4s)](https://github.com/e4s2022/e4s)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_Fine-Grained_Face_Swapping_via_Regional_GAN_Inversion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14068-b31b1b.svg)](http://arxiv.org/abs/2211.14068) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Z-cmKVeXHvY) |
| OTAvatar: One-Shot Talking Face Avatar With Controllable Tri-Plane Rendering | [![GitHub](https://img.shields.io/github/stars/theEricMa/OTAvatar)](https://github.com/theEricMa/OTAvatar) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ma_OTAvatar_One-Shot_Talking_Face_Avatar_With_Controllable_Tri-Plane_Rendering_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14662-b31b1b.svg)](http://arxiv.org/abs/2303.14662) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qpIoMYFr7Aw) |
| Deep Stereo Video Inpainting | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Deep_Stereo_Video_Inpainting_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| StyleGAN Salon: Multi-View Latent Optimization for Pose-Invariant Hairstyle Transfer | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://stylegan-salon.github.io/)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Khwanmuang_StyleGAN_Salon_Multi-View_Latent_Optimization_for_Pose-Invariant_Hairstyle_Transfer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02744-b31b1b.svg)](http://arxiv.org/abs/2304.02744) | :heavy_minus_sign: |
| Cross-GAN Auditing: Unsupervised Identification of Attribute Level Similarities and Differences Between Pretrained Generative Models | [![GitHub](https://img.shields.io/github/stars/mattolson93/cross_gan_auditing)](https://github.com/mattolson93/cross_gan_auditing) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Olson_Cross-GAN_Auditing_Unsupervised_Identification_of_Attribute_Level_Similarities_and_Differences_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10774-b31b1b.svg)](http://arxiv.org/abs/2303.10774) | :heavy_minus_sign: |
| Unsupervised Volumetric Animation | [![GitHub](https://img.shields.io/github/stars/snap-research/unsupervised-volumetric-animation)](https://github.com/snap-research/unsupervised-volumetric-animation)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Siarohin_Unsupervised_Volumetric_Animation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.11326-b31b1b.svg)](http://arxiv.org/abs/2301.11326) | :heavy_minus_sign: |
| SINE: SINgle Image Editing With Text-to-Image Diffusion Models | [![GitHub](https://img.shields.io/github/stars/zhang-zx/SINE)](https://github.com/zhang-zx/SINE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_SINE_SINgle_Image_Editing_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04489-b31b1b.svg)](http://arxiv.org/abs/2212.04489) | :heavy_minus_sign: |
| Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis | [![GitHub](https://img.shields.io/github/stars/Dorniwang/PD-FGC-inference)](https://github.com/Dorniwang/PD-FGC-inference)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Progressive_Disentangled_Representation_Learning_for_Fine-Grained_Controllable_Talking_Head_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14506-b31b1b.svg)](http://arxiv.org/abs/2211.14506) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PdSQt_zNbC4) |
| CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer | [![GitHub](https://img.shields.io/github/stars/linfengWen98/CAP-VSTNet)](https://github.com/linfengWen98/CAP-VSTNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wen_CAP-VSTNet_Content_Affinity_Preserved_Versatile_Style_Transfer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17867-b31b1b.svg)](https://arxiv.org/abs/2303.17867) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://m.youtube.com/watch?v=OTJ1wEe29Hc) |
| DeepVecFont-v2: Exploiting Transformers To Synthesize Vector Fonts With Higher Quality | [![GitHub](https://img.shields.io/github/stars/yizhiwang96/deepvecfont-v2)](https://github.com/yizhiwang96/deepvecfont-v2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_DeepVecFont-v2_Exploiting_Transformers_To_Synthesize_Vector_Fonts_With_Higher_Quality_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14585-b31b1b.svg)](https://arxiv.org/abs/2303.14585) | :heavy_minus_sign: |
| LEMaRT: Label-Efficient Masked Region Transform for Image Harmonization | [![GitHub](https://img.shields.io/github/stars/yizhiwang96/deepvecfont-v2)](https://github.com/yizhiwang96/deepvecfont-v2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_LEMaRT_Label-Efficient_Masked_Region_Transform_for_Image_Harmonization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.13166-b31b1b.svg)](http://arxiv.org/abs/2304.13166) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xSS4RChu7zk) |
| SINE: Semantic-Driven Image-Based NeRF Editing With Prior-Guided Editing Field | [![GitHub](https://img.shields.io/github/stars/zju3dv/SINE)](https://github.com/zju3dv/SINE)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Bao_SINE_Semantic-Driven_Image-Based_NeRF_Editing_With_Prior-Guided_Editing_Field_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13277-b31b1b.svg)](http://arxiv.org/abs/2303.13277) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=AfAR-PoZ8SM) |
| Exploring Intra-Class Variation Factors With Learnable Cluster Prompts for Semi-Supervised Image Synthesis |  :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Exploring_Intra-Class_Variation_Factors_With_Learnable_Cluster_Prompts_for_Semi-Supervised_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Image Cropping With Spatial-Aware Feature and Rank Consistency | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Image_Cropping_With_Spatial-Aware_Feature_and_Rank_Consistency_CVPR_2023_paper.pdf) <br /> [![thecvf](https://img.shields.io/badge/sup-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/supplemental/Wang_Image_Cropping_With_CVPR_2023_supplemental.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=J8ImNnEWwGQ) |
| Picture That Sketch: Photorealistic Image Generation From Abstract Sketches | [![GitHub](https://img.shields.io/github/stars/subhadeepkoley/PictureThatSketch)](https://github.com/subhadeepkoley/PictureThatSketch) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Koley_Picture_That_Sketch_Photorealistic_Image_Generation_From_Abstract_Sketches_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11162-b31b1b.svg)](http://arxiv.org/abs/2303.11162) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=k7xFbELpnv4) |
| MonoHuman: Animatable Human Neural Field From Monocular Video | [![GitHub](https://img.shields.io/github/stars/Yzmblog/MonoHuman)](https://github.com/Yzmblog/MonoHuman) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_MonoHuman_Animatable_Human_Neural_Field_From_Monocular_Video_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02001-b31b1b.svg)](http://arxiv.org/abs/2304.02001) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=T91fXw9dOmM) |
| PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sheng_PixHt-Lab_Pixel_Height_Based_Light_Effect_Generation_for_Image_Compositing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.00137-b31b1b.svg)](https://arxiv.org/abs/2303.00137) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=H2B0yrEf86I) |
| Neural Pixel Composition for 3D-4D View Synthesis From Multi-Views | [![Web Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.aayushbansal.xyz/npc/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Bansal_Neural_Pixel_Composition_for_3D-4D_View_Synthesis_From_Multi-Views_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2207.10663-b31b1b.svg)](https://arxiv.org/abs/2207.10663) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=oTJyUUH2uCk) |
| SpaText: Spatio-Textual Representation for Controllable Image Generation | [![Web Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://omriavrahami.com/spatext/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Avrahami_SpaText_Spatio-Textual_Representation_for_Controllable_Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14305-b31b1b.svg)](http://arxiv.org/abs/2211.14305) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VlieNoCwHO4) |
| Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhou_Exploring_Motion_Ambiguity_and_Alignment_for_High-Quality_Video_Frame_Interpolation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2203.10291-b31b1b.svg)](http://arxiv.org/abs/2203.10291) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WoAyz1S_nTI) |
| MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation | [![GitHub](https://img.shields.io/github/stars/researchmm/MM-Diffusion)](https://github.com/researchmm/MM-Diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ruan_MM-Diffusion_Learning_Multi-Modal_Diffusion_Models_for_Joint_Audio_and_Video_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09478-b31b1b.svg)](https://arxiv.org/abs/2212.09478) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DthMxv2VogU) |
| Synthesizing Photorealistic Virtual Humans Through Cross-Modal Disentanglement | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ondrejtexler.github.io/synthesizing_humans/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ravichandran_Synthesizing_Photorealistic_Virtual_Humans_Through_Cross-Modal_Disentanglement_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.01320-b31b1b.svg)](http://arxiv.org/abs/2209.01320) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://youtu.be/oNYy2-_xuhM) |
| Video Probabilistic Diffusion Models in Projected Latent Space | [![Web Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sihyun.me/PVDM/) <br/> [![GitHub](https://img.shields.io/github/stars/sihyun-yu/PVDM)](https://github.com/sihyun-yu/PVDM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_Video_Probabilistic_Diffusion_Models_in_Projected_Latent_Space_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.07685-b31b1b.svg)](http://arxiv.org/abs/2302.07685) | :heavy_minus_sign: |
| Variational Distribution Learning for Unsupervised Text-to-Image Generation |  :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kang_Variational_Distribution_Learning_for_Unsupervised_Text-to-Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16105-b31b1b.svg)](http://arxiv.org/abs/2303.16105) | :heavy_minus_sign: |
| Linking Garment With Person via Semantically Associated Landmarks for Virtual Try-On | [![Web Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://modelscope.cn/datasets/damo/SAL-HG/summary) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yan_Linking_Garment_With_Person_via_Semantically_Associated_Landmarks_for_Virtual_CVPR_2023_paper.pdf)  | :heavy_minus_sign: |
| UV Volumes for Real-Time Rendering of Editable Free-View Human Performance | [![GitHub](https://img.shields.io/github/stars/fanegg/UV-Volumes)](https://github.com/fanegg/UV-Volumes) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_UV_Volumes_for_Real-Time_Rendering_of_Editable_Free-View_Human_Performance_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2203.14402-b31b1b.svg)](http://arxiv.org/abs/2203.14402) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=v3PsN-rMAUw) |
| NULL-Text Inversion for Editing Real Images Using Guided Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://null-text-inversion.github.io/)  <br/> [![GitHub](https://img.shields.io/github/stars/google/prompt-to-prompt)](https://github.com/google/prompt-to-prompt/#null-text-inversion-for-editing-real-images) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Mokady_NULL-Text_Inversion_for_Editing_Real_Images_Using_Guided_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.09794-b31b1b.svg)](http://arxiv.org/abs/2211.09794) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qzTlzrMWU2M) |
| Polynomial Implicit Neural Representations for Large Diverse Datasets <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/Rajhans0/Poly_INR)](https://github.com/Rajhans0/Poly_INR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Singh_Polynomial_Implicit_Neural_Representations_for_Large_Diverse_Datasets_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11424-b31b1b.svg)](http://arxiv.org/abs/2303.11424) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=YdFpzITgV8M) |
| Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation | [![GitHub](https://img.shields.io/github/stars/MichalGeyer/plug-and-play)](https://github.com/MichalGeyer/plug-and-play) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tumanyan_Plug-and-Play_Diffusion_Features_for_Text-Driven_Image-to-Image_Translation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12572-b31b1b.svg)](http://arxiv.org/abs/2211.12572) |  [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=eemzbXXU59E) |
| Conditional Image-to-Video Generation With Latent Flow Diffusion Models | [![GitHub](https://img.shields.io/github/stars/nihaomiao/CVPR23_LFDM)](https://github.com/nihaomiao/CVPR23_LFDM)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ni_Conditional_Image-to-Video_Generation_With_Latent_Flow_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13744-b31b1b.svg)](http://arxiv.org/abs/2303.13744) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dgawtQGmMbA) |
| Local 3D Editing via 3D Distillation of CLIP Knowledge | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hyung_Local_3D_Editing_via_3D_Distillation_of_CLIP_Knowledge_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.12570-b31b1b.svg)](https://arxiv.org/abs/2306.12570) | :heavy_minus_sign: |
| Private Image Generation With Dual-Purpose Auxiliary Classifier <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Private_Image_Generation_With_Dual-Purpose_Auxiliary_Classifier_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZsjYIZ2s0fw) |
| MAGVIT: Masked Generative Video Transformer <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![Web Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://magvit.cs.cmu.edu/) <br/> [![GitHub](https://img.shields.io/github/stars/google-research/magvit)](https://github.com/google-research/magvit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_MAGVIT_Masked_Generative_Video_Transformer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.05199-b31b1b.svg)](http://arxiv.org/abs/2212.05199) | :heavy_minus_sign: |
| Dimensionality-Varying Diffusion Process | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Dimensionality-Varying_Diffusion_Process_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.16032-b31b1b.svg)](http://arxiv.org/abs/2211.16032)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zpNhHo3s4Eo) |
| VIVE3D: Viewpoint-Independent Video Editing Using 3D-Aware GANs | [![GitHub](https://img.shields.io/github/stars/afruehstueck/VIVE3D)](https://github.com/afruehstueck/VIVE3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Fruhstuck_VIVE3D_Viewpoint-Independent_Video_Editing_Using_3D-Aware_GANs_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15893-b31b1b.svg)](https://arxiv.org/abs/2303.15893) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qfYGQwOw8pg) |
| LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data | [![GitHub](https://img.shields.io/github/stars/KU-CVLAB/LANIT)](https://github.com/KU-CVLAB/LANIT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Park_LANIT_Language-Driven_Image-to-Image_Translation_for_Unlabeled_Data_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.14889-b31b1b.svg)](http://arxiv.org/abs/2208.14889) | :heavy_minus_sign: |
| DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model | [![GitHub](https://img.shields.io/github/stars/gwang-kim/DATID-3D)](https://github.com/gwang-kim/DATID-3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kim_DATID-3D_Diversity-Preserved_Domain_Adaptation_Using_Text-to-Image_Diffusion_for_3D_Generative_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.16374-b31b1b.svg)](https://arxiv.org/abs/2211.16374) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bjXQ4LTVE3E) |
| Delving StyleGAN Inversion for Image Editing: A Foundation Latent Space Viewpoint | [![GitHub](https://img.shields.io/github/stars/KumapowerLIU/CLCAE)](https://github.com/KumapowerLIU/CLCAE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_Delving_StyleGAN_Inversion_for_Image_Editing_A_Foundation_Latent_Space_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11448-b31b1b.svg)](http://arxiv.org/abs/2211.11448) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hsB9Wv50dm0) |
| High-Fidelity and Freely Controllable Talking Head Video Generation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_High-Fidelity_and_Freely_Controllable_Talking_Head_Video_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10168-b31b1b.svg)](http://arxiv.org/abs/2304.10168) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_syQLfiQ0_c) |
| SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sadtalker.github.io/)  <br/> [![GitHub](https://img.shields.io/github/stars/OpenTalker/SadTalker)](https://github.com/OpenTalker/SadTalker) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_SadTalker_Learning_Realistic_3D_Motion_Coefficients_for_Stylized_Audio-Driven_Single_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12194-b31b1b.svg)](http://arxiv.org/abs/2211.12194) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TjUOalcGDtE) |
| StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://kunhao-liu.github.io/StyleRF/)  <br/> [![GitHub](https://img.shields.io/github/stars/Kunhao-Liu/StyleRF)](https://github.com/Kunhao-Liu/StyleRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_StyleRF_Zero-Shot_3D_Style_Transfer_of_Neural_Radiance_Fields_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10598-b31b1b.svg)](http://arxiv.org/abs/2303.10598) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DkbRmmzTU40) |
| MOSO: Decomposing MOtion, Scene and Object for Video Prediction | [![GitHub](https://img.shields.io/github/stars/iva-mzsun/MOSO)](https://github.com/iva-mzsun/MOSO) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sun_MOSO_Decomposing_MOtion_Scene_and_Object_for_Video_Prediction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03684-b31b1b.svg)](http://arxiv.org/abs/2303.03684) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5kLsKpfJFrQ) |
| Multi Domain Learning for Motion Magnification | [![GitHub](https://img.shields.io/github/stars/jasdeep-singh-007/Multi-Domain-Learning-for-Motion-Magnification)](https://github.com/jasdeep-singh-007/Multi-Domain-Learning-for-Motion-Magnification) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Singh_Multi_Domain_Learning_for_Motion_Magnification_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://m.youtube.com/watch?v=Sg7_sXMnRLo&pp=ygUKI-ydgOuLiey4tQ%3D%3D) |
| GazeNeRF: 3D-Aware Gaze Redirection With Neural Radiance Fields | [![GitHub](https://img.shields.io/github/stars/AlessandroRuzzi/GazeNeRF)](https://github.com/AlessandroRuzzi/GazeNeRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ruzzi_GazeNeRF_3D-Aware_Gaze_Redirection_With_Neural_Radiance_Fields_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04823-b31b1b.svg)](http://arxiv.org/abs/2212.04823) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JwqKbmUR3DE) |
| Hierarchical B-Frame Video Coding Using Two-Layer CANF Without Motion Coding | [![GitHub](https://img.shields.io/github/stars/nycu-clab/tlzmc-cvpr)](https://github.com/nycu-clab/tlzmc-cvpr) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Alexandre_Hierarchical_B-Frame_Video_Coding_Using_Two-Layer_CANF_Without_Motion_Coding_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02690-b31b1b.svg)](https://arxiv.org/abs/2304.02690) | :heavy_minus_sign: |
| Blemish-Aware and Progressive Face Retouching With Limited Paired Data | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xie_Blemish-Aware_and_Progressive_Face_Retouching_With_Limited_Paired_Data_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qKy6t8JbUOs) |
| Text-Guided Unsupervised Latent Transformation for Multi-Attribute Image Manipulation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wei_Text-Guided_Unsupervised_Latent_Transformation_for_Multi-Attribute_Image_Manipulation_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| NeuralField-LDM: Scene Generation With Hierarchical Latent Diffusion Models | [![Web Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://research.nvidia.com/labs/toronto-ai/NFLDM/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kim_NeuralField-LDM_Scene_Generation_With_Hierarchical_Latent_Diffusion_Models_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Fix the Noise: Disentangling Source Feature for Controllable Domain Translation | [![GitHub](https://img.shields.io/github/stars/LeeDongYeun/FixNoise)](https://github.com/LeeDongYeun/FixNoise) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lee_Fix_the_Noise_Disentangling_Source_Feature_for_Controllable_Domain_Translation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11545-b31b1b.svg)](http://arxiv.org/abs/2303.11545) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VqN-rACydQw) |
| Class-Balancing Diffusion Models | [![GitHub](https://img.shields.io/github/stars/qym7/CBDM-pytorch)](https://github.com/qym7/CBDM-pytorch) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Qin_Class-Balancing_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.00562-b31b1b.svg)](http://arxiv.org/abs/2305.00562) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OWY_2OZ4e_4) |
| DPE: Disentanglement of Pose and Expression for General Video Portrait Editing | [![GitHub](https://img.shields.io/github/stars/OpenTalker/DPE)](https://github.com/OpenTalker/DPE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Pang_DPE_Disentanglement_of_Pose_and_Expression_for_General_Video_Portrait_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.06281-b31b1b.svg)](http://arxiv.org/abs/2301.06281) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vj9LELgXVJ0) |
| Inversion-Based Style Transfer With Diffusion Models | [![GitHub](https://img.shields.io/github/stars/zyxElsa/InST)](https://github.com/zyxElsa/InST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Inversion-Based_Style_Transfer_With_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13203-b31b1b.svg)](http://arxiv.org/abs/2211.13203) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=W3urLYx9JZY) |
| Deep Curvilinear Editing: Commutative and Nonlinear Image Manipulation for Pretrained Deep Generative Model | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Aoshima_Deep_Curvilinear_Editing_Commutative_and_Nonlinear_Image_Manipulation_for_Pretrained_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14573-b31b1b.svg)](http://arxiv.org/abs/2211.14573) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=iVyvQOAhLqI) |
| FlowGrad: Controlling the Output of Generative ODEs With Gradients | [![GitHub](https://img.shields.io/github/stars/gnobitab/FlowGrad)](https://github.com/gnobitab/FlowGrad) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_FlowGrad_Controlling_the_Output_of_Generative_ODEs_With_Gradients_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Graph Transformer GANs for Graph-Constrained House Generation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tang_Graph_Transformer_GANs_for_Graph-Constrained_House_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08225-b31b1b.svg)](http://arxiv.org/abs/2303.08225) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rwx9OPkRc4M) |
| Master: Meta Style Transformer for Controllable Zero-Shot and Few-Shot Artistic Style Transfer | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tang_Master_Meta_Style_Transformer_for_Controllable_Zero-Shot_and_Few-Shot_Artistic_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.11818-b31b1b.svg)](http://arxiv.org/abs/2304.11818) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PJaTztiUsTQ) |
| Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]()  | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mrtornado24.github.io/Next3D/)  <br/> [![GitHub](https://img.shields.io/github/stars/MrTornado24/Next3D)](https://github.com/MrTornado24/Next3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sun_Next3D_Generative_Neural_Texture_Rasterization_for_3D-Aware_Head_Avatars_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11208-b31b1b.svg)](http://arxiv.org/abs/2211.11208) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0F6Pmj-1sfI) |
| Ham2Pose: Animating Sign Language Notation Into Pose Sequences | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rotem-shalev.github.io/ham-to-pose/)  <br/> [![GitHub](https://img.shields.io/github/stars/rotem-shalev/Ham2Pose)](https://github.com/rotem-shalev/Ham2Pose) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Arkushin_Ham2Pose_Animating_Sign_Language_Notation_Into_Pose_Sequences_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13613-b31b1b.svg)](https://arxiv.org/abs/2211.13613) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_XOsxnwjo7s) |
| Neural Transformation Fields for Arbitrary-Styled Font Generation | [![GitHub](https://img.shields.io/github/stars/fubinfb/NTF)](https://github.com/fubinfb/NTF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Fu_Neural_Transformation_Fields_for_Arbitrary-Styled_Font_Generation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Fxp8N7TDVkQ) |
| LayoutDM: Transformer-Based Diffusion Model for Layout Generation | [![GitHub](https://img.shields.io/github/stars/CyberAgentAILab/layout-dm)](https://github.com/CyberAgentAILab/layout-dm) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chai_LayoutDM_Transformer-Based_Diffusion_Model_for_Layout_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.02567-b31b1b.svg)](http://arxiv.org/abs/2305.02567) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=n3akFx3mtYU) | 
| Removing Objects from Neural Radiance Fields |  |  |  |
| Person Image Synthesis via Denoising Diffusion Model |  |  |  |
| AdaptiveMix: Improving GAN Training via Feature Space Shrinkage |  |  |  |
| Learning Joint Latent Space EBM Prior Model for Multi-Layer Generator |  |  |  |
| 3D Neural Field Generation using Triplane Diffusion|  |  |  |
| OmniAvatar: Geometry-guided Controllable 3D Head Synthesis |  |  |  |
| RWSC-Fusion: Region-Wise Style-Controlled Fusion Network for the Prohibited X-ray Security Image Synthesis |  |  |  |
| ObjectStitch: Object Compositing with Diffusion Model |  |  |  |
| Persistent Nature: A Generative Model of Unbounded 3D Worlds |  |  |  |
| Masked and Adaptive Transformer for Exemplar based Image Translation |  |  |  |
| Spider GAN: Leveraging Friendly Neighbors to Accelerate GAN Training |  |  |  |
| Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild |  |  |  |
| Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models |  |  |  |
| All are Worth Words: A ViT Backbone for Diffusion Models |  |  |  |
| Few-Shot Semantic Image Synthesis with Class Affinity Transfer |  |  |  |
| Blowing in the Wind: CycleNet for Human Cinemagraphs from Still Images |  |  |  |
| StyleGene: Crossover and Mutation of Region-Level Facial Genes for Kinship Face Synthesis |  |  |  |
| MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs |  |  |  |
| MoStGAN-V: Video Generation with Temporal Motion Styles |  |  |  |
| Frame Interpolation Transformer and Uncertainty Guidance |  |  |  |
| Towards End-to-End Generative Modeling of Long Videos with Memory-Efficient Bidirectional Transformers |  |  |  |
| HOLODIFFUSION: Training a 3D Diffusion Model using 2D Images |  |  |  |
| Neural Texture Synthesis with Guided Correspondence |  |  |  |
| PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360&deg; |  |  |  |
| InstructPix2Pix: Learning to Follow Image Editing Instructions |  |  |  |
| Unpaired Image-to-Image Translation with Shortest Path Regularization |  |  |  |
| Freestyle Layout-to-Image Synthesis |  |  |  |
| On Distillation of Guided Diffusion Models |  |  |  |
| Single Image Backdoor Inversion via Robust Smoothed Classifiers |  |  |  |
| Make-a-Story: Visual Memory Conditioned Consistent Story Generation |  |  |  |
| Towards Practical Plug-and-Play Diffusion Models |  |  |  |
| Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis |  |  |  |
| Wavelet Diffusion Models are Fast and Scalable Image Generators |  |  |  |
| 3D GAN Inversion with Facial Symmetry Prior |  |  |  |
| Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert |  |  |  |
| PCT-Net: Full Resolution Image Harmonization Using Pixel-Wise Color Transformations |  |  |  |
| ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts |  |  |  |
| Video Compression with Entropy-Constrained Neural Representations |  |  |  |
| Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models |  |  |  |
| CoralStyleCLIP: Co-optimized Region and Layer Selection for Image Editing |  |  |  |
| Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding |  |  |  |
| Sequential Training of GANs Against GAN-classifiers Reveals Correlated ``Knowledge Gaps`` Present among Independently Trained GAN Instances |  |  |  |
| Attribute-Preserving Face Dataset Anonymization via Latent Code Optimization |  |  |  |
| Shifted Diffusion for Text-to-Image Generation |  |  |  |
| HandsOff: Labeled Dataset Generation with no Additional Human Annotations |  |  |  |
| Lookahead Diffusion Probabilistic Models for Refining Mean Estimation |  |  |  |
| Imagen Editor and EditBench: Advancing and Evaluating Text-guided Image Inpainting |  |  |  |
| Re-GAN: Data-Efficient GANs Training via Architectural Reconfiguration |  |  |  |
| BBDM: Image-to-Image Translation with Brownian Bridge Diffusion Models |  |  |  |
| VectorFusion: Text-to-SVG by Abstracting Pixel-based Diffusion Models |  |  |  |
