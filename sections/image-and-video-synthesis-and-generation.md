# CVPR-2023-Papers

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/3d-from-multi-view-and-sensors.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/humans-face-body-pose-gesture-movement.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" />
    </a>
</div>

## Image and Video Synthesis and Generation

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Towards Universal Fake Image Detectors That Generalize Across Generative Models | [![GitHub](https://img.shields.io/github/stars/Yuheng-Li/UniversalFakeDetect)](https://github.com/Yuheng-Li/UniversalFakeDetect)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ojha_Towards_Universal_Fake_Image_Detectors_That_Generalize_Across_Generative_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.10174-b31b1b.svg)](http://arxiv.org/abs/2302.10174)|:heavy_minus_sign: |
| Implicit Diffusion Models for Continuous Super-Resolution | [![GitHub](https://img.shields.io/github/stars/Ree1s/IDM)](https://github.com/Ree1s/IDM)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_Implicit_Diffusion_Models_for_Continuous_Super-Resolution_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16491-b31b1b.svg)](http://arxiv.org/abs/2303.16491)| :heavy_minus_sign: | 
| High-Fidelity Guided Image Synthesis With Latent Diffusion Models |  [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://1jsingh.github.io/gradop) <br /> [![GitHub](https://img.shields.io/github/stars/1jsingh/GradOP-Guided-Image-Synthesis)](https://github.com/1jsingh/GradOP-Guided-Image-Synthesis) |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Singh_High-Fidelity_Guided_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.17084-b31b1b.svg)](http://arxiv.org/abs/2211.17084)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Yk83RPCOa2o) |
| DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields | [![GitHub](https://img.shields.io/github/stars/AIBluefisher/dbarf)](https://github.com/AIBluefisher/dbarf)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_DBARF_Deep_Bundle-Adjusting_Generalizable_Neural_Radiance_Fields_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14478-b31b1b.svg)](http://arxiv.org/abs/2303.14478)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wFPO403wtAg) | 
| Deep Arbitrary-Scale Image Super-Resolution via Scale-Equivariance Pursuit | [![GitHub](https://img.shields.io/github/stars/neuralchen/EQSR)](https://github.com/neuralchen/EQSR)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Deep_Arbitrary-Scale_Image_Super-Resolution_via_Scale-Equivariance_Pursuit_CVPR_2023_paper.pdf) |[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Pq9eI5kxqUE) |
| Balanced Spherical Grid for Egocentric View Synthesis | [![GitHub](https://img.shields.io/github/stars/changwoonchoi/EgoNeRF)](https://github.com/changwoonchoi/EgoNeRF)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Choi_Balanced_Spherical_Grid_for_Egocentric_View_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12408-b31b1b.svg)](http://arxiv.org/abs/2303.12408)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=D-lsBhVP8zw) |
| SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation | [![GitHub](https://img.shields.io/github/stars/yccyenchicheng/SDFusion)](https://github.com/yccyenchicheng/SDFusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cheng_SDFusion_Multimodal_3D_Shape_Completion_Reconstruction_and_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04493-b31b1b.svg)](http://arxiv.org/abs/2212.04493) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6EvHJRlUMFQ) |
| DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation <br/> [![CVPR - Award](https://img.shields.io/badge/CVPR-Award-294A7C)]() | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dreambooth.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.12242-b31b1b.svg)](http://arxiv.org/abs/2208.12242) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=82x4XTSFwBQ) |
| Self-Guided Diffusion Models | [![GitHub](https://img.shields.io/github/stars/dongzhuoyao/self-guided-diffusion-models)](https://github.com/dongzhuoyao/self-guided-diffusion-models) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hu_Self-Guided_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.06462-b31b1b.svg)](http://arxiv.org/abs/2210.06462) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zwkn640t-u8) |
| Multi-Concept Customization of Text-to-Image Diffusion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://www.cs.cmu.edu/~custom-diffusion/dataset.html) <br /> [![GitHub](https://img.shields.io/github/stars/adobe-research/custom-diffusion)](https://github.com/adobe-research/custom-diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04488-b31b1b.svg)](http://arxiv.org/abs/2212.04488) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WWNA_IPLO84) |
| 3D-Aware Conditional Image Synthesis | [![GitHub](https://img.shields.io/github/stars/dunbar12138/pix2pix3D)](https://github.com/dunbar12138/pix2pix3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Deng_3D-Aware_Conditional_Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.08509-b31b1b.svg)](http://arxiv.org/abs/2302.08509)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dDRoI1gjbzk) |
| QuantArt: Quantizing Image Style Transfer Towards High Visual Fidelity | [![GitHub](https://img.shields.io/github/stars/siyuhuang/QuantArt)](https://github.com/siyuhuang/QuantArt) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_QuantArt_Quantizing_Image_Style_Transfer_Towards_High_Visual_Fidelity_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.10431-b31b1b.svg)](http://arxiv.org/abs/2212.10431) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WWNA_IPLO84) |
| SceneComposer: Any-Level Semantic Image Synthesis <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() |  [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zengyu.me/scenec/) <br /> [![GitHub](https://img.shields.io/github/stars/zengxianyu/scenec)](https://github.com/zengxianyu/scenec)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zeng_SceneComposer_Any-Level_Semantic_Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11742-b31b1b.svg)](http://arxiv.org/abs/2211.11742) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ky0LWZ_USRA) |
| DiffCollage: Parallel Generation of Large Content With Diffusion Models | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://research.nvidia.com/labs/dir/diffcollage/) |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_DiffCollage_Parallel_Generation_of_Large_Content_With_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17076-b31b1b.svg)](http://arxiv.org/abs/2303.17076) | :heavy_minus_sign: |
| Putting People in Their Place: Affordance-Aware Human Insertion Into Scenes | [![GitHub](https://img.shields.io/github/stars/adobe-research/affordance-insertion)](https://github.com/adobe-research/affordance-insertion)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kulal_Putting_People_in_Their_Place_Affordance-Aware_Human_Insertion_Into_Scenes_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.14406-b31b1b.svg)](http://arxiv.org/abs/2304.14406) | :heavy_minus_sign: |
| Hybrid Neural Rendering for Large-Scale Scenes With Motion Blur | [![GitHub](https://img.shields.io/github/stars/CVMI-Lab/HybridNeuralRendering)](https://github.com/CVMI-Lab/HybridNeuralRendering)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Dai_Hybrid_Neural_Rendering_for_Large-Scale_Scenes_With_Motion_Blur_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.12652-b31b1b.svg)](http://arxiv.org/abs/2304.12652) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hAhFfKRqDgE) |
| Binary Latent Diffusion | [![GitHub](https://img.shields.io/github/stars/ZeWang95/BinaryLatentDiffusion)](https://github.com/ZeWang95/BinaryLatentDiffusion)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Binary_Latent_Diffusion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04820-b31b1b.svg)](http://arxiv.org/abs/2304.04820) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/channel/UCzkUNNsV1TYuf6U_wGnMlnw) |
| StyleRes: Transforming the Residuals for Real Image Editing With StyleGAN | [![GitHub](https://img.shields.io/github/stars/hamzapehlivan/StyleRes)](https://github.com/hamzapehlivan/StyleRes)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Pehlivan_StyleRes_Transforming_the_Residuals_for_Real_Image_Editing_With_StyleGAN_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.14359-b31b1b.svg)](http://arxiv.org/abs/2212.14359) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=S9ZswKv8enw) |
| KD-DLGAN: Data Limited Image Generation via Knowledge Distillation | [![GitHub](https://img.shields.io/github/stars/cuikaiwen18/KD_DLGAN)](https://github.com/cuikaiwen18/KD_DLGAN)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cui_KD-DLGAN_Data_Limited_Image_Generation_via_Knowledge_Distillation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17158-b31b1b.svg)](https://arxiv.org/abs/2303.17158) | :heavy_minus_sign: |
| SeaThru-NeRF: Neural Radiance Fields in Scattering Media | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sea-thru-nerf.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/deborahLevy130/seathru_NeRF)](https://github.com/deborahLevy130/seathru_NeRF)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Levy_SeaThru-NeRF_Neural_Radiance_Fields_in_Scattering_Media_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.07743-b31b1b.svg)](https://arxiv.org/abs/2304.07743) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dXKCJS4cscg) |
| PointAvatar: Deformable Point-Based Head Avatars From Videos | [![GitHub](https://img.shields.io/github/stars/zhengyuf/PointAvatar)](https://github.com/zhengyuf/PointAvatar)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zheng_PointAvatar_Deformable_Point-Based_Head_Avatars_From_Videos_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08377-b31b1b.svg)](http://arxiv.org/abs/2212.08377) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wll_XtgpU7U) |
| 3DAvatarGAN: Bridging Domains for Personalized Editable Avatars | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rameenabdal.github.io/3DAvatarGAN/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Abdal_3DAvatarGAN_Bridging_Domains_for_Personalized_Editable_Avatars_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.02700-b31b1b.svg)](http://arxiv.org/abs/2301.02700) | :heavy_minus_sign: |
| Neural Preset for Color Style Transfer | [![GitHub](https://img.shields.io/github/stars/ZHKKKe/NeuralPreset)](https://github.com/ZHKKKe/NeuralPreset)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ke_Neural_Preset_for_Color_Style_Transfer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13511-b31b1b.svg)](http://arxiv.org/abs/2303.13511) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=x6fLAvTPesk) |
| Zero-Shot Generative Model Adaptation via Image-Specific Prompt Learning | [![GitHub](https://img.shields.io/github/stars/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation)](https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Guo_Zero-Shot_Generative_Model_Adaptation_via_Image-Specific_Prompt_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03119-b31b1b.svg)](http://arxiv.org/abs/2304.03119) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vw9-C3Sz5nM) |
| DyNCA: Real-Time Dynamic Texture Synthesis Using Neural Cellular Automata | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dynca.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/IVRL/DyNCA)](https://github.com/IVRL/DyNCA)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Pajouheshgar_DyNCA_Real-Time_Dynamic_Texture_Synthesis_Using_Neural_Cellular_Automata_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11417-b31b1b.svg)](http://arxiv.org/abs/2211.11417)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ELZC2mX5Z9U) |
| Exploring Incompatible Knowledge Transfer in Few-Shot Image Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yunqing-me.github.io/RICK/) <br /> [![GitHub](https://img.shields.io/github/stars/yunqing-me/RICK)](https://github.com/yunqing-me/RICK) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhao_Exploring_Incompatible_Knowledge_Transfer_in_Few-Shot_Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.07574-b31b1b.svg)](http://arxiv.org/abs/2304.07574) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=s14bA8filtw) |
| HouseDiffusion: Vector Floorplan Generation via a Diffusion Model With Discrete and Continuous Denoising | [![GitHub](https://img.shields.io/github/stars/aminshabani/house_diffusion)](https://github.com/aminshabani/house_diffusion)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shabani_HouseDiffusion_Vector_Floorplan_Generation_via_a_Diffusion_Model_With_Discrete_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13287-b31b1b.svg)](http://arxiv.org/abs/2211.13287) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ku6_gr94n5Q) |
| Towards Accurate Image Coding: Improved Autoregressive Image Generation With Dynamic Vector Quantization <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/CrossmodalGroup/DynamicVectorQuantization)](https://github.com/CrossmodalGroup/DynamicVectorQuantization) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Towards_Accurate_Image_Coding_Improved_Autoregressive_Image_Generation_With_Dynamic_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.11718-b31b1b.svg)](http://arxiv.org/abs/2305.11718) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ir60YW9JCjU) |
| RiDDLE: Reversible and Diversified De-Identification With Latent Encryptor | [![GitHub](https://img.shields.io/github/stars/ldz666666/RiDDLE)](https://github.com/ldz666666/RiDDLE)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_RiDDLE_Reversible_and_Diversified_De-Identification_With_Latent_Encryptor_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05171-b31b1b.svg)](http://arxiv.org/abs/2303.05171) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=gqs9Q6ReEn0) |
| LayoutDiffusion: Controllable Diffusion Model for Layout-to-Image Generation | [![GitHub](https://img.shields.io/github/stars/ZGCTroy/LayoutDiffusion)](https://github.com/ZGCTroy/LayoutDiffusion)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zheng_LayoutDiffusion_Controllable_Diffusion_Model_for_Layout-to-Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17189-b31b1b.svg)](http://arxiv.org/abs/2303.17189) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bJOpJnvhw3s) |
| LipFormer: High-Fidelity and Generalizable Talking Face Generation With a Pre-Learned Facial Codebook | [![GitHub](https://img.shields.io/github/stars/DaddyJin/awesome-faceReenactment)](https://github.com/DaddyJin/awesome-faceReenactment)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_LipFormer_High-Fidelity_and_Generalizable_Talking_Face_Generation_With_a_Pre-Learned_CVPR_2023_paper.pdf)  | :heavy_minus_sign: |
| Not All Image Regions Matter: Masked Vector Quantization for Autoregressive Image Generation | [![GitHub](https://img.shields.io/github/stars/CrossmodalGroup/MaskedVectorQuantization)](https://github.com/CrossmodalGroup/MaskedVectorQuantization)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Not_All_Image_Regions_Matter_Masked_Vector_Quantization_for_Autoregressive_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.13607-b31b1b.svg)](http://arxiv.org/abs/2305.13607) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=o2eyRscEejw) |
| GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis | [![GitHub](https://img.shields.io/github/stars/tobran/GALIP)](https://github.com/tobran/GALIP)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tao_GALIP_Generative_Adversarial_CLIPs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.12959-b31b1b.svg)](http://arxiv.org/abs/2301.12959)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lwhTDY4du_g) |
| High-Fidelity Generalized Emotional Talking Face Generation With Multi-Modal Emotion Space Learning | :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_High-Fidelity_Generalized_Emotional_Talking_Face_Generation_With_Multi-Modal_Emotion_Space_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.02572-b31b1b.svg)](http://arxiv.org/abs/2305.02572) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=eQG6ql83T0w) |
| Consistent View Synthesis With Pose-Guided Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://poseguided-diffusion.github.io/)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tseng_Consistent_View_Synthesis_With_Pose-Guided_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17598-b31b1b.svg)](http://arxiv.org/abs/2303.17598) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://youtu.be/eV1jwq14lE0) |
| StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-Based Generator | [![GitHub](https://img.shields.io/github/stars/guanjz20/StyleSync)](https://github.com/guanjz20/StyleSync)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Guan_StyleSync_High-Fidelity_Generalized_and_Personalized_Lip_Sync_in_Style-Based_Generator_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.05445-b31b1b.svg)](http://arxiv.org/abs/2305.05445) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=yAPDl2dVonY) |
| Imagic: Text-Based Real Image Editing With Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://poseguided-diffusion.github.io/)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kawar_Imagic_Text-Based_Real_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.09276-b31b1b.svg)](http://arxiv.org/abs/2210.09276) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lOZvBGz47wQ) |
| Large-Capacity and Flexible Video Steganography via Invertible Neural Network | [![GitHub](https://img.shields.io/github/stars/MC-E/LF-VSN)](https://github.com/MC-E/LF-VSN)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Mou_Large-Capacity_and_Flexible_Video_Steganography_via_Invertible_Neural_Network_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.12300-b31b1b.svg)](http://arxiv.org/abs/2304.12300) | :heavy_minus_sign: |
| Quantitative Manipulation of Custom Attributes on 3D-Aware Image Synthesis | :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Do_Quantitative_Manipulation_of_Custom_Attributes_on_3D-Aware_Image_Synthesis_CVPR_2023_paper.pdf)  | :heavy_minus_sign: |
| Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent Portrait Synthesis From Monocular Image | [![GitHub](https://img.shields.io/github/stars/YuDeng/GRAMInverter)](https://github.com/YuDeng/GRAMInverter)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Deng_Learning_Detailed_Radiance_Manifolds_for_High-Fidelity_and_3D-Consistent_Portrait_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13901-b31b1b.svg)](http://arxiv.org/abs/2211.13901) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nclBOg_CiJo) |
| CF-Font: Content Fusion for Few-Shot Font Generation | [![GitHub](https://img.shields.io/github/stars/wangchi95/CF-Font)](https://github.com/wangchi95/CF-Font)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_CF-Font_Content_Fusion_for_Few-Shot_Font_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14017-b31b1b.svg)](https://arxiv.org/abs/2303.14017) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=biwFd0K3X9o) |
| One-Shot High-Fidelity Talking-Head Synthesis With Deformable Neural Radiance Field | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.waytron.net/hidenerf/)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_One-Shot_High-Fidelity_Talking-Head_Synthesis_With_Deformable_Neural_Radiance_Field_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05097-b31b1b.svg)](http://arxiv.org/abs/2304.05097) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=opLdLY8_VYQ) |
| Unsupervised Domain Adaption With Pixel-Level Discriminator for Image-Aware Layout Generation | :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Unsupervised_Domain_Adaption_With_Pixel-Level_Discriminator_for_Image-Aware_Layout_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14377-b31b1b.svg)](http://arxiv.org/abs/2303.14377) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DBHFzw02T1I) |
| Diffusion Probabilistic Model Made Slim | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_Diffusion_Probabilistic_Model_Made_Slim_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.17106-b31b1b.svg)](http://arxiv.org/abs/2211.17106) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=82N6FsRUfr4) |
| Collaborative Diffusion for Multi-Modal Face Generation and Editing | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ziqihuangg.github.io/projects/collaborative-diffusion.html) <br/> [![GitHub](https://img.shields.io/github/stars/ziqihuangg/Collaborative-Diffusion)](https://github.com/ziqihuangg/Collaborative-Diffusion)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Collaborative_Diffusion_for_Multi-Modal_Face_Generation_and_Editing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10530-b31b1b.svg)](http://arxiv.org/abs/2304.10530)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=inLK4c8sNhc) |
| High-Fidelity Facial Avatar Reconstruction From Monocular Video With Generative Priors | [![GitHub](https://img.shields.io/github/stars/bbaaii/HFA-GP)](https://github.com/bbaaii/HFA-GP)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Bai_High-Fidelity_Facial_Avatar_Reconstruction_From_Monocular_Video_With_Generative_Priors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.15064-b31b1b.svg)](http://arxiv.org/abs/2211.15064)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZWdF8ASl0BQ) |
| Network-Free, Unsupervised Semantic Segmentation With Synthetic Images |  :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Feng_Network-Free_Unsupervised_Semantic_Segmentation_With_Synthetic_Images_CVPR_2023_paper.pdf) <br /> [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/network-free-unsupervised-semantic-segmentation-with-synthetic-images) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mfFZdvaF1Tw) |
| Visual Prompt Tuning for Generative Transfer Learning | [![GitHub](https://img.shields.io/github/stars/google-research/generative_transfer)](https://github.com/google-research/generative_transfer)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sohn_Visual_Prompt_Tuning_for_Generative_Transfer_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.00990-b31b1b.svg)](http://arxiv.org/abs/2210.00990) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kOza8-xop_g) |
| Specialist Diffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models To Learn Any Unseen Style|[![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://specialist-diffusion.github.io/) <br/> [![GitHub](https://img.shields.io/github/stars/Picsart-AI-Research/Specialist-Diffusion)](https://github.com/Picsart-AI-Research/Specialist-Diffusion)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lu_Specialist_Diffusion_Plug-and-Play_Sample-Efficient_Fine-Tuning_of_Text-to-Image_Diffusion_Models_To_CVPR_2023_paper.pdf)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5-hkImpVsNI) |
| Catch Missing Details: Image Reconstruction With Frequency Augmented Variational Autoencoder|[![GitHub](https://img.shields.io/github/stars/JiauZhang/FA-VAE)](https://github.com/JiauZhang/FA-VAE)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_Catch_Missing_Details_Image_Reconstruction_With_Frequency_Augmented_Variational_Autoencoder_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.02541-b31b1b.svg)](http://arxiv.org/abs/2305.02541) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wJ9U7tAnQEo) |
| Towards Bridging the Performance Gaps of Joint Energy-Based Models|[![GitHub](https://img.shields.io/github/stars/sndnyang/sadajem)](https://github.com/sndnyang/sadajem)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_Towards_Bridging_the_Performance_Gaps_of_Joint_Energy-Based_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.07959-b31b1b.svg)](http://arxiv.org/abs/2209.07959) | :heavy_minus_sign: |
| GLeaD: Improving GANs With a Generator-Leading Task|[![GitHub](https://img.shields.io/github/stars/EzioBy/glead)](https://github.com/EzioBy/glead)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Bai_GLeaD_Improving_GANs_With_a_Generator-Leading_Task_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03752-b31b1b.svg)](http://arxiv.org/abs/2212.03752) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bP8Iq_qLuU0) | 
| Structural Multiplane Image: Bridging Neural View Synthesis and 3D Reconstruction|[![GitHub](https://img.shields.io/github/stars/mf-zhang/Structural-MPI)](https://github.com/mf-zhang/Structural-MPI)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Structural_Multiplane_Image_Bridging_Neural_View_Synthesis_and_3D_Reconstruction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05937-b31b1b.svg)](http://arxiv.org/abs/2303.05937) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8Bbl8oZKAOs) |
| SPARF: Neural Radiance Fields From Sparse and Noisy Poses <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() |[![GitHub](https://img.shields.io/github/stars/google-research/sparf)](https://github.com/google-research/sparf)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Truong_SPARF_Neural_Radiance_Fields_From_Sparse_and_Noisy_Poses_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11738-b31b1b.svg)](http://arxiv.org/abs/2211.11738) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_s3_p2Brd_8) |
| DeltaEdit: Exploring Text-Free Training for Text-Driven Image Manipulation|[![GitHub](https://img.shields.io/github/stars/Yueming6568/DeltaEdit)](https://github.com/Yueming6568/DeltaEdit)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lyu_DeltaEdit_Exploring_Text-Free_Training_for_Text-Driven_Image_Manipulation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06285-b31b1b.svg)](http://arxiv.org/abs/2303.06285) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8cdZSbhDMIA) |
| Inferring and Leveraging Parts From Object Shape for Improving Semantic Image Synthesis|[![GitHub](https://img.shields.io/github/stars/csyxwei/iPOSE)](https://github.com/csyxwei/iPOSE)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wei_Inferring_and_Leveraging_Parts_From_Object_Shape_for_Improving_Semantic_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.19547-b31b1b.svg)](https://arxiv.org/abs/2305.19547) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=yVUmjQU9-v4) |
| VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation|:heavy_minus_sign:|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Luo_VideoFusion_Decomposed_Diffusion_Models_for_High-Quality_Video_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08320-b31b1b.svg)](http://arxiv.org/abs/2303.08320)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hxA0DTZScg0) |
| MaskSketch: Unpaired Structure-Guided Masked Image Generation <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() |:heavy_minus_sign:|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Bashkirova_MaskSketch_Unpaired_Structure-Guided_Masked_Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.05496-b31b1b.svg)](http://arxiv.org/abs/2302.05496) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2tBzEGASeo0) |
| Affordance Diffusion: Synthesizing Hand-Object Interactions|[![GitHub](https://img.shields.io/github/stars/NVlabs/affordance_diffusion)](https://github.com/NVlabs/affordance_diffusion)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ye_Affordance_Diffusion_Synthesizing_Hand-Object_Interactions_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12538-b31b1b.svg)](http://arxiv.org/abs/2303.12538) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=omhEoLzsopo) |
| Interactive Cartoonization With Controllable Perceptual Factors|:heavy_minus_sign:|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ahn_Interactive_Cartoonization_With_Controllable_Perceptual_Factors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09555-b31b1b.svg)](http://arxiv.org/abs/2212.09555) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=z8B2RiB4DyM) | |
| MetaPortrait: Identity-Preserving Talking Head Generation With Fast Personalized Adaptation | [![GitHub](https://img.shields.io/github/stars/Meta-Portrait/MetaPortrait)](https://github.com/Meta-Portrait/MetaPortrait)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_MetaPortrait_Identity-Preserving_Talking_Head_Generation_With_Fast_Personalized_Adaptation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08062-b31b1b.svg)](http://arxiv.org/abs/2212.08062) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DDEnjbCNNY4) |
| Paint by Example: Exemplar-Based Image Editing With Diffusion Models | [![GitHub](https://img.shields.io/github/stars/Fantasy-Studio/Paint-by-Example)](https://github.com/Fantasy-Studio/Paint-by-Example)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_Paint_by_Example_Exemplar-Based_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13227-b31b1b.svg)](http://arxiv.org/abs/2211.13227) | :heavy_minus_sign: |
| GLIGEN: Open-Set Grounded Text-to-Image Generation | [![GitHub](https://img.shields.io/github/stars/gligen/GLIGEN)](https://github.com/gligen/GLIGEN)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_GLIGEN_Open-Set_Grounded_Text-to-Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.07093-b31b1b.svg)](http://arxiv.org/abs/2301.07093) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-MCkU7IAGKs) |
| L-CoIns: Language-Based Colorization With Instance Awareness | [![GitHub](https://img.shields.io/github/stars/changzheng123/L-CoIns)](https://github.com/changzheng123/L-CoIns)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chang_L-CoIns_Language-Based_Colorization_With_Instance_Awareness_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation | [![GitHub](https://img.shields.io/github/stars/sstzal/DiffTalk)](https://github.com/sstzal/DiffTalk)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shen_DiffTalk_Crafting_Diffusion_Models_for_Generalized_Audio-Driven_Portraits_Animation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.03786-b31b1b.svg)](http://arxiv.org/abs/2301.03786) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tup5kbsOJXc) |
| Evading DeepFake Detectors via Adversarial Statistical Consistency |  :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hou_Evading_DeepFake_Detectors_via_Adversarial_Statistical_Consistency_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.11670-b31b1b.svg)](http://arxiv.org/abs/2304.11670) | :heavy_minus_sign: |
| GlassesGAN: Eyewear Personalization Using Synthetic Appearance Discovery and Targeted Subspace Modeling | :heavy_minus_sign:|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Plesh_GlassesGAN_Eyewear_Personalization_Using_Synthetic_Appearance_Discovery_and_Targeted_Subspace_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.14145-b31b1b.svg)](http://arxiv.org/abs/2210.14145) |  [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=oMiV__LWV4A) |
| GP-VTON: Towards General Purpose Virtual Try-On via Collaborative Local-Flow Global-Parsing Learning | [![GitHub](https://img.shields.io/github/stars/xiezhy6/GP-VTON)](https://github.com/xiezhy6/GP-VTON)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xie_GP-VTON_Towards_General_Purpose_Virtual_Try-On_via_Collaborative_Local-Flow_Global-Parsing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13756-b31b1b.svg)](https://arxiv.org/abs/2303.13756) |  [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=b-FDMJ0jrw0) |
| Where Is My Spot? Few-Shot Image Generation via Latent Subspace Optimization | [![GitHub](https://img.shields.io/github/stars/chansey0529/LSO)](https://github.com/chansey0529/LSO) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zheng_Where_Is_My_Spot_Few-Shot_Image_Generation_via_Latent_Subspace_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Regularized Vector Quantization for Tokenized Image Synthesis |  :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Regularized_Vector_Quantization_for_Tokenized_Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06424-b31b1b.svg)](http://arxiv.org/abs/2303.06424) |  :heavy_minus_sign: |
| EDICT: Exact Diffusion Inversion via Coupled Transformations | [![GitHub](https://img.shields.io/github/stars/salesforce/EDICT)](https://github.com/salesforce/EDICT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wallace_EDICT_Exact_Diffusion_Inversion_via_Coupled_Transformations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12446-b31b1b.svg)](http://arxiv.org/abs/2211.12446) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2k6DiE_h1eY) |
| Scaling Up GANs for Text-to-Image Synthesis <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mingukkang.github.io/GigaGAN/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kang_Scaling_Up_GANs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05511-b31b1b.svg)](http://arxiv.org/abs/2303.05511) |  [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZjxtuDQkOPY) |
| Shape-Aware Text-Driven Layered Video Editing | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://text-video-edit.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lee_Shape-Aware_Text-Driven_Layered_Video_Editing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.13173-b31b1b.svg)](http://arxiv.org/abs/2301.13173) | :heavy_minus_sign: |
| A Unified Pyramid Recurrent Network for Video Frame Interpolation | [![GitHub](https://img.shields.io/github/stars/srcn-ivl/UPR-Net)](https://github.com/srcn-ivl/UPR-Net) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jin_A_Unified_Pyramid_Recurrent_Network_for_Video_Frame_Interpolation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.03456-b31b1b.svg)](http://arxiv.org/abs/2211.03456) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=clvrjUKgfhI) |
| TAPS3D: Text-Guided 3D Textured Shape Generation From Pseudo Supervision | [![GitHub](https://img.shields.io/github/stars/plusmultiply/TAPS3D)](https://github.com/plusmultiply/TAPS3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wei_TAPS3D_Text-Guided_3D_Textured_Shape_Generation_From_Pseudo_Supervision_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13273-b31b1b.svg)](http://arxiv.org/abs/2303.13273) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-eWBEwAkThA) |
| Fine-Grained Face Swapping via Regional GAN Inversion | [![GitHub](https://img.shields.io/github/stars/e4s2022/e4s)](https://github.com/e4s2022/e4s)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_Fine-Grained_Face_Swapping_via_Regional_GAN_Inversion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14068-b31b1b.svg)](http://arxiv.org/abs/2211.14068) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Z-cmKVeXHvY) |
| OTAvatar: One-Shot Talking Face Avatar With Controllable Tri-Plane Rendering | [![GitHub](https://img.shields.io/github/stars/theEricMa/OTAvatar)](https://github.com/theEricMa/OTAvatar) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ma_OTAvatar_One-Shot_Talking_Face_Avatar_With_Controllable_Tri-Plane_Rendering_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14662-b31b1b.svg)](http://arxiv.org/abs/2303.14662) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qpIoMYFr7Aw) |
| Deep Stereo Video Inpainting | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Deep_Stereo_Video_Inpainting_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| StyleGAN Salon: Multi-View Latent Optimization for Pose-Invariant Hairstyle Transfer |  |  |  |
| Cross-GAN Auditing: Unsupervised Identification of Attribute Level Similarities and Differences between Pretrained Generative Models |  |  |  |
| Unsupervised Volumetric Animation |  |  |  |
| SINE: SINgle Image Editing with Text-to-Image Diffusion Models |  |  |  |
| Progressive Disentangled Representation Learning for Fine-grained Controllable Talking Head Synthesis |  |  |  |
| CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer |  |  |  |
| DeepVecFont-v2: Exploiting Transformers to Synthesize Vector Fonts with Higher Quality |  |  |  |
| LEMaRT: Label-Efficient Masked Region Transform for Image Harmonization |  |  |  |
| SINE: Semantic-Driven Image-based NeRF Editing with Prior-guided Editing Field |  |  |  |
| Exploring Intra-Class Variation Factors with Learnable Cluster Prompts for Semi-Supervised Image Synthesis |  |  |  |
| Image Cropping with Spatial-Aware Feature and Rank Consistency |  |  |  |
| <i>Picture that Sketch</i>: Photorealistic Image Generation from Abstract Sketches |  |  |  |
| MonoHuman: Animatable Human Neural Field from Monocular Video |  |  |  |
| PixHt-Lab: Pixel Height based Light Effect Generation for Image Compositing |  |  |  |
| Neural Pixel Composition for 3D-4D View Synthesis from Multi-Views |  |  |  |
| SpaText: Spatio-Textual Representation for Controllable Image Generation |  |  |  |
| Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation |  |  |  |
| MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation |  |  |  |
| Synthesizing Photorealistic Virtual Humans Through Cross-Modal Disentanglement |  |  |  |
| Video Probabilistic Diffusion Models in Projected Latent Space |  |  |  |
| Variational Distribution Learning for Unsupervised Text-to-Image Generation |  |  |  |
| Linking Garment with Person via Semantically Associated Landmarks for Virtual Try-On |  |  |  |
| UV Volumes for Real-Time Rendering of Editable Free-View Human Performance |  |  |  |
| Null-Text Inversion for Editing Real Images using Guided Diffusion Models |  |  |  |
| Polynomial Implicit Neural Representations for Large Diverse Datasets |  |  |  |
| Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation |  |  |  |
| Conditional Image-to-Video Generation with Latent Flow Diffusion Models |  |  |  |
| Local 3D Editing via 3D Distillation of CLIP Knowledge |  |  |  |
| Private Image Generation with Dual-Purpose Auxiliary Classifier |  |  |  |
| MAGVIT: Masked Generative Video Transformer |  |  |  |
| Dimensionality-Varying Diffusion Process |  |  |  |
| VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs |  |  |  |
| LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data |  |  |  |
| DATID-3D: Diversity-Preserved Domain Adaptation using Text-to-Image Diffusion for 3D Generative Model |  |  |  |
| Delving StyleGAN Inversion for Image Editing: A Foundation Latent Space Viewpoint |  |  |  |
| High-Fidelity and Freely Controllable Talking Head Video Generation |  |  |  |
| SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation |  |  |  |
| StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields |  |  |  |
| MOSO: Decomposing MOtion, Scene and Object for Video Prediction |  |  |  |
| Multi Domain Learning for Motion Magnification |  |  |  |
| GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields |  |  |  |
| Hierarchical B-frame Video Coding using Two-Layer CANF without Motion Coding |  |  |  |
| Blemish-Aware and Progressive Face Retouching with Limited Paired Data |  |  |  |
| Text-guided Unsupervised Latent Transformation for Multi-attribute Image Manipulation |  |  |  |
| NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models |  |  |  |
| Fix the Noise: Disentangling Source Feature for Controllable Domain Translation |  |  |  |
| Class-Balancing Diffusion Models |  |  |  |
| DPE: Disentanglement of Pose and Expression for General Video Portrait Editing |  |  |  |
| Inversion-based Style Transfer with Diffusion Models |  |  |  |
| Deep Curvilinear Editing: Commutative and Nonlinear Image Manipulation for Pretrained Deep Generative Model |  |  |  |
| FlowGrad: Controlling the Output of Generative ODEs with Gradients |  |  |  |
| Graph Transformer GANs for Graph-Constrained House Generation |  |  |  |
| Master: Meta Style Transformer for Controllable Zero-Shot and Few-Shot Artistic Style Transfer |  |  |  |
| Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars |  |  |  |
| Ham2Pose: Animating Sign Language Notation into Pose Sequences |  |  |  |
| Neural Transformation Fields for Arbitrary-Styled Font Generation |  |  |  |
| LayoutDM: Transformer-based Diffusion Model for Layout Generation |  |  |  |
| Removing Objects from Neural Radiance Fields |  |  |  |
| Person Image Synthesis via Denoising Diffusion Model |  |  |  |
| AdaptiveMix: Improving GAN Training via Feature Space Shrinkage |  |  |  |
| Learning Joint Latent Space EBM Prior Model for Multi-Layer Generator |  |  |  |
| 3D Neural Field Generation using Triplane Diffusion|  |  |  |
| OmniAvatar: Geometry-guided Controllable 3D Head Synthesis |  |  |  |
| RWSC-Fusion: Region-Wise Style-Controlled Fusion Network for the Prohibited X-ray Security Image Synthesis |  |  |  |
| ObjectStitch: Object Compositing with Diffusion Model |  |  |  |
| Persistent Nature: A Generative Model of Unbounded 3D Worlds |  |  |  |
| Masked and Adaptive Transformer for Exemplar based Image Translation |  |  |  |
| Spider GAN: Leveraging Friendly Neighbors to Accelerate GAN Training |  |  |  |
| Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild |  |  |  |
| Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models |  |  |  |
| All are Worth Words: A ViT Backbone for Diffusion Models |  |  |  |
| Few-Shot Semantic Image Synthesis with Class Affinity Transfer |  |  |  |
| Blowing in the Wind: CycleNet for Human Cinemagraphs from Still Images |  |  |  |
| StyleGene: Crossover and Mutation of Region-Level Facial Genes for Kinship Face Synthesis |  |  |  |
| MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs |  |  |  |
| MoStGAN-V: Video Generation with Temporal Motion Styles |  |  |  |
| Frame Interpolation Transformer and Uncertainty Guidance |  |  |  |
| Towards End-to-End Generative Modeling of Long Videos with Memory-Efficient Bidirectional Transformers |  |  |  |
| HOLODIFFUSION: Training a 3D Diffusion Model using 2D Images |  |  |  |
| Neural Texture Synthesis with Guided Correspondence |  |  |  |
| PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360&deg; |  |  |  |
| InstructPix2Pix: Learning to Follow Image Editing Instructions |  |  |  |
| Unpaired Image-to-Image Translation with Shortest Path Regularization |  |  |  |
| Freestyle Layout-to-Image Synthesis |  |  |  |
| On Distillation of Guided Diffusion Models |  |  |  |
| Single Image Backdoor Inversion via Robust Smoothed Classifiers |  |  |  |
| Make-a-Story: Visual Memory Conditioned Consistent Story Generation |  |  |  |
| Towards Practical Plug-and-Play Diffusion Models |  |  |  |
| Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis |  |  |  |
| Wavelet Diffusion Models are Fast and Scalable Image Generators |  |  |  |
| 3D GAN Inversion with Facial Symmetry Prior |  |  |  |
| Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert |  |  |  |
| PCT-Net: Full Resolution Image Harmonization Using Pixel-Wise Color Transformations |  |  |  |
| ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts |  |  |  |
| Video Compression with Entropy-Constrained Neural Representations |  |  |  |
| Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models |  |  |  |
| CoralStyleCLIP: Co-optimized Region and Layer Selection for Image Editing |  |  |  |
| Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding |  |  |  |
| Sequential Training of GANs Against GAN-classifiers Reveals Correlated ``Knowledge Gaps`` Present among Independently Trained GAN Instances |  |  |  |
| Attribute-Preserving Face Dataset Anonymization via Latent Code Optimization |  |  |  |
| Shifted Diffusion for Text-to-Image Generation |  |  |  |
| HandsOff: Labeled Dataset Generation with no Additional Human Annotations |  |  |  |
| Lookahead Diffusion Probabilistic Models for Refining Mean Estimation |  |  |  |
| Imagen Editor and EditBench: Advancing and Evaluating Text-guided Image Inpainting |  |  |  |
| Re-GAN: Data-Efficient GANs Training via Architectural Reconfiguration |  |  |  |
| BBDM: Image-to-Image Translation with Brownian Bridge Diffusion Models |  |  |  |
| VectorFusion: Text-to-SVG by Abstracting Pixel-based Diffusion Models |  |  |  |
