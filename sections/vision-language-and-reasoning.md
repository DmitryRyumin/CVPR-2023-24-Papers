# CVPR-2023-Papers

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/recognition-categorization-detection-retrieval.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/low-level-vision.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Vision, Language, and Reasoning

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://antoyang.github.io/vid2seq.html) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_Vid2Seq_Large-Scale_Pretraining_of_a_Visual_Language_Model_for_Dense_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.14115-b31b1b.svg)](http://arxiv.org/abs/2302.14115) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hXP-2fYzq4g) |
| Open-Vocabulary Panoptic Segmentation With Text-to-Image Diffusion Models <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/NVlabs/ODISE?style=flat)](https://github.com/NVlabs/ODISE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Open-Vocabulary_Panoptic_Segmentation_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.04803-b31b1b.svg)](http://arxiv.org/abs/2303.04803)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=eW2vF8o_7p0) |
| Iterative Proposal Refinement for Weakly-Supervised Video Grounding | [![GitHub](https://img.shields.io/github/stars/ttengwang/Awesome_Long_Term_Video_Understanding?style=flat)](https://github.com/ttengwang/Awesome_Long_Term_Video_Understanding) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cao_Iterative_Proposal_Refinement_for_Weakly-Supervised_Video_Grounding_CVPR_2023_paper.pdf)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=BbGvHI_pVXk) |
| MetaCLUE: Towards Comprehensive Visual Metaphors Research | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://metaclue.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Akula_MetaCLUE_Towards_Comprehensive_Visual_Metaphors_Research_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09898-b31b1b.svg)](http://arxiv.org/abs/2212.09898) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=V3TmeNETL-o) |
| PolyFormer: Referring Image Segmentation As Sequential Polygon Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://polyformer.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/amazon-science/polygon-transformer?style=flat)](https://github.com/amazon-science/polygon-transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_PolyFormer_Referring_Image_Segmentation_As_Sequential_Polygon_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.07387-b31b1b.svg)](http://arxiv.org/abs/2302.07387) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6LNrqoxQR1M) |
| GeneCIS: A Benchmark for General Conditional Image Similarity <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) |[![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sgvaze.github.io/genecis/) <br /> [![GitHub](https://img.shields.io/github/stars/facebookresearch/genecis?style=flat)](https://github.com/facebookresearch/genecis) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Vaze_GeneCIS_A_Benchmark_for_General_Conditional_Image_Similarity_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.07969-b31b1b.svg)](https://arxiv.org/abs/2306.07969)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wu3U2iNGIUw) |
| FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) |[![GitHub](https://img.shields.io/github/stars/BrandonHanx/FAME-ViL?style=flat)](https://github.com/BrandonHanx/FAME-ViL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Han_FAME-ViL_Multi-Tasking_Vision-Language_Model_for_Heterogeneous_Fashion_Tasks_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02483-b31b1b.svg)](https://arxiv.org/abs/2303.02483) | :heavy_minus_sign: |
| Generative Bias for Robust Visual Question Answering | [![GitHub](https://img.shields.io/github/stars/chojw/genb?style=flat)](https://github.com/chojw/genb) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cho_Generative_Bias_for_Robust_Visual_Question_Answering_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.00690-b31b1b.svg)](http://arxiv.org/abs/2208.00690)| :heavy_minus_sign: |
| Advancing Visual Grounding With Scene Knowledge: Benchmark and Method | [![GitHub](https://img.shields.io/github/stars/zhjohnchan/SK-VG?style=flat)](https://github.com/zhjohnchan/SK-VG) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Song_Advancing_Visual_Grounding_With_Scene_Knowledge_Benchmark_and_Method_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11558-b31b1b.svg)](https://arxiv.org/abs/2307.11558) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DmmPiseO59o) |
| Gloss Attention for Gloss-Free Sign Language Translation | [![GitHub](https://img.shields.io/github/stars/YinAoXiong/GASLT?style=flat)](https://github.com/YinAoXiong/GASLT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yin_Gloss_Attention_for_Gloss-Free_Sign_Language_Translation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.07361-b31b1b.svg)](https://arxiv.org/abs/2307.07361) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=NEoWvxkJXfU) |
| You Can Ground Earlier Than See: An Effective and Efficient Pipeline for Temporal Sentence Grounding in Compressed Videos | :heavy_minus_sign:|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Fang_You_Can_Ground_Earlier_Than_See_An_Effective_and_Efficient_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07863-b31b1b.svg)](http://arxiv.org/abs/2303.07863) | :heavy_minus_sign: |
| Generalized Decoding for Pixel, Image, and Language | [![GitHub](https://img.shields.io/github/stars/microsoft/X-Decoder?style=flat)](https://github.com/microsoft/X-Decoder) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zou_Generalized_Decoding_for_Pixel_Image_and_Language_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.11270-b31b1b.svg)](http://arxiv.org/abs/2212.11270) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wYp6vmyolqE) |
| Accelerating Vision-Language Pretraining With Free Language Modeling | [![GitHub](https://img.shields.io/github/stars/TencentARC/FLM?style=flat)](https://github.com/TencentARC/FLM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Accelerating_Vision-Language_Pretraining_With_Free_Language_Modeling_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14038-b31b1b.svg)](http://arxiv.org/abs/2303.14038) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WbH_5DH_jfY) |
| GRES: Generalized Referring Expression Segmentation <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://henghuiding.github.io/GRES/) <br /> [![GitHub](https://img.shields.io/github/stars/henghuiding/ReLA?style=flat)](https://github.com/henghuiding/ReLA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_GRES_Generalized_Referring_Expression_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.00968-b31b1b.svg)](http://arxiv.org/abs/2306.00968) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=eWjAgYUU6Do) |
| BUFFER: Balancing Accuracy, Efficiency, and Generalizability in Point Cloud Registration | [![GitHub](https://img.shields.io/github/stars/The-Learning-And-Vision-Atelier-LAVA/BUFFER?style=flat)](https://github.com/The-Learning-And-Vision-Atelier-LAVA/BUFFER) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ao_BUFFER_Balancing_Accuracy_Efficiency_and_Generalizability_in_Point_Cloud_Registration_CVPR_2023_paper.pdf)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=STmAkRWuSiY) |
| RGB No More: Minimally-Decoded JPEG Vision Transformers | [![GitHub](https://img.shields.io/github/stars/JeongsooP/RGB-no-more?style=flat)](https://github.com/JeongsooP/RGB-no-more) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Park_RGB_No_More_Minimally-Decoded_JPEG_Vision_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.16421-b31b1b.svg)](http://arxiv.org/abs/2211.16421) | :heavy_minus_sign: |
| Scaling Language-Image Pre-Training via Masking |  |  |  |
| EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding |  |  |  |
| RefTeacher: A Strong Baseline for Semi-Supervised Referring Expression Comprehension |  |  |  |
| Mobile User Interface Element Detection Via Adaptively Prompt Tuning |  |  |  |
| Context-Aware Alignment and Mutual Masking for 3D-Language Pre-Training |  |  |  |
| Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation |  |  |  |
| Meta Compositional Referring Expression Segmentation |  |  |  |
| VindLU: A Recipe for Effective Video-and-Language Pretraining |  |  |  |
| Super-CLEVR: A Virtual Benchmark to Diagnose Domain Robustness in Visual Reasoning |  |  |  |
| GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods |  |  |  |
| Learning Customized Visual Models with Retrieval-Augmented Knowledge |  |  |  |
| LAVENDER: Unifying Video-Language Understanding as Masked Language Modeling |  |  |  |
| An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling |  |  |  |
| NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations |  |  |  |
| Clover: Towards a Unified Video-Language Alignment and Fusion Model |  |  |  |
| Dual Alignment Unsupervised Domain Adaptation for Video-Text Retrieval |  |  |  |
| Task Residual for Tuning Vision-Language Models |  |  |  |
| Dream3D: Zero-Shot Text-to-3D Synthesis using 3D Shape Prior and Text-to-Image Diffusion Models |  |  |  |
| End-to-End 3D Dense Captioning with Vote2Cap-DETR |  |  |  |
| Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training |  |  |  |
| Adaptive Zone-Aware Hierarchical Planner for Vision-Language Navigation |  |  |  |
| Visual Programming: Compositional Visual Reasoning without Training |  |  |  |
| Exploring the Effect of Primitives for Compositional Generalization in Vision-and-Language |  |  |  |
| Referring Multi-Object Tracking |  |  |  |
| MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis |  |  |  |
| MIST : Multi-Modal Iterative Spatial-Temporal Transformer for Long-Form Video Question Answering |  |  |  |
| Learning to Segment Every Referring Object Point by Point |  |  |  |
| Contrastive Grouping with Transformer for Referring Image Segmentation |  |  |  |
| Prototype-based Embedding Network for Scene Graph Generation |  |  |  |
| Collaborative Static and Dynamic Vision-Language Streams for Spatio-Temporal Video Grounding |  |  |  |
| S<sup>3</sup>C: Semi-Supervised VQA Natural Language Explanation via Self-Critical Learning |  |  |  |
| REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory |  |  |  |
| Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering |  |  |  |
| Cap4Video: What can Auxiliary Captions do for Text-Video Retrieval? |  |  |  |
| Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning |  |  |  |
| HAAV: Hierarchical Aggregation of Augmented Views for Image Captioning |  |  |  |
| Zero-Shot Referring Image Segmentation with Global-Local Context Features |  |  |  |
| Hierarchical Semantic Correspondence Networks for Video Paragraph Grounding |  |  |  |
| Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training |  |  |  |
| Probabilistic Prompt Learning for Dense Prediction |  |  |  |
| DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-Training via Word-Region Alignment |  |  |  |
| All in One: Exploring Unified Video-Language Pre-Training |  |  |  |
| Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding |  |  |  |
| Divide and Conquer: Answering Questions with Object Factorization and Compositional Reasoning |  |  |  |
| ConZIC: Controllable Zero-Shot Image Captioning by Sampling-based Polishing |  |  |  |
| RefCLIP: A Universal Teacher for Weakly Supervised Referring Expression Comprehension |  |  |  |
| KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation |  |  |  |
| ANetQA: A Large-Scale Benchmark for Fine-grained Compositional Reasoning over Untrimmed Videos |  |  |  |
| ViLEM: Visual-Language Error Modeling for Image-Text Retrieval |  |  |  |
| Multi-Modal Representation Learning with Text-Driven Soft Masks |  |  |  |
| Meta-Personalizing Vision-Language Models to Find Named Instances in Video |  |  |  |
| ReCo: Region-Controlled Text-to-Image Generation |  |  |  |
| Are Deep Neural Networks SMARTer than Second Graders? |  |  |  |
| Graph Representation for Order-Aware Visual Transformation |  |  |  |
| 3D Concept Learning and Reasoning from Multi-View Images |  |  |  |
| Text with Knowledge Graph Augmented Transformer for Video Captioning |  |  |  |
| Crossing the Gap: Domain Generalization for Image Captioning |  |  |  |
| MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering |  |  |  |
| VQACL: A Novel Visual Question Answering Continual Learning Setting |  |  |  |
| Improving Selective Visual Question Answering by Learning from Your Peers |  |  |  |
| High-Fidelity 3D Face Generation from Natural Language Descriptions |  |  |  |
| Language-guided Audio-Visual Source Separation via Trimodal Consistency |  |  |  |
| Test of Time: Instilling Video-Language Models with a Sense of Time |  |  |  |
| Learning Situation Hyper-Graphs for Video Question Answering |  |  |  |
| Pic2Word: Mapping Pictures to Words for Zero-Shot Composed Image Retrieval |  |  |  |
| Fine-grained Audible Video Description |  |  |  |
| Discovering the Real Association: Multimodal Causal Reasoning in Video Question Answering |  |  |  |
| A-Cap: Anticipation Captioning with Commonsense Knowledge |  |  |  |
| Cross-Domain Image Captioning with Discriminative Finetuning |  |  |  |
| Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations |  |  |  |
| The Dialog Must Go on: Improving Visual Dialog via Generative Self-Training |  |  |  |
| Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images! |  |  |  |
| Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP |  |  |  |
| Similarity Maps for Self-Training Weakly-Supervised Phrase Grounding |  |  |  |
| Language Adaptive Weight Generation for Multi-Task Visual Grounding |  |  |  |
| CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment |  |  |  |
| Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers |  |  |  |
| A Simple Framework for Text-Supervised Semantic Segmentation |  |  |  |
| Learning to Name Classes for Vision and Language Models |  |  |  |
| Iterative Vision-and-Language Navigation |  |  |  |
| Behavioral Analysis of Vision-and-Language Navigation Agents |  |  |  |
| Towards Fast Adaptation of Pretrained Contrastive Models for Multi-Channel Video-Language Retrieval |  |  |  |
| SynthVSR: Scaling Up Visual Speech Recognition with Synthetic Supervision |  |  |  |
| METransformer: Radiology Report Generation by Transformer with Multiple Learnable Expert Tokens |  |  |  |
| Fusing Pre-trained Language Models with Multimodal Prompts through Reinforcement Learning |  |  |  |
| Abstract Visual Reasoning: An Algebraic Approach for Solving Raven's Progressive Matrices |  |  |  |
| Hierarchical Prompt Learning for Multi-Task Learning |  |  |  |
| Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image Person Retrieval |  |  |  |
| SViTT: Temporal Learning of Sparse Video-Text Transformers |  |  |  |
| How You Feelin'? Learning Emotions and Mental States in Movie Scenes |  |  |  |
| Logical Implications for Visual Question Answering Consistency |  |  |  |
| Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks |  |  |  |
| DeCo: Decomposition and Reconstruction for Compositional Temporal Grounding via Coarse-to-Fine Contrastive Ranking |  |  |  |
| iCLIP: Bridging Image Classification and Contrastive Language-Image Pre-Training for Visual Recognition |  |  |  |
| Semantic-Conditional Diffusion Networks for Image Captioning |  |  |  |
| CREPE: Can Vision-Language Foundation Models Reason Compositionally? |  |  |  |
| RMLVQA: A Margin Loss Approach for Visual Question Answering with Language Biases |  |  |  |
| Improving Vision-and-Language Navigation by Generating Future-View Image Semantics |  |  |  |
| Prefix Conditioning Unifies Language and Label Supervision |  |  |  |
| A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning |  |  |  |
| From Images to Textual Prompts: Zero-Shot Visual Question Answering with Frozen Large Language Models |  |  |  |
| Hierarchical Video-Moment Retrieval and Step-Captioning |  |  |  |
