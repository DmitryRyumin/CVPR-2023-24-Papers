# CVPR-2023-Papers

<div align="center">
  <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/robotics.md">
    <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" />
  </a>
  <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
    <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" />
  </a>
  <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/explainable-computer-vision.md">
    <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" />
  </a>
</div>

## Transparency, Fairness, Accountability, Privacy, Ethics in Vision

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Effective Ambiguity Attack Against Passport-based DNN Intellectual Property Protection Schemes through Fully Connected Layer Substitution |  |  |  |
| Progressive Open Space Expansion for Open-Set Model Attribution |  |  |  |
| Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack |  |  |  |
| DartBlur: Privacy Preservation with Detection Artifact Suppression |  |  |  |
| Reinforcement Learning-based Black-Box Model Inversion Attacks |  |  |  |
| Model-Agnostic Gender Debiased Image Captioning |  |  |  |
| Uncurated Image-Text Datasets: Shedding Light on Demographic Bias |  |  |  |
| AltFreezing for more General Video Face Forgery Detection |  |  |  |
| Make Landscape Flatter in Differentially Private Federated Learning |  |  |  |
| DynaFed: Tackling Client Data Heterogeneity with Global Dynamics |  |  |  |
| Re-Thinking Model Inversion Attacks Against Deep Neural Networks |  |  |  |
| Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models |  |  |  |
| TrojViT: Trojan Insertion in Vision Transformers |  |  |  |
| Difficulty-based Sampling for Debiased Contrastive Representation Learning |  |  |  |
| Model Barrier: A Compact Un-Transferable Isolation Domain for Model Intellectual Property Protection |  |  |  |
| Fair Scratch Tickets: Finding Fair Sparse Networks without Weight Training |  |  |  |
| CLIP2Protect: Protecting Facial Privacy using Text-guided Makeup via Adversarial Latent Search |  |  |  |
| Bias in Pruned Vision Models: In-Depth Analysis and Countermeasures |  |  |  |
| Learning to Generate Image Embeddings with User-Level Differential Privacy |  |  |  |
| Bias Mimicking: A Simple Sampling Approach for Bias Mitigation |  |  |  |
| CaPriDe Learning: Confidential and Private Decentralized Learning based on Encryption-Friendly Distillation Loss |  |  |  |
| DeAR: Debiasing Vision-Language Models with Additive Residuals |  |  |  |
| Deep Deterministic Uncertainty: A New Simple Baseline |  |  |  |
| Manipulating Transfer Learning for Property Inference |  |  |  |
| Training Debiased Subnetworks with Contrastive Weight Pruning |  |  |  |
| Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models |  |  |  |
| STDLens: Model Hijacking-Resilient Federated Learning for Object Detection |  |  |  |
| Architectural Backdoors in Neural Networks |  |  |  |
| MEDIC: Remove Model Backdoors via Importance Driven Cloning |  |  |  |
| Learning Debiased Representations via Conditional Attribute Interpolation |  |  |  |
