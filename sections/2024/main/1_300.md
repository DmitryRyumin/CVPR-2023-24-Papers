# CVPR-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/🤗-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
    <tr>
        <td><strong>Previous Collections</strong></td>
        <td>
            <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README_2023.md">
                <img src="http://img.shields.io/badge/CVPR-2023-0073AE.svg" alt="Conference">
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/sections/2024/main/301_600.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## 1-300 papers

![Section Papers](https://img.shields.io/badge/Section%20Papers-50-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-40-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-33-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-16-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Open-Vocabulary Video Anomaly Detection | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2311.07042-b31b1b.svg)](http://arxiv.org/abs/2311.07042) | :heavy_minus_sign: |
| Any-Shift Prompting for Generalization over Distributions | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2402.10099-b31b1b.svg)](http://arxiv.org/abs/2402.10099) | :heavy_minus_sign: |
| BlockGCN: Redefine Topology Awareness for Skeleton-based Action Recognition | [![GitHub](https://img.shields.io/github/stars/ZhouYuxuanYX/BlockGCN?style=flat)](https://github.com/ZhouYuxuanYX/BlockGCN) | :heavy_minus_sign: | :heavy_minus_sign: |
| Fine-grained Prototypical Voting with Heterogeneous Mixup for Semi-Supervised 2D-3D Cross-Modal Retrieval | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| OneLLM: One Framework to Align All Modalities with Language | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://onellm.csuhan.com/) <br /> [![GitHub](https://img.shields.io/github/stars/csuhan/OneLLM?style=flat)](https://github.com/csuhan/OneLLM) <br /> [![Hugging Face](https://img.shields.io/badge/🤗-model-FFD21F.svg)](https://huggingface.co/csuhan/OneLLM-7B) | [![arXiv](https://img.shields.io/badge/arXiv-2312.03700-b31b1b.svg)](http://arxiv.org/abs/2312.03700) | :heavy_minus_sign: |
| SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yihua7.github.io/SC-GS-web/) <br /> [![GitHub](https://img.shields.io/github/stars/yihua7/SC-GS?style=flat)](https://github.com/yihua7/SC-GS) | [![arXiv](https://img.shields.io/badge/arXiv-2312.14937-b31b1b.svg)](http://arxiv.org/abs/2312.14937) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CYQYX_0xi5E) |
| Attentive Illumination Decomposition Model for Multi-Illuminant White Balancing | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.dykim.me/projects/aid) | [![arXiv](https://img.shields.io/badge/arXiv-2402.18277-b31b1b.svg)](http://arxiv.org/abs/2402.18277) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=f2yNsVVS2h0) |
| Not All Prompts are Secure: A Switchable Backdoor Attack Against Pre-trained Vision Transfomers | [![GitHub](https://img.shields.io/github/stars/20000yshust/SWARM?style=flat)](https://github.com/20000yshust/SWARM) | :heavy_minus_sign: | :heavy_minus_sign: |
| Weakly Supervised Monocular 3D Detection with a Single-View Image | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2402.19144-b31b1b.svg)](http://arxiv.org/abs/2402.19144) | :heavy_minus_sign: |
| Coherent Temporal Synthesis for Incremental Action Segmentation | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://guodongding.cn/projects/itas/itas.html) | [![arXiv](https://img.shields.io/badge/arXiv-2403.06102-b31b1b.svg)](http://arxiv.org/abs/2403.06102) | :heavy_minus_sign: |
| Autoregressive Queries for Adaptive Tracking with Spatio-Temporal Transformers | [![GitHub](https://img.shields.io/github/stars/GXNU-ZhongLab/AQATrack?style=flat)](https://github.com/GXNU-ZhongLab/AQATrack) | [![arXiv](https://img.shields.io/badge/arXiv-2403.10574-b31b1b.svg)](http://arxiv.org/abs/2403.10574) | :heavy_minus_sign: |
| Language Models as Black-Box Optimizers for Vision-Language Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://llm-can-optimize-vlm.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/shihongl1998/LLM-as-a-blackbox-optimizer?style=flat)](https://github.com/shihongl1998/LLM-as-a-blackbox-optimizer) | [![arXiv](https://img.shields.io/badge/arXiv-2309.05950-b31b1b.svg)](http://arxiv.org/abs/2309.05950) | :heavy_minus_sign: |
| Domain Prompt Learning with Quaternion Networks | [![GitHub](https://img.shields.io/github/stars/caoql98/DPLQ?style=flat)](https://github.com/caoql98/DPLQ) | [![arXiv](https://img.shields.io/badge/arXiv-2312.08878-b31b1b.svg)](http://arxiv.org/abs/2312.08878) | :heavy_minus_sign: |
| ZePT: Zero-Shot Pan-Tumor Segmentation via Query-Disentangling and Self-Prompting | [![GitHub](https://img.shields.io/github/stars/Yankai96/ZePT?style=flat)](https://github.com/Yankai96/ZePT) | [![arXiv](https://img.shields.io/badge/arXiv-2312.04964-b31b1b.svg)](http://arxiv.org/abs/2312.04964) | :heavy_minus_sign: |
| ODCR: Orthogonal Decoupling Contrastive Regularization for Unpaired Image Dehazing | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2404.17825-b31b1b.svg)](http://arxiv.org/abs/2404.17825) | :heavy_minus_sign: |
| Learning CNN on ViT: A Hybrid Model to Explicitly Class-Specific Boundaries for Domain Adaptation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dotrannhattuong.github.io/ECB/website/) <br /> [![GitHub](https://img.shields.io/github/stars/dotrannhattuong/ECB?style=flat)](https://github.com/dotrannhattuong/ECB) | [![arXiv](https://img.shields.io/badge/arXiv-2403.18360-b31b1b.svg)](http://arxiv.org/abs/2403.18360) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZYAhJLIkR_4) |
| MemoNav: Working Memory Model for Visual Navigation | [![GitHub](https://img.shields.io/github/stars/ZJULiHongxin/MemoNav?style=flat)](https://github.com/ZJULiHongxin/MemoNav) | [![arXiv](https://img.shields.io/badge/arXiv-2402.19161-b31b1b.svg)](http://arxiv.org/abs/2402.19161) | :heavy_minus_sign: |
| Inversion-Free Image Editing with Language-Guided Diffusion Models | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| RoHM: Robust Human Motion Reconstruction via Diffusion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sanweiliti.github.io/ROHM/ROHM.html) <br /> [![GitHub](https://img.shields.io/github/stars/sanweiliti/RoHM?style=flat)](https://github.com/sanweiliti/RoHM) | [![arXiv](https://img.shields.io/badge/arXiv-2401.08570-b31b1b.svg)](http://arxiv.org/abs/2401.08570) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hX7yO2c1hEE) |
| SceneTex: High-Quality Texture Synthesis for Indoor Scenes via Diffusion Priors | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://daveredrum.github.io/SceneTex/) <br /> [![GitHub](https://img.shields.io/github/stars/daveredrum/SceneTex?style=flat)](https://github.com/daveredrum/SceneTex) | [![arXiv](https://img.shields.io/badge/arXiv-2311.17261-b31b1b.svg)](http://arxiv.org/abs/2311.17261) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-x8oMeLovDU) |
| MAPLM: A Real-World Large-Scale Vision-Language Benchmark for Map and Traffic Scene Understanding | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs | [![GitHub](https://img.shields.io/github/stars/PurdueDigitalTwin/LaMPilot?style=flat)](https://github.com/PurdueDigitalTwin/LaMPilot) | [![arXiv](https://img.shields.io/badge/arXiv-2312.04372-b31b1b.svg)](http://arxiv.org/abs/2312.04372) | :heavy_minus_sign: |
| SuperPrimitive: Scene Reconstruction at a Primitive Level | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://makezur.github.io/SuperPrimitive/) <br /> [![GitHub](https://img.shields.io/github/stars/makezur/super_primitive?style=flat)](https://github.com/makezur/super_primitive) | [![arXiv](https://img.shields.io/badge/arXiv-2312.05889-b31b1b.svg)](http://arxiv.org/abs/2312.05889) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hKYgAf6MoB8) |
| HOLD: Category-Agnostic 3D Reconstruction of Interacting Hands and Objects from Video | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zc-alexfan.github.io/hold) <br /> [![GitHub](https://img.shields.io/github/stars/zc-alexfan/hold?style=flat)](https://github.com/zc-alexfan/hold) | [![arXiv](https://img.shields.io/badge/arXiv-2311.18448-b31b1b.svg)](http://arxiv.org/abs/2311.18448) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xm6WSkr2sIs) |
| Robust Depth Enhancement via Polarization Prompt Fusion Tuning | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lastbasket.github.io/PPFT/) <br /> [![GitHub](https://img.shields.io/github/stars/lastbasket/Polarization-Prompt-Fusion-Tuning?style=flat)](https://github.com/lastbasket/Polarization-Prompt-Fusion-Tuning) | [![arXiv](https://img.shields.io/badge/arXiv-2404.04318-b31b1b.svg)](http://arxiv.org/abs/2404.04318) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KlnD0C-VeKw) |
| MAP: MAsk-Pruning for Source-Free Model Intellectual Property Protection | [![GitHub](https://img.shields.io/github/stars/ispc-lab/MAP?style=flat)](https://github.com/ispc-lab/MAP) | [![arXiv](https://img.shields.io/badge/arXiv-2403.04149-b31b1b.svg)](http://arxiv.org/abs/2403.04149) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tBEms735eY8) |
| Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://otonari726.github.io/entitynerf/) | [![arXiv](https://img.shields.io/badge/arXiv-2403.16141-b31b1b.svg)](http://arxiv.org/abs/2403.16141) | :heavy_minus_sign: |
| LeftRefill: Filling Right Canvas based on Left Reference through Generalized Text-to-Image Diffusion Model | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ewrfcas.github.io/LeftRefill/) <br /> [![GitHub](https://img.shields.io/github/stars/ewrfcas/LeftRefill?style=flat)](https://github.com/ewrfcas/LeftRefill) | [![arXiv](https://img.shields.io/badge/arXiv-2305.11577-b31b1b.svg)](http://arxiv.org/abs/2305.11577) | :heavy_minus_sign: |
| MVBench: A Comprehensive Multi-Modal Video Understanding Benchmark | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat2) | [![arXiv](https://img.shields.io/badge/arXiv-2311.17005-b31b1b.svg)](http://arxiv.org/abs/2311.17005) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OMXlbt7A2OU) |
| EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-Centric View of Procedural Activities in Real World | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://egoexolearn.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/OpenGVLab/EgoExoLearn?style=flat)](https://github.com/OpenGVLab/EgoExoLearn) | [![arXiv](https://img.shields.io/badge/arXiv-2403.16182-b31b1b.svg)](http://arxiv.org/abs/2403.16182) | :heavy_minus_sign: |
| Puff-Net: Efficient Style Transfer with Pure Content and Style Feature Fusion Network | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Object Recognition as Next Token Prediction | [![GitHub](https://img.shields.io/github/stars/kaiyuyue/nxtp?style=flat)](https://github.com/kaiyuyue/nxtp) <br /> [![Hugging Face](https://img.shields.io/badge/🤗-model-FFD21F.svg)](https://huggingface.co/kaiyuyue/nxtp) | [![arXiv](https://img.shields.io/badge/arXiv-2312.02142-b31b1b.svg)](http://arxiv.org/abs/2312.02142) | :heavy_minus_sign: |
| Garment Recovery with Shape and Deformation Priors | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://liren2515.github.io/page/prior/prior.html) <br /> [![GitHub](https://img.shields.io/github/stars/liren2515/GarmentRecovery?style=flat)](https://github.com/liren2515/GarmentRecovery) | [![arXiv](https://img.shields.io/badge/arXiv-2311.10356-b31b1b.svg)](http://arxiv.org/abs/2311.10356) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xoO029AyC7U) |
| OmniVid: A Generative Framework for Universal Video Understanding | [![GitHub](https://img.shields.io/github/stars/wangjk666/OmniVid?style=flat)](https://github.com/wangjk666/OmniVid) | [![arXiv](https://img.shields.io/badge/arXiv-2403.17935-b31b1b.svg)](http://arxiv.org/abs/2403.17935) | :heavy_minus_sign: |
| MotionEditor: Editing Video Motion via Content-Aware Diffusion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://francis-rings.github.io/MotionEditor/) <br /> [![GitHub](https://img.shields.io/github/stars/Francis-Rings/MotionEditor?style=flat)](https://github.com/Francis-Rings/MotionEditor) | [![arXiv](https://img.shields.io/badge/arXiv-2311.18830-b31b1b.svg)](http://arxiv.org/abs/2311.18830) | :heavy_minus_sign: |
| Doubly Abductive Counterfactual Inference for Text-based Image Editing | [![GitHub](https://img.shields.io/github/stars/xuesong39/DAC?style=flat)](https://github.com/xuesong39/DAC) | [![arXiv](https://img.shields.io/badge/arXiv-2403.02981-b31b1b.svg)](http://arxiv.org/abs/2403.02981) | :heavy_minus_sign: |
| Learning to Rank Patches for Unbiased Image Redundancy Reduction | [![GitHub](https://img.shields.io/github/stars/irsLu/ltrp?style=flat)](https://github.com/irsLu/ltrp) | [![arXiv](https://img.shields.io/badge/arXiv-2404.00680-b31b1b.svg)](http://arxiv.org/abs/2404.00680) | :heavy_minus_sign: |
| SimDA: Simple Diffusion Adapter for Efficient Video Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://chenhsing.github.io/SimDA/) <br /> [![GitHub](https://img.shields.io/github/stars/ChenHsing/SimDA?style=flat)](https://github.com/ChenHsing/SimDA) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09710-b31b1b.svg)](http://arxiv.org/abs/2308.09710) | :heavy_minus_sign: |
| Misalignment-Robust Frequency Distribution Loss for Image Transformation | [![GitHub](https://img.shields.io/github/stars/eezkni/FDL?style=flat)](https://github.com/eezkni/FDL) | [![arXiv](https://img.shields.io/badge/arXiv-2402.18192-b31b1b.svg)](http://arxiv.org/abs/2402.18192) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=x31JcP0UoMI) |
| InteractDiffusion: Interaction Control in Text-to-Image Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jiuntian.github.io/interactdiffusion/) <br /> [![GitHub](https://img.shields.io/github/stars/jiuntian/interactdiffusion?style=flat)](https://github.com/jiuntian/interactdiffusion) <br /> [![Hugging Face](https://img.shields.io/badge/🤗-demo-FFD21F.svg)](https://huggingface.co/interactdiffusion/interactdiffusion) | [![arXiv](https://img.shields.io/badge/arXiv-2312.05849-b31b1b.svg)](http://arxiv.org/abs/2312.05849) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Uunzufq8m6Y) |
| Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yusufma03.github.io/projects/hdp/) <br /> [![GitHub](https://img.shields.io/github/stars/dyson-ai/hdp?style=flat)](https://github.com/dyson-ai/hdp) | [![arXiv](https://img.shields.io/badge/arXiv-2403.03890-b31b1b.svg)](http://arxiv.org/abs/2403.03890) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=f6vmzd3AKwY) |
| Multi-Scale Video Anomaly Detection by Multi-Grained Spatio-Temporal Representation Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Consistent Prompting for Rehearsal-Free Continual Learning | [![GitHub](https://img.shields.io/github/stars/Zhanxin-Gao/CPrompt?style=flat)](https://github.com/Zhanxin-Gao/CPrompt) | [![arXiv](https://img.shields.io/badge/arXiv-2403.08568-b31b1b.svg)](http://arxiv.org/abs/2403.08568) | :heavy_minus_sign: |
| TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models | [![GitHub](https://img.shields.io/github/stars/ModelTC/TFMQ-DM?style=flat)](https://github.com/ModelTC/TFMQ-DM) | [![arXiv](https://img.shields.io/badge/arXiv-2311.16503-b31b1b.svg)](http://arxiv.org/abs/2311.16503) | :heavy_minus_sign: |
| PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2403.07589-b31b1b.svg)](http://arxiv.org/abs/2403.07589) | :heavy_minus_sign: |
| Towards Better Vision-Inspired Vision-Language Models | :heavy_minus_sign: | :heavy_minus_sign: | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=d91e0EwAIZc) |
| An Upload-Efficient Scheme for Transferring Knowledge from a Server-Side Pre-trained Generator to Clients in Heterogeneous Federated Learning | [![GitHub](https://img.shields.io/github/stars/TsingZ0/FedKTL?style=flat)](https://github.com/TsingZ0/FedKTL) | [![arXiv](https://img.shields.io/badge/arXiv-2403.15760-b31b1b.svg)](http://arxiv.org/abs/2403.15760) | :heavy_minus_sign: |
| Bidirectional Autoregessive Diffusion Model for Dance Generation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| SEAS: ShapE-Aligned Supervision for Person Re-Identification | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| XScale-NVS: Cross-Scale Novel View Synthesis with Hash Featurized Manifold | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://xscalenvs.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/THU-luvision/XScale-NVS?style=flat)](https://github.com/THU-luvision/XScale-NVS) | [![arXiv](https://img.shields.io/badge/arXiv-2403.19517-b31b1b.svg)](http://arxiv.org/abs/2403.19517) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=x06QnYyIiGQ) |
| Objects as Volumes: A Stochastic Geometry View of Opaque Solids | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://imaging.cs.cmu.edu/volumetric_opaque_solids/) <br /> [![GitHub](https://img.shields.io/github/stars/cmu-ci-lab/volumetric_opaque_solids?style=flat)](https://github.com/cmu-ci-lab/volumetric_opaque_solids) | [![arXiv](https://img.shields.io/badge/arXiv-2312.15406-b31b1b.svg)](http://arxiv.org/abs/2312.15406) | :heavy_minus_sign: |
| Neural Refinement for Absolute Pose Regression with Feature Synthesis | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://nefes.active.vision/) <br /> [![GitHub](https://img.shields.io/github/stars/ActiveVisionLab/NeFeS?style=flat)](https://github.com/ActiveVisionLab/NeFeS) | [![arXiv](https://img.shields.io/badge/arXiv-2303.10087-b31b1b.svg)](http://arxiv.org/abs/2303.10087) | :heavy_minus_sign: |
| TextNeRF: A Novel Scene-Text Image Synthesis Method based on Neural Radiance Fields | [![GitHub](https://img.shields.io/github/stars/cuijl-ai/TextNeRF?style=flat)](https://github.com/cuijl-ai/TextNeRF) | :heavy_minus_sign: | :heavy_minus_sign: |
| EpiDiff: Enhancing Multi-View Synthesis via Localized Epipolar-Constrained Diffusion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://huanngzh.github.io/EpiDiff/) <br /> [![GitHub](https://img.shields.io/github/stars/huanngzh/EpiDiff?style=flat)](https://github.com/huanngzh/EpiDiff) | [![arXiv](https://img.shields.io/badge/arXiv-2312.06725-b31b1b.svg)](http://arxiv.org/abs/2312.06725) | :heavy_minus_sign: |
| No more Ambiguity in 360° Room Layout via Bi-Layout Estimation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://liagm.github.io/Bi_Layout/) | [![arXiv](https://img.shields.io/badge/arXiv-2404.09993-b31b1b.svg)](http://arxiv.org/abs/2404.09993) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pgR-IgOcNgg) |
| A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://stem-inv.github.io/page/) <br /> [![GitHub](https://img.shields.io/github/stars/STEM-Inv/stem-inv?style=flat)](https://github.com/STEM-Inv/stem-inv) | [![arXiv](https://img.shields.io/badge/arXiv-2312.05856-b31b1b.svg)](http://arxiv.org/abs/2312.05856) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FZnDn4wBPqY) |
| Pixel-Level Semantic Correspondence through Layout-Aware Representation Learning and Multi-Scale Matching Integration | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://freedomgu.github.io/DiffPortrait3D/) <br /> [![GitHub](https://img.shields.io/github/stars/FreedomGu/DiffPortrait3D?style=flat)](https://github.com/FreedomGu/DiffPortrait3D) | [![arXiv](https://img.shields.io/badge/arXiv-2312.13016-b31b1b.svg)](http://arxiv.org/abs/2312.13016) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mI8RJ_f3Csw) |
| Learning Group Activity Features through Person Attribute Prediction | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.toyota-ti.ac.jp/Lab/Denshi/iim/ukita/selection/CVPR2024-GAFL.html) <br /> [![GitHub](https://img.shields.io/github/stars/chihina/GAFL-CVPR2024?style=flat)](https://github.com/chihina/GAFL-CVPR2024) | [![arXiv](https://img.shields.io/badge/arXiv-2403.02753-b31b1b.svg)](http://arxiv.org/abs/2403.02753) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Jnnhvsrij74) |
| Real-World Efficient Blind Motion Deblurring via Blur Pixel Discretization | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2404.12168-b31b1b.svg)](http://arxiv.org/abs/2404.12168) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=GB-88qsaYtg) |
| Egocentric Full Body Motion Capture with FisheyeViT and Diffusion-Based Motion Refinement |  |  |  |
| Revamping Federated Learning Security from a Defender's Perspective: A Unified Defense with Homomorphic Encrypted Data Space |  |  |  |
| Learning Transferable Negative Prompts for Out-of-Distribution Detection |  |  |  |
| Training-free Pretrained Model Merging |  |  |  |
| How Far Can We Compress Instant NGP-Based NeRF? |  |  |  |
| Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training |  |  |  |
| Class Incremental Learning with Multi-Teacher Distillation |  |  |  |
| Unsupervised Semantic Segmentation Through Depth-Guided Feature Correlation and Sampling |  |  |  |
| CLIP-Driven Open-Vocabulary 3D Scene Graph Generation via Cross-Modality Contrastive Learning |  |  |  |
| Self-Distilled Masked Auto-Encoders are Efficient  Video Anomaly Detectors |  |  |  |
| Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling |  |  |  |
| Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians |  |  |  |
| LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking |  |  |  |
| A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models |  |  |  |
| FedMef: Towards Memory-efficient Federated Dynamic Pruning |  |  |  |
| 3D Human Pose Perception from Egocentric Stereo Videos |  |  |  |
| EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams |  |  |  |
| Federated Online Adaptation for Deep Stereo |  |  |  |
| Super-Resolution Reconstruction from Bayer-Pattern Spike Streams |  |  |  |
| A Vision Check-up for Language Models |  |  |  |
| SCE-MAE: Selective Correspondence Enhancement with Masked Autoencoder for Self-Supervised Landmark Estimation |  |  |  |
| RealNet: A Feature Selection Network with Realistic Synthetic Anomaly for Anomaly Detection |  |  |  |
| Using Human Feedback to Fine-tune Diffusion Models  without Any Reward Model |  |  |  |
| OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation |  |  |  |
| Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation |  |  |  |
| Robust Synthetic-to-Real Transfer for Stereo Matching |  |  |  |
| UniMODE: Unified Monocular 3D Object Detection |  |  |  |
| A Dynamic Kernel Prior Model for Unsupervised Blind Image Super-Resolution |  |  |  |
| Audio-Visual Segmentation via Unlabeled Frame Exploitation |  |  |  |
| EFHQ: Multi-purpose ExtremePose-Face-HQ dataset |  |  |  |
| Editable Scene Simulation for Autonomous Driving via LLM-Agent Collaboration |  |  |  |
| Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs |  |  |  |
| Enhancing Quality of Compressed Images by Mitigating Enhancement Bias Towards Compression Domain |  |  |  |
| Precise Image Editing via Recognition and Generation Tasks |  |  |  |
| Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo |  |  |  |
| Leak and Learn: An Attacker's Cookbook to Train Using Leaked Data from Federated Learning |  |  |  |
| EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised Semantic Segmentation |  |  |  |
| A Physics-informed Low-rank Deep Neural Network for Blind and Universal Lens Aberration Correction |  |  |  |
| The devil is in the fine-grained details: Evaluating open-vocabulary object detectors for fine-grained understanding |  |  |  |
| VRP-SAM: SAM with Visual Reference Prompt |  |  |  |
| Vanishing-Point-Guided Video Semantic Segmentation of Driving Scenes |  |  |  |
| Data Poisoning based Backdoor Attacks to Contrastive Learning |  |  |  |
| A2XP: Towards Private Domain Generalization |  |  |  |
| ParameterNet: Parameters Are All You Need for Large-scale Visual Pretraining of Mobile Networks |  |  |  |
| An Empirical Study of Scaling Law for Scene Text Recognition |  |  |  |
| FMA-Net: Flow Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring |  |  |  |
| Deep Imbalanced Regression via Hierarchical Classification Adjustment |  |  |  |
| Don't Look into the Dark: Latent Codes for Pluralistic Image Inpainting |  |  |  |
| FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication |  |  |  |
| Modular Blind Video Quality Assessment |  |  |  |
| GlitchBench: Can large multimodal models detect video game glitches? |  |  |  |
| LAA-Net: Localized Artifact Attention Network for Quality-Agnostic and Generalizable Deepfake Detection |  |  |  |
| DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields |  |  |  |
| DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior |  |  |  |
| Open-vocabulary object 6D pose estimation |  |  |  |
| GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting |  |  |  |
| Motion2VecSets: 4D Latent Vector Set Diffusion for Non-rigid Shape Reconstruction and Tracking |  |  |  |
| From Activation to Initialization: Scaling Insights for Optimizing Neural Fields |  |  |  |
| MoST: Multi-modality Scene Tokenization for Motion Prediction |  |  |  |
| RankED: Addressing Imbalance and Uncertainty in Edge Detection Using Ranking-based Losses |  |  |  |
| Efficient Model Stealing Defense with Noise Transition Matrix |  |  |  |
| Streaming Dense Video Captioning |  |  |  |
| End-to-End Spatio-Temporal Action Localisation with Video Transformers |  |  |  |
| Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction |  |  |  |
| Morphological Prototyping for Unsupervised Slide Representation Learning in Computational Pathology |  |  |  |
| DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision |  |  |  |
| ZONE: Zero-Shot Instruction-Guided Local Editing |  |  |  |
| DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving |  |  |  |
| Describing Differences in Image Sets with Natural Language |  |  |  |
| Shadows Don’t Lie and Lines Can't Bend! Generative Models don't know Projective Geometry...for now |  |  |  |
| Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging Perturbations That Efficiently Fool Customized Diffusion Models |  |  |  |
| BioCLIP: A Vision Foundation Model for the Tree of Life |  |  |  |
| Dual-View Visual Contextualization for Web Navigation |  |  |  |
| Learned representation-guided diffusion models for large-image generation |  |  |  |
| Desigen: A Pipeline for Controllable Design Template Generation |  |  |  |
| From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations |  |  |  |
| Multiview Aerial Visual RECognition (MAVREC) Dataset: Can Multi-view Improve Aerial Visual Perception? |  |  |  |
| A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions |  |  |  |
| Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives |  |  |  |
| Gaussian Shell Maps for Efficient 3D Human Generation |  |  |  |
| 4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling |  |  |  |
| Improving Single Domain-Generalized Object Detection: A Focus on Diversification and Alignment |  |  |  |
| Looking Similar, Sounding Different: Leveraging Counterfactual Cross-Modal Pairs for Audiovisual Representation Learning |  |  |  |
| MeshPose: Unifying DensePose and 3D Body Mesh reconstruction |  |  |  |
| MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation |  |  |  |
| Taming Self-Training for Open-Vocabulary Object Detection |  |  |  |
| Generating Enhanced Negatives for Training Language-Based Object Detectors |  |  |  |
| Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark |  |  |  |
| Practical Measurements of Translucent Materials with Inter-Pixel Translucency Prior |  |  |  |
| Unsupervised Universal Image Segmentation |  |  |  |
| Gated Fields: Learning Scene Reconstruction from Gated Videos |  |  |  |
| FADES: Fair Disentanglement with Sensitive Relevance |  |  |  |
| MonoNPHM: Dynamic Head Reconstruction from Monocular Videos |  |  |  |
| Codebook Transfer with Part-of-Speech for Vector-Quantized Image Modeling |  |  |  |
| TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding |  |  |  |
| Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation guided by the Characteristic Dance Primitives |  |  |  |
| ProxyCap: Real-time Monocular Full-body Capture in World Space via Human-Centric Proxy-to-Motion Learning |  |  |  |
| HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation |  |  |  |
| DeiT-LT: Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets |  |  |  |
| Validating Privacy-Preserving Face Recognition under a Minimum Assumption |  |  |  |
| Low-power, Continuous Remote Behavioral Localization with Event Cameras |  |  |  |
| Multimodal Sense-Informed Prediction of 3D Human Motions |  |  |  |
| MiKASA: Multi-Key-Anchor Scene-Aware Transformer for 3D Visual Grounding |  |  |  |
| LIVE: Online Large Video-Language Model for Streaming Video |  |  |  |
| MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception |  |  |  |
| SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame Interpolation |  |  |  |
| CLIP-KD: An Empirical Study of CLIP Model Distillation |  |  |  |
| InNeRF360: Text-Guided 3D-Consistent Object Inpainting on 360-degree Neural Radiance Fields |  |  |  |
| Total-Decom: Decomposed 3D Scene Reconstruction with Minimal Interaction |  |  |  |
| CAPE: CAM as a Probabilistic Ensemble for Enhanced DNN Interpretation |  |  |  |
| GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos |  |  |  |
| CN-RMA: Combined Network with Ray Marching Aggregation for 3D Indoor Object Detection from Multi-view Images |  |  |  |
| Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing |  |  |  |
| LightIt: Illumination Modeling and Control for Diffusion Models |  |  |  |
| Test-Time Zero-Shot Temporal Action Localization |  |  |  |
| HIVE: Harnessing Human Feedback for Instructional Visual Editing |  |  |  |
| NeRF Director: Revisiting View Selection in Neural Volume Rendering |  |  |  |
| Unbiased Faster R-CNN for Single-source Domain Generalized Object Detection |  |  |  |
| Collaborative Semantic Occupancy Prediction with Hybrid Feature Fusion in Connected Automated Vehicles |  |  |  |
| GAFusion: Adaptive Fusing LiDAR and Camera with Multiple Guidance for 3D Object Detection |  |  |  |
| NIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis |  |  |  |
| Prompting Vision Foundation Models for Pathology Image Analysis |  |  |  |
| XFibrosis: Explicit Vessel-Fiber Modeling for Fibrosis Staging from Liver Pathology Images |  |  |  |
| The More You See in 2D, the More You Perceive in 3D |  |  |  |
| LLM4SGG: Large Language Models for Weakly Supervised Scene Graph Generation |  |  |  |
| Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use |  |  |  |
| LEOD: Label-Efficient Object Detection for Event Cameras |  |  |  |
| Producing and Leveraging Online Map Uncertainty in Trajectory Prediction |  |  |  |
| SPAD: Spatially Aware Multiview Diffusers |  |  |  |
| CDMAD: Class-Distribution-Mismatch-Aware Debiasing for Class-Imbalanced Semi-Supervised Learning |  |  |  |
| MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding |  |  |  |
| Towards More Unified In-context Visual Understanding |  |  |  |
| PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF |  |  |  |
| MuseChat: A Conversational Music Recommendation System for Videos |  |  |  |
| Rotation-Agnostic Image Representation Learning for Digital Pathology |  |  |  |
| CodedEvents: Optimal Point-Spread-Function Engineering for 3D-Tracking with Event Cameras |  |  |  |
| The Audio-Visual Conversational Graph: From an Egocentric-Exocentric Perspective |  |  |  |
| LEDITS++: Limitless Image Editing using Text-to-Image Models |  |  |  |
| MemFlow: Optical Flow Estimation and Prediction with Memory |  |  |  |
| SemCity: Semantic Scene Generation with Triplane Diffusion |  |  |  |
| UnScene3D: Unsupervised 3D Instance Segmentation for Indoor Scenes |  |  |  |
| TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models |  |  |  |
| Boosting Diffusion Models with Moving Average Sampling in Frequency Domain |  |  |  |
| Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution |  |  |  |
| SD-DiT: Unleashing the Power of Self-supervised Discrimination in Diffusion Transformer |  |  |  |
| VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation |  |  |  |
| Riemannian Multinomial Logistics Regression for SPD Neural Networks |  |  |  |
| VideoMAC: Video Masked Autoencoders Meet ConvNets |  |  |  |
| Generalized Large-Scale Data Condensation via Various Backbone and Statistical Matching |  |  |  |
| ICP-Flow: LiDAR Scene Flow Estimation with ICP |  |  |  |
| GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding |  |  |  |
| JRDB-Social: A Multifaceted Robotic Dataset for Understanding of Context and Dynamics of Human Interactions Within Social Groups |  |  |  |
| MOHO: Learning Single-view Hand-held Object Reconstruction with Multi-view Occlusion-Aware Supervision |  |  |  |
| Exploring Region-Word Alignment in Built-in Detector for Open-Vocabulary Object Detection |  |  |  |
| CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive Self-Supervised Transformers |  |  |  |
| MoST: Motion Style Transformer between Diverse Action Contents |  |  |  |
| Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications |  |  |  |
| Low-Resource Vision Challenges for Foundation Models |  |  |  |
| AdaRevD: Adaptive Patch Exiting Reversible Decoder Pushes the Limit of Image Deblurring |  |  |  |
| KP-RED: Exploiting Semantic Keypoints for Joint 3D Shape Retrieval and Deformation |  |  |  |
| MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures |  |  |  |
| DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning |  |  |  |
| SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation |  |  |  |
| Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld |  |  |  |
| POCE: Primal Policy Optimization with Conservative Estimation for Multi-constraint Offline Reinforcement Learning |  |  |  |
| Bayesian Differentiable Physics for Cloth Digitalization |  |  |  |
| Monocular Identity-Conditioned Facial Reflectance Reconstruction |  |  |  |
| Ensemble Diversity Facilitates Adversarial Transferability |  |  |  |
| From a Bird’s Eye View to See: Joint Camera and Subject Registration without the Camera Calibration |  |  |  |
| Accept the Modality Gap: An Exploration in the Hyperbolic Space |  |  |  |
| Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance |  |  |  |
| PHYSCENE: Physically Interactable 3D Scene Synthesis for Embodied AI |  |  |  |
| 6D-Diff: A Keypoint Diffusion Framework for 6D Object Pose Estimation |  |  |  |
| Multi-criteria Token Fusion with One-step-ahead Attention for Efficient Vision Transformers |  |  |  |
| DIMAT: Decentralized Iterative Merging-And-Training for Deep Learning Models |  |  |  |
| A Semi-supervised Nighttime Dehazing Baseline with Spatial-Frequency Aware and Realistic Brightness Constraint |  |  |  |
| A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose Estimation |  |  |  |
| OpticalDR: A Deep Optical Imaging Model for Privacy-Protective Depression Recognition |  |  |  |
| Enhancing the Power of OOD Detection via Sample-Aware Model Selection |  |  |  |
| SPU-PMD: Self-Supervised Point Cloud Upsampling via Progressive Mesh Deformation |  |  |  |
| Hearing Anything Anywhere |  |  |  |
| MICap: A Unified Model for Identity-aware Movie Descriptions |  |  |  |
| SD2Event: Self-supervised Learning of Dynamic Detectors and Contextual Descriptors for Event Cameras |  |  |  |
| Prompt-Free Diffusion: Taking “Text” out of Text-to-Image Diffusion Models |  |  |  |
| MeaCap: Memory-Augmented Zero-shot Image Captioning |  |  |  |
| Re-thinking Data Availability Attacks Against Deep Neural Networks |  |  |  |
| Self-Supervised Facial Representation Learning with Facial Region Awareness |  |  |  |
| Breathing Life Into Sketches Using Text-to-Video Priors |  |  |  |
| Abductive Ego-View Accident Video Understanding for Safe Driving Perception |  |  |  |
| FedHCA$^2$: Towards Hetero-Client Federated Multi-Task Learning |  |  |  |
| Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching |  |  |  |
| MCPNet: An Interpretable Classifier via Multi-Level Concept Prototypes |  |  |  |
| 3D Feature Tracking via Event Camera |  |  |  |
| Seeing Unseen: Discover Novel Biomedical Concepts via Geometry-Constrained Probabilistic Modeling |  |  |  |
| Segment Any Event Streams via Weighted Adaptation of Pivotal Tokens |  |  |  |
| DETRs Beat YOLOs on Real-time Object Detection |  |  |  |
| Neural Clustering based Visual Representation Learning |  |  |  |
| I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions |  |  |  |
| AvatarGPT: All-in-One Framework for Motion Understanding, Planning, Generation and Beyond |  |  |  |
| De-confounded Data-free Knowledge Distillation for Handling Distribution Shifts |  |  |  |
| HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data |  |  |  |
| Think Twice Before Selection: Federated Evidential Active Learning for Medical Image Analysis with Domain Shifts |  |  |  |
| OHTA: One-shot Hand Avatar via Data-driven Implicit Priors |  |  |  |
| Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation |  |  |  |
| Exploring Vision Transformers for 3D Human Motion-Language Models with Motion Patches |  |  |  |
| Structure-Guided Adversarial Training of Diffusion Models |  |  |  |
| MonoHair: High-Fidelity Hair Modeling from a Monocular Video |  |  |  |
| CMA: A Chromaticity Map Adapter for Robust Detection of Screen-Recapture Document Images |  |  |  |
| MovieChat: From Dense Token to Sparse Memory for Long Video Understanding |  |  |  |
| Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning |  |  |  |
| Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation |  |  |  |
| No Time to Train: Empowering Non-Parametric Networks for Few-shot 3D Scene Segmentation |  |  |  |
| Brain Decodes Deep Nets |  |  |  |
| Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models |  |  |  |
| EvalCrafter: Benchmarking and Evaluating Large Video Generation Models |  |  |  |
| MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation |  |  |  |
| PH-Net: Semi-Supervised Breast Lesion Segmentation via Patch-wise Hardness |  |  |  |
| Towards Text-guided 3D Scene Composition |  |  |  |
| MemSAM: Taming Segment Anything Model for Echocardiography Video Segmentation |  |  |  |
| Structure Matters: Tackling the Semantic Discrepancy in Diffusion Models for Image Inpainting |  |  |  |
| SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM |  |  |  |
| Density-Guided Semi-Supervised 3D Semantic Segmentation with Dual-Space Hardness Sampling |  |  |  |
| From Correspondences to Pose: Non-minimal Certifiably Optimal Relative Pose without Disambiguation |  |  |  |
| Visual Anagrams: Synthesizing Multi-View Optical Illusions with Diffusion Models |  |  |  |
| SI-MIL: Taming Deep MIL for Self-Interpretability in Gigapixel Histopathology |  |  |  |
| Low-Latency Neural Stereo Streaming |  |  |  |
| mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration |  |  |  |
| Forgery-aware Adaptive Transformer for Generalizable Synthetic Image Detection |  |  |  |
| OmniParser: A Unified Framework for Text Spotting, Key Information Extraction and Table Recognition |  |  |  |
| Transferable and Principled Efficiency for Open-Vocabulary Segmentation |  |  |  |
| LowRankOcc: Tensor Decomposition and Low-Rank Recovery for Vision-based 3D Semantic Occupancy Prediction |  |  |  |
| SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities |  |  |  |
| Taming Mode Collapse in Score Distillation for Text-to-3D Generation |  |  |  |
| HAVE-FUN: Human Avatar Reconstruction from Few-Shot Unconstrained Images |  |  |  |
| Sequential Modeling Enables Scalable Learning for Large Vision Models |  |  |  |
| Distraction is All You Need: Memory-Efficient Image Immunization against Diffusion-Based Image Editing |  |  |  |
| OmniMedVQA: A New Large-Scale Comprehensive  Evaluation Benchmark for Medical LVLM |  |  |  |
| UniPAD: A Universal Pre-training Paradigm for Autonomous Driving |  |  |  |
| Semantics, Distortion, and Style Matter: Towards Source-free UDA for Panoramic Segmentation |  |  |  |
| In2SET: Intra-Inter Similarity Exploiting Transformer for  Dual-Camera Compressive Hyperspectral Imaging |  |  |  |
