# CVPR-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
    <tr>
        <td><strong>Previous Collections</strong></td>
        <td>
            <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README_2023.md">
                <img src="http://img.shields.io/badge/CVPR-2023-0073AE.svg" alt="Conference">
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/sections/2024/main/3d-from-multi-view-and-sensors.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Image and Video Synthesis and Generation

![Section Papers](https://img.shields.io/badge/Section%20Papers-329-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-46-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-36-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-23-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| [Alchemist: Parametric Control of Material Properties with Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Sharma_Alchemist_Parametric_Control_of_Material_Properties_with_Diffusion_Models_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.prafullsharma.net/alchemist/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Sharma_Alchemist_Parametric_Control_of_Material_Properties_with_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02970-b31b1b.svg)](http://arxiv.org/abs/2312.02970) | :heavy_minus_sign: |
| [Analyzing and Improving the Training Dynamics of Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Karras_Analyzing_and_Improving_the_Training_Dynamics_of_Diffusion_Models_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/NVlabs/edm2?style=flat)](https://github.com/NVlabs/edm2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Karras_Analyzing_and_Improving_the_Training_Dynamics_of_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02696-b31b1b.svg)](http://arxiv.org/abs/2312.02696) | :heavy_minus_sign: |
| [Attention Calibration for Disentangled Text-to-Image Personalization](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Attention_Calibration_for_Disentangled_Text-to-Image_Personalization_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/Monalissaa/DisenDiff?style=flat)](https://github.com/Monalissaa/DisenDiff) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Attention_Calibration_for_Disentangled_Text-to-Image_Personalization_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.18551-b31b1b.svg)](http://arxiv.org/abs/2403.18551) | :heavy_minus_sign: |
| [FreeU: Free Lunch in Diffusion U-Net](https://openaccess.thecvf.com/content/CVPR2024/html/Si_FreeU_Free_Lunch_in_Diffusion_U-Net_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://chenyangsi.top/FreeU) <br /> [![GitHub](https://img.shields.io/github/stars/ChenyangSi/FreeU?style=flat)](https://github.com/ChenyangSi/FreeU) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Si_FreeU_Free_Lunch_in_Diffusion_U-Net_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.11497-b31b1b.svg)](http://arxiv.org/abs/2309.11497) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-CZ5uWxvX30) |
| [Generative Image Dynamics](https://openaccess.thecvf.com/content/CVPR2024/html/Li_Generative_Image_Dynamics_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://generative-dynamics.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/fltwr/generative-image-dynamics?style=flat)](https://github.com/fltwr/generative-image-dynamics) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Generative_Image_Dynamics_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.07906-b31b1b.svg)](http://arxiv.org/abs/2309.07906) | :heavy_minus_sign: |
| [Instruct-Imagen: Image Generation with Multi-Modal Instruction](https://openaccess.thecvf.com/content/CVPR2024/html/Hu_Instruct-Imagen_Image_Generation_with_Multi-modal_Instruction_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://instruct-imagen.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_Instruct-Imagen_Image_Generation_with_Multi-modal_Instruction_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.01952-b31b1b.svg)](http://arxiv.org/abs/2401.01952) | :heavy_minus_sign: |
| [NoiseCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions in Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Dalva_NoiseCLR_A_Contrastive_Learning_Approach_for_Unsupervised_Discovery_of_Interpretable_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://noiseclr.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Dalva_NoiseCLR_A_Contrastive_Learning_Approach_for_Unsupervised_Discovery_of_Interpretable_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.05390-b31b1b.svg)](http://arxiv.org/abs/2312.05390) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=RA2KzZ25F5I) |
| [Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following](https://openaccess.thecvf.com/content/CVPR2024/html/Feng_Ranni_Taming_Text-to-Image_Diffusion_for_Accurate_Instruction_Following_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ranni-t2i.github.io/Ranni/) <br /> [![GitHub](https://img.shields.io/github/stars/ali-vilab/Ranni?style=flat)](https://github.com/ali-vilab/Ranni) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Feng_Ranni_Taming_Text-to-Image_Diffusion_for_Accurate_Instruction_Following_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.17002-b31b1b.svg)](http://arxiv.org/abs/2311.17002) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=1IIat83Atjk) |
| [Style Aligned Image Generation via Shared Attention](https://openaccess.thecvf.com/content/CVPR2024/html/Hertz_Style_Aligned_Image_Generation_via_Shared_Attention_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://style-aligned-gen.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/google/style-aligned?style=flat)](https://github.com/google/style-aligned) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Hertz_Style_Aligned_Image_Generation_via_Shared_Attention_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02133-b31b1b.svg)](http://arxiv.org/abs/2312.02133) | :heavy_minus_sign: |
| [Visual Anagrams: Generating Multi-View Optical Illusions with Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Geng_Visual_Anagrams_Generating_Multi-View_Optical_Illusions_with_Diffusion_Models_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dangeng.github.io/visual_anagrams/) <br /> [![GitHub](https://img.shields.io/github/stars/dangeng/visual_anagrams?style=flat)](https://github.com/dangeng/visual_anagrams) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Geng_Visual_Anagrams_Generating_Multi-View_Optical_Illusions_with_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.17919-b31b1b.svg)](http://arxiv.org/abs/2311.17919) | :heavy_minus_sign: |
| [Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Ling_Align_Your_Gaussians_Text-to-4D_with_Dynamic_3D_Gaussians_and_Composed_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Ling_Align_Your_Gaussians_Text-to-4D_with_Dynamic_3D_Gaussians_and_Composed_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.13763-b31b1b.svg)](http://arxiv.org/abs/2312.13763) | :heavy_minus_sign: |
| [Amodal Completion via Progressive Mixed Context Diffusion](https://openaccess.thecvf.com/content/CVPR2024/html/Xu_Amodal_Completion_via_Progressive_Mixed_Context_Diffusion_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://k8xu.github.io/amodal/) <br /> [![GitHub](https://img.shields.io/github/stars/k8xu/amodal?style=flat)](https://github.com/k8xu/amodal) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Amodal_Completion_via_Progressive_Mixed_Context_Diffusion_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.15540-b31b1b.svg)](http://arxiv.org/abs/2312.15540) | :heavy_minus_sign: |
| [CLiC: Concept Learning in Context](https://openaccess.thecvf.com/content/CVPR2024/html/Safaee_CLiC_Concept_Learning_in_Context_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mehdi0xc.github.io/clic/) <br /> [![GitHub](https://img.shields.io/github/stars/Mehdi0xC/clic?style=flat)](https://github.com/Mehdi0xC/clic) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Safaee_CLiC_Concept_Learning_in_Context_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.17083-b31b1b.svg)](http://arxiv.org/abs/2311.17083) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8g--nx3RyEQ) |
| [Clockwork Diffusion: Efficient Generation with Model-Step Distillation](https://openaccess.thecvf.com/content/CVPR2024/html/Habibian_Clockwork_Diffusion_Efficient_Generation_With_Model-Step_Distillation_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/Qualcomm-AI-research/clockwork-diffusion?style=flat)](https://github.com/Qualcomm-AI-research/clockwork-diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Habibian_Clockwork_Diffusion_Efficient_Generation_With_Model-Step_Distillation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.08128-b31b1b.svg)](http://arxiv.org/abs/2312.08128) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jdpOFQn8zKw) |
| [Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis](https://openaccess.thecvf.com/content/CVPR2024/html/Lu_Coarse-to-Fine_Latent_Diffusion_for_Pose-Guided_Person_Image_Synthesis_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/YanzuoLu/CFLD?style=flat)](https://github.com/YanzuoLu/CFLD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_Coarse-to-Fine_Latent_Diffusion_for_Pose-Guided_Person_Image_Synthesis_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2402.18078-b31b1b.svg)](http://arxiv.org/abs/2402.18078) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZqMdjfzaj-I) |
| [CoDeF: Content Deformation Fields for Temporally Consistent Video Processing](https://openaccess.thecvf.com/content/CVPR2024/html/Ouyang_CoDeF_Content_Deformation_Fields_for_Temporally_Consistent_Video_Processing_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://qiuyu96.github.io/CoDeF/) <br /> [![GitHub](https://img.shields.io/github/stars/qiuyu96/CoDeF?style=flat)](https://github.com/qiuyu96/CoDeF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Ouyang_CoDeF_Content_Deformation_Fields_for_Temporally_Consistent_Video_Processing_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07926-b31b1b.svg)](http://arxiv.org/abs/2308.07926) | :heavy_minus_sign: |
| [Correcting Diffusion Generation through Resampling](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Correcting_Diffusion_Generation_through_Resampling_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/UCSB-NLP-Chang/diffusion_resampling?style=flat)](https://github.com/UCSB-NLP-Chang/diffusion_resampling) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Correcting_Diffusion_Generation_through_Resampling_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.06038-b31b1b.svg)](http://arxiv.org/abs/2312.06038) | :heavy_minus_sign: |
| [CosmicMan: A Text-to-Image Foundation Model for Humans](https://openaccess.thecvf.com/content/CVPR2024/html/Li_CosmicMan_A_Text-to-Image_Foundation_Model_for_Humans_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cosmicman-cvpr2024.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/cosmicman-cvpr2024/CosmicMan?style=flat)](https://github.com/cosmicman-cvpr2024/CosmicMan) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-demo-FFD21F.svg)](https://huggingface.co/spaces/cosmicman/CosmicMan-SDXL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_CosmicMan_A_Text-to-Image_Foundation_Model_for_Humans_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2404.01294-b31b1b.svg)](http://arxiv.org/abs/2404.01294) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CsZKA27tQDA) |
| [DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations](https://openaccess.thecvf.com/content/CVPR2024/html/Qi_DEADiff_An_Efficient_Stylization_Diffusion_Model_with_Disentangled_Representations_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tianhao-qi.github.io/DEADiff/) <br /> [![GitHub](https://img.shields.io/github/stars/bytedance/DEADiff?style=flat)](https://github.com/bytedance/DEADiff) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Qi_DEADiff_An_Efficient_Stylization_Diffusion_Model_with_Disentangled_Representations_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.06951-b31b1b.svg)](http://arxiv.org/abs/2403.06951) | :heavy_minus_sign: |
| [Diffusion Handles Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D](https://openaccess.thecvf.com/content/CVPR2024/html/Pandey_Diffusion_Handles_Enabling_3D_Edits_for_Diffusion_Models_by_Lifting_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://diffusionhandles.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/adobe-research/DiffusionHandles?style=flat)](https://github.com/adobe-research/DiffusionHandles) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Pandey_Diffusion_Handles_Enabling_3D_Edits_for_Diffusion_Models_by_Lifting_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02190-b31b1b.svg)](http://arxiv.org/abs/2312.02190) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OxOjiFaTSZg) |
| [DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Li_DistriFusion_Distributed_Parallel_Inference_for_High-Resolution_Diffusion_Models_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://hanlab.mit.edu/projects/distrifusion) <br /> [![GitHub](https://img.shields.io/github/stars/mit-han-lab/distrifuser?style=flat)](https://github.com/mit-han-lab/distrifuser) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_DistriFusion_Distributed_Parallel_Inference_for_High-Resolution_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2402.19481-b31b1b.svg)](http://arxiv.org/abs/2402.19481) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EZX7srDDmW0) |
| [Don't Drop Your Samples! Coherence-Aware Training Benefits Conditional Diffusion](https://openaccess.thecvf.com/content/CVPR2024/html/Dufour_Dont_Drop_Your_Samples_Coherence-Aware_Training_Benefits_Conditional_Diffusion_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://nicolas-dufour.github.io/cad) <br /> [![GitHub](https://img.shields.io/github/stars/nicolas-dufour/CAD?style=flat)](https://github.com/nicolas-dufour/CAD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Dufour_Dont_Drop_Your_Samples_Coherence-Aware_Training_Benefits_Conditional_Diffusion_CVPR_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=4Tu-x2-Zcxs) |
| [DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing](https://openaccess.thecvf.com/content/CVPR2024/html/Shi_DragDiffusion_Harnessing_Diffusion_Models_for_Interactive_Point-based_Image_Editing_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yujun-shi.github.io/projects/dragdiffusion.html) <br /> [![GitHub](https://img.shields.io/github/stars/Yujun-Shi/DragDiffusion?style=flat)](https://github.com/Yujun-Shi/DragDiffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Shi_DragDiffusion_Harnessing_Diffusion_Models_for_Interactive_Point-based_Image_Editing_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.14435-b31b1b.svg)](http://arxiv.org/abs/2306.14435) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rysOFTpDBhc) |
| [Dynamic Policy-Driven Adaptive Multi-Instance Learning for whole Slide Image Classification](https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Dynamic_Policy-Driven_Adaptive_Multi-Instance_Learning_for_Whole_Slide_Image_Classification_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://vilab.hit.edu.cn/projects/pamil/) <br /> [![GitHub](https://img.shields.io/github/stars/titizheng/PAMIL?style=flat)](https://github.com/titizheng/PAMIL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_Dynamic_Policy-Driven_Adaptive_Multi-Instance_Learning_for_Whole_Slide_Image_Classification_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.07939-b31b1b.svg)](http://arxiv.org/abs/2403.07939) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ThDuM2tJPzs) |
| [Fast ODE-based Sampling for Diffusion Models in Around 5 Steps](https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Fast_ODE-based_Sampling_for_Diffusion_Models_in_Around_5_Steps_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/zju-pi/diff-sampler?style=flat)](https://github.com/zju-pi/diff-sampler) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Fast_ODE-based_Sampling_for_Diffusion_Models_in_Around_5_Steps_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.00094-b31b1b.svg)](http://arxiv.org/abs/2312.00094) | :heavy_minus_sign: |
| [FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis](https://openaccess.thecvf.com/content/CVPR2024/html/Liang_FlowVid_Taming_Imperfect_Optical_Flows_for_Consistent_Video-to-Video_Synthesis_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jeff-liangf.github.io/projects/flowvid/) <br /> [![GitHub](https://img.shields.io/github/stars/Jeff-LiangF/FlowVid?style=flat)](https://github.com/Jeff-LiangF/FlowVid) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_FlowVid_Taming_Imperfect_Optical_Flows_for_Consistent_Video-to-Video_Synthesis_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.17681-b31b1b.svg)](http://arxiv.org/abs/2312.17681) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=y5IlgGl8Y24) |
| [Generative Powers of Ten](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Generative_Powers_of_Ten_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://powers-of-10.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/aryanmikaeili/generative_powers_of_ten?style=flat)](https://github.com/aryanmikaeili/generative_powers_of_ten) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Generative_Powers_of_Ten_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02149-b31b1b.svg)](http://arxiv.org/abs/2312.02149) | :heavy_minus_sign: |
| [HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_HumanGaussian_Text-Driven_3D_Human_Generation_with_Gaussian_Splatting_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://alvinliu0.github.io/projects/HumanGaussian) <br /> [![GitHub](https://img.shields.io/github/stars/alvinliu0/HumanGaussian?style=flat)](https://github.com/alvinliu0/HumanGaussian) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_HumanGaussian_Text-Driven_3D_Human_Generation_with_Gaussian_Splatting_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.17061-b31b1b.svg)](http://arxiv.org/abs/2311.17061) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=S3djzHoqPKY) |
| [Image Neural Field Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Image_Neural_Field_Diffusion_Models_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yinboc.github.io/infd/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Image_Neural_Field_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2406.07480-b31b1b.svg)](http://arxiv.org/abs/2406.07480) | :heavy_minus_sign: |
| [Learning Adaptive Spatial Coherent Correlations for Speech-Preserving Facial Expression Manipulation](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Learning_Adaptive_Spatial_Coherent_Correlations_for_Speech-Preserving_Facial_Expression_Manipulation_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/jianmanlincjx/ASCCL?style=flat)](https://github.com/jianmanlincjx/ASCCL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Learning_Adaptive_Spatial_Coherent_Correlations_for_Speech-Preserving_Facial_Expression_Manipulation_CVPR_2024_paper.pdf) | :heavy_minus_sign: |
| [LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching](https://openaccess.thecvf.com/content/CVPR2024/html/Liang_LucidDreamer_Towards_High-Fidelity_Text-to-3D_Generation_via_Interval_Score_Matching_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/EnVision-Research/LucidDreamer?style=flat)](https://github.com/EnVision-Research/LucidDreamer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_LucidDreamer_Towards_High-Fidelity_Text-to-3D_Generation_via_Interval_Score_Matching_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.11284-b31b1b.svg)](http://arxiv.org/abs/2311.11284) | :heavy_minus_sign: |
| [MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_MicroCinema_A_Divide-and-Conquer_Approach_for_Text-to-Video_Generation_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://wangyanhui666.github.io/MicroCinema.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_MicroCinema_A_Divide-and-Conquer_Approach_for_Text-to-Video_Generation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.18829-b31b1b.svg)](http://arxiv.org/abs/2311.18829) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=H7O-Ku_lqPA) |
| [MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis](https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_MIGC_Multi-Instance_Generation_Controller_for_Text-to-Image_Synthesis_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://migcproject.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/limuloo/MIGC?style=flat)](https://github.com/limuloo/MIGC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_MIGC_Multi-Instance_Generation_Controller_for_Text-to-Image_Synthesis_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2402.05408-b31b1b.svg)](http://arxiv.org/abs/2402.05408) | :heavy_minus_sign: |
| [One-Dimensional Adapter to Rule them All: Concepts Diffusion Models and Erasing Applications](https://openaccess.thecvf.com/content/CVPR2024/html/Lyu_One-dimensional_Adapter_to_Rule_Them_All_Concepts_Diffusion_Models_and_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lyumengyao.github.io/projects/spm) <br /> [![GitHub](https://img.shields.io/github/stars/Con6924/SPM?style=flat)](https://github.com/Con6924/SPM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Lyu_One-dimensional_Adapter_to_Rule_Them_All_Concepts_Diffusion_Models_and_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.16145-b31b1b.svg)](http://arxiv.org/abs/2312.16145) | :heavy_minus_sign: |
| [Orthogonal Adaptation for Modular Customization of Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Po_Orthogonal_Adaptation_for_Modular_Customization_of_Diffusion_Models_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://ryanpo.com/ortha/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Po_Orthogonal_Adaptation_for_Modular_Customization_of_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02432-b31b1b.svg)](http://arxiv.org/abs/2312.02432) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=4lVEFBtYE4A) |
| [PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis](https://openaccess.thecvf.com/content/CVPR2024/html/Lv_PLACE_Adaptive_Layout-Semantic_Fusion_for_Semantic_Image_Synthesis_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cszy98.github.io/PLACE/) <br /> [![GitHub](https://img.shields.io/github/stars/cszy98/PLACE?style=flat)](https://github.com/cszy98/PLACE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Lv_PLACE_Adaptive_Layout-Semantic_Fusion_for_Semantic_Image_Synthesis_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.01852-b31b1b.svg)](http://arxiv.org/abs/2403.01852) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=47mMAmclPWw) |
| [Emu Edit: Precise Image Editing via Recognition and Generation Tasks](https://openaccess.thecvf.com/content/CVPR2024/html/Sheynin_Emu_Edit_Precise_Image_Editing_via_Recognition_and_Generation_Tasks_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://emu-edit.metademolab.com/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Sheynin_Emu_Edit_Precise_Image_Editing_via_Recognition_and_Generation_Tasks_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.10089-b31b1b.svg)](http://arxiv.org/abs/2311.10089) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=UNDR55ehSYM) |
| [Predicated Diffusion: Predicate Logic-based Attention Guidance for Text-to-Image Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Sueyoshi_Predicated_Diffusion_Predicate_Logic-Based_Attention_Guidance_for_Text-to-Image_Diffusion_Models_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Sueyoshi_Predicated_Diffusion_Predicate_Logic-Based_Attention_Guidance_for_Text-to-Image_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.16117-b31b1b.svg)](http://arxiv.org/abs/2311.16117) | :heavy_minus_sign: |
| [RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Kara_RAVE_Randomized_Noise_Shuffling_for_Fast_and_Consistent_Video_Editing_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rave-video.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/RehgLab/RAVE?style=flat)](https://github.com/RehgLab/RAVE) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-demo-FFD21F.svg)](https://huggingface.co/spaces/ozgurkara/RAVE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Kara_RAVE_Randomized_Noise_Shuffling_for_Fast_and_Consistent_Video_Editing_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.04524-b31b1b.svg)](http://arxiv.org/abs/2312.04524) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2hQho5AC9T0) |
| [Readout Guidance: Learning Control from Diffusion Features](https://openaccess.thecvf.com/content/CVPR2024/html/Luo_Readout_Guidance_Learning_Control_from_Diffusion_Features_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://readout-guidance.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/google-research/readout_guidance?style=flat)](https://github.com/google-research/readout_guidance) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Luo_Readout_Guidance_Learning_Control_from_Diffusion_Features_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02150-b31b1b.svg)](http://arxiv.org/abs/2312.02150) | :heavy_minus_sign: |
| [Real-Time 3D-Aware Portrait Video Relighting](https://openaccess.thecvf.com/content/CVPR2024/html/Cai_Real-time_3D-aware_Portrait_Video_Relighting_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](http://geometrylearning.com/VideoRelighting/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Cai_Real-time_3D-aware_Portrait_Video_Relighting_CVPR_2024_paper.pdf) | :heavy_minus_sign: |
| [Residual Learning in Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Residual_Learning_in_Diffusion_Models_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Residual_Learning_in_Diffusion_Models_CVPR_2024_paper.pdf) | :heavy_minus_sign: |
| [Rethinking FID: Towards a Better Evaluation Metric for Image Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Jayasumana_Rethinking_FID_Towards_a_Better_Evaluation_Metric_for_Image_Generation_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/google-research/google-research/tree/master/cmmd) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Jayasumana_Rethinking_FID_Towards_a_Better_Evaluation_Metric_for_Image_Generation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.09603-b31b1b.svg)](http://arxiv.org/abs/2401.09603) | :heavy_minus_sign: |
| [Sat2Scene: 3D Urban Scene Generation from Satellite Images with Diffusion](https://openaccess.thecvf.com/content/CVPR2024/html/Li_Sat2Scene_3D_Urban_Scene_Generation_from_Satellite_Images_with_Diffusion_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Sat2Scene_3D_Urban_Scene_Generation_from_Satellite_Images_with_Diffusion_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.10786-b31b1b.svg)](http://arxiv.org/abs/2401.10786) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=NqFy20zjFHU) |
| [SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing](https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_SCEdit_Efficient_and_Controllable_Image_Diffusion_Generation_via_Skip_Connection_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://scedit.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/ali-vilab/SCEdit?style=flat)](https://github.com/ali-vilab/SCEdit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_SCEdit_Efficient_and_Controllable_Image_Diffusion_Generation_via_Skip_Connection_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.11392-b31b1b.svg)](http://arxiv.org/abs/2312.11392) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=XJrK3-NgB1Q) |
| [SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models](https://openaccess.thecvf.com/content/CVPR2024/html/Huang_SmartEdit_Exploring_Complex_Instruction-based_Image_Editing_with_Multimodal_Large_Language_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yuzhou914.github.io/SmartEdit/) <br /> [![GitHub](https://img.shields.io/github/stars/TencentARC/SmartEdit?style=flat)](https://github.com/TencentARC/SmartEdit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_SmartEdit_Exploring_Complex_Instruction-based_Image_Editing_with_Multimodal_Large_Language_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.06739-b31b1b.svg)](http://arxiv.org/abs/2312.06739) | :heavy_minus_sign: |
| [Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis](https://openaccess.thecvf.com/content/CVPR2024/html/Menapace_Snap_Video_Scaled_Spatiotemporal_Transformers_for_Text-to-Video_Synthesis_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://snap-research.github.io/snapvideo/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Menapace_Snap_Video_Scaled_Spatiotemporal_Transformers_for_Text-to-Video_Synthesis_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2402.14797-b31b1b.svg)](http://arxiv.org/abs/2402.14797) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=aL2zq_IKSBg) |
| [Style Injection in Diffusion: A Training-Free Approach for Adapting Large-Scale Diffusion Models for Style Transfer](https://openaccess.thecvf.com/content/CVPR2024/html/Chung_Style_Injection_in_Diffusion_A_Training-free_Approach_for_Adapting_Large-scale_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jiwoogit.github.io/StyleID_site/) <br /> [![GitHub](https://img.shields.io/github/stars/jiwoogit/StyleID?style=flat)](https://github.com/jiwoogit/StyleID) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Chung_Style_Injection_in_Diffusion_A_Training-free_Approach_for_Adapting_Large-scale_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.09008-b31b1b.svg)](http://arxiv.org/abs/2312.09008) | :heavy_minus_sign: |
| [Tackling the Singularities at the Endpoints of Time Intervals in Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Tackling_the_Singularities_at_the_Endpoints_of_Time_Intervals_in_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://pangzecheung.github.io/SingDiffusion/) <br /> [![GitHub](https://img.shields.io/github/stars/PangzeCheung/SingDiffusion?style=flat)](https://github.com/PangzeCheung/SingDiffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Tackling_the_Singularities_at_the_Endpoints_of_Time_Intervals_in_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.08381-b31b1b.svg)](http://arxiv.org/abs/2403.08381) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3Rt17MnHEJQ) |
| [Taming Stable Diffusion for Text to 360 Panorama Image Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Taming_Stable_Diffusion_for_Text_to_360_Panorama_Image_Generation_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://chengzhag.github.io/publication/panfusion/) <br /> [![GitHub](https://img.shields.io/github/stars/chengzhag/PanFusion?style=flat)](https://github.com/chengzhag/PanFusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Taming_Stable_Diffusion_for_Text_to_360_Panorama_Image_Generation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2404.07949-b31b1b.svg)](http://arxiv.org/abs/2404.07949) | :heavy_minus_sign: |
