# CVPR-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
    <tr>
        <td><strong>Previous Collections</strong></td>
        <td>
            <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README_2023.md">
                <img src="http://img.shields.io/badge/CVPR-2023-0073AE.svg" alt="Conference">
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/sections/2024/main/3d-from-multi-view-and-sensors.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Image and Video Synthesis and Generation

![Section Papers](https://img.shields.io/badge/Section%20Papers-329-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-130-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-111-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-73-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| [Alchemist: Parametric Control of Material Properties with Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Sharma_Alchemist_Parametric_Control_of_Material_Properties_with_Diffusion_Models_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.prafullsharma.net/alchemist/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Sharma_Alchemist_Parametric_Control_of_Material_Properties_with_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02970-b31b1b.svg)](http://arxiv.org/abs/2312.02970) | :heavy_minus_sign: |
| [Analyzing and Improving the Training Dynamics of Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Karras_Analyzing_and_Improving_the_Training_Dynamics_of_Diffusion_Models_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/NVlabs/edm2?style=flat)](https://github.com/NVlabs/edm2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Karras_Analyzing_and_Improving_the_Training_Dynamics_of_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02696-b31b1b.svg)](http://arxiv.org/abs/2312.02696) | :heavy_minus_sign: |
| [Attention Calibration for Disentangled Text-to-Image Personalization](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Attention_Calibration_for_Disentangled_Text-to-Image_Personalization_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/Monalissaa/DisenDiff?style=flat)](https://github.com/Monalissaa/DisenDiff) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Attention_Calibration_for_Disentangled_Text-to-Image_Personalization_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.18551-b31b1b.svg)](http://arxiv.org/abs/2403.18551) | :heavy_minus_sign: |
| [FreeU: Free Lunch in Diffusion U-Net](https://openaccess.thecvf.com/content/CVPR2024/html/Si_FreeU_Free_Lunch_in_Diffusion_U-Net_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://chenyangsi.top/FreeU) <br /> [![GitHub](https://img.shields.io/github/stars/ChenyangSi/FreeU?style=flat)](https://github.com/ChenyangSi/FreeU) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Si_FreeU_Free_Lunch_in_Diffusion_U-Net_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.11497-b31b1b.svg)](http://arxiv.org/abs/2309.11497) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-CZ5uWxvX30) |
| [Generative Image Dynamics](https://openaccess.thecvf.com/content/CVPR2024/html/Li_Generative_Image_Dynamics_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://generative-dynamics.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/fltwr/generative-image-dynamics?style=flat)](https://github.com/fltwr/generative-image-dynamics) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Generative_Image_Dynamics_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.07906-b31b1b.svg)](http://arxiv.org/abs/2309.07906) | :heavy_minus_sign: |
| [Instruct-Imagen: Image Generation with Multi-Modal Instruction](https://openaccess.thecvf.com/content/CVPR2024/html/Hu_Instruct-Imagen_Image_Generation_with_Multi-modal_Instruction_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://instruct-imagen.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_Instruct-Imagen_Image_Generation_with_Multi-modal_Instruction_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.01952-b31b1b.svg)](http://arxiv.org/abs/2401.01952) | :heavy_minus_sign: |
| [NoiseCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions in Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Dalva_NoiseCLR_A_Contrastive_Learning_Approach_for_Unsupervised_Discovery_of_Interpretable_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://noiseclr.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Dalva_NoiseCLR_A_Contrastive_Learning_Approach_for_Unsupervised_Discovery_of_Interpretable_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.05390-b31b1b.svg)](http://arxiv.org/abs/2312.05390) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=RA2KzZ25F5I) |
| [Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following](https://openaccess.thecvf.com/content/CVPR2024/html/Feng_Ranni_Taming_Text-to-Image_Diffusion_for_Accurate_Instruction_Following_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ranni-t2i.github.io/Ranni/) <br /> [![GitHub](https://img.shields.io/github/stars/ali-vilab/Ranni?style=flat)](https://github.com/ali-vilab/Ranni) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Feng_Ranni_Taming_Text-to-Image_Diffusion_for_Accurate_Instruction_Following_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.17002-b31b1b.svg)](http://arxiv.org/abs/2311.17002) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=1IIat83Atjk) |
| [Style Aligned Image Generation via Shared Attention](https://openaccess.thecvf.com/content/CVPR2024/html/Hertz_Style_Aligned_Image_Generation_via_Shared_Attention_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://style-aligned-gen.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/google/style-aligned?style=flat)](https://github.com/google/style-aligned) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Hertz_Style_Aligned_Image_Generation_via_Shared_Attention_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02133-b31b1b.svg)](http://arxiv.org/abs/2312.02133) | :heavy_minus_sign: |
| [Visual Anagrams: Generating Multi-View Optical Illusions with Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Geng_Visual_Anagrams_Generating_Multi-View_Optical_Illusions_with_Diffusion_Models_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dangeng.github.io/visual_anagrams/) <br /> [![GitHub](https://img.shields.io/github/stars/dangeng/visual_anagrams?style=flat)](https://github.com/dangeng/visual_anagrams) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Geng_Visual_Anagrams_Generating_Multi-View_Optical_Illusions_with_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.17919-b31b1b.svg)](http://arxiv.org/abs/2311.17919) | :heavy_minus_sign: |
| [Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Ling_Align_Your_Gaussians_Text-to-4D_with_Dynamic_3D_Gaussians_and_Composed_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Ling_Align_Your_Gaussians_Text-to-4D_with_Dynamic_3D_Gaussians_and_Composed_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.13763-b31b1b.svg)](http://arxiv.org/abs/2312.13763) | :heavy_minus_sign: |
| [Amodal Completion via Progressive Mixed Context Diffusion](https://openaccess.thecvf.com/content/CVPR2024/html/Xu_Amodal_Completion_via_Progressive_Mixed_Context_Diffusion_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://k8xu.github.io/amodal/) <br /> [![GitHub](https://img.shields.io/github/stars/k8xu/amodal?style=flat)](https://github.com/k8xu/amodal) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Amodal_Completion_via_Progressive_Mixed_Context_Diffusion_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.15540-b31b1b.svg)](http://arxiv.org/abs/2312.15540) | :heavy_minus_sign: |
| [CLiC: Concept Learning in Context](https://openaccess.thecvf.com/content/CVPR2024/html/Safaee_CLiC_Concept_Learning_in_Context_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mehdi0xc.github.io/clic/) <br /> [![GitHub](https://img.shields.io/github/stars/Mehdi0xC/clic?style=flat)](https://github.com/Mehdi0xC/clic) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Safaee_CLiC_Concept_Learning_in_Context_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.17083-b31b1b.svg)](http://arxiv.org/abs/2311.17083) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8g--nx3RyEQ) |
| [Clockwork Diffusion: Efficient Generation with Model-Step Distillation](https://openaccess.thecvf.com/content/CVPR2024/html/Habibian_Clockwork_Diffusion_Efficient_Generation_With_Model-Step_Distillation_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/Qualcomm-AI-research/clockwork-diffusion?style=flat)](https://github.com/Qualcomm-AI-research/clockwork-diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Habibian_Clockwork_Diffusion_Efficient_Generation_With_Model-Step_Distillation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.08128-b31b1b.svg)](http://arxiv.org/abs/2312.08128) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jdpOFQn8zKw) |
| [Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis](https://openaccess.thecvf.com/content/CVPR2024/html/Lu_Coarse-to-Fine_Latent_Diffusion_for_Pose-Guided_Person_Image_Synthesis_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/YanzuoLu/CFLD?style=flat)](https://github.com/YanzuoLu/CFLD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_Coarse-to-Fine_Latent_Diffusion_for_Pose-Guided_Person_Image_Synthesis_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2402.18078-b31b1b.svg)](http://arxiv.org/abs/2402.18078) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZqMdjfzaj-I) |
| [CoDeF: Content Deformation Fields for Temporally Consistent Video Processing](https://openaccess.thecvf.com/content/CVPR2024/html/Ouyang_CoDeF_Content_Deformation_Fields_for_Temporally_Consistent_Video_Processing_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://qiuyu96.github.io/CoDeF/) <br /> [![GitHub](https://img.shields.io/github/stars/qiuyu96/CoDeF?style=flat)](https://github.com/qiuyu96/CoDeF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Ouyang_CoDeF_Content_Deformation_Fields_for_Temporally_Consistent_Video_Processing_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07926-b31b1b.svg)](http://arxiv.org/abs/2308.07926) | :heavy_minus_sign: |
| [Correcting Diffusion Generation through Resampling](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Correcting_Diffusion_Generation_through_Resampling_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/UCSB-NLP-Chang/diffusion_resampling?style=flat)](https://github.com/UCSB-NLP-Chang/diffusion_resampling) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Correcting_Diffusion_Generation_through_Resampling_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.06038-b31b1b.svg)](http://arxiv.org/abs/2312.06038) | :heavy_minus_sign: |
| [CosmicMan: A Text-to-Image Foundation Model for Humans](https://openaccess.thecvf.com/content/CVPR2024/html/Li_CosmicMan_A_Text-to-Image_Foundation_Model_for_Humans_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cosmicman-cvpr2024.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/cosmicman-cvpr2024/CosmicMan?style=flat)](https://github.com/cosmicman-cvpr2024/CosmicMan) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-demo-FFD21F.svg)](https://huggingface.co/spaces/cosmicman/CosmicMan-SDXL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_CosmicMan_A_Text-to-Image_Foundation_Model_for_Humans_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2404.01294-b31b1b.svg)](http://arxiv.org/abs/2404.01294) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CsZKA27tQDA) |
| [DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations](https://openaccess.thecvf.com/content/CVPR2024/html/Qi_DEADiff_An_Efficient_Stylization_Diffusion_Model_with_Disentangled_Representations_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tianhao-qi.github.io/DEADiff/) <br /> [![GitHub](https://img.shields.io/github/stars/bytedance/DEADiff?style=flat)](https://github.com/bytedance/DEADiff) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Qi_DEADiff_An_Efficient_Stylization_Diffusion_Model_with_Disentangled_Representations_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.06951-b31b1b.svg)](http://arxiv.org/abs/2403.06951) | :heavy_minus_sign: |
| [Diffusion Handles Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D](https://openaccess.thecvf.com/content/CVPR2024/html/Pandey_Diffusion_Handles_Enabling_3D_Edits_for_Diffusion_Models_by_Lifting_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://diffusionhandles.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/adobe-research/DiffusionHandles?style=flat)](https://github.com/adobe-research/DiffusionHandles) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Pandey_Diffusion_Handles_Enabling_3D_Edits_for_Diffusion_Models_by_Lifting_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02190-b31b1b.svg)](http://arxiv.org/abs/2312.02190) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OxOjiFaTSZg) |
| [DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Li_DistriFusion_Distributed_Parallel_Inference_for_High-Resolution_Diffusion_Models_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://hanlab.mit.edu/projects/distrifusion) <br /> [![GitHub](https://img.shields.io/github/stars/mit-han-lab/distrifuser?style=flat)](https://github.com/mit-han-lab/distrifuser) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_DistriFusion_Distributed_Parallel_Inference_for_High-Resolution_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2402.19481-b31b1b.svg)](http://arxiv.org/abs/2402.19481) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EZX7srDDmW0) |
| [Don't Drop Your Samples! Coherence-Aware Training Benefits Conditional Diffusion](https://openaccess.thecvf.com/content/CVPR2024/html/Dufour_Dont_Drop_Your_Samples_Coherence-Aware_Training_Benefits_Conditional_Diffusion_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://nicolas-dufour.github.io/cad) <br /> [![GitHub](https://img.shields.io/github/stars/nicolas-dufour/CAD?style=flat)](https://github.com/nicolas-dufour/CAD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Dufour_Dont_Drop_Your_Samples_Coherence-Aware_Training_Benefits_Conditional_Diffusion_CVPR_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=4Tu-x2-Zcxs) |
| [DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing](https://openaccess.thecvf.com/content/CVPR2024/html/Shi_DragDiffusion_Harnessing_Diffusion_Models_for_Interactive_Point-based_Image_Editing_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yujun-shi.github.io/projects/dragdiffusion.html) <br /> [![GitHub](https://img.shields.io/github/stars/Yujun-Shi/DragDiffusion?style=flat)](https://github.com/Yujun-Shi/DragDiffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Shi_DragDiffusion_Harnessing_Diffusion_Models_for_Interactive_Point-based_Image_Editing_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.14435-b31b1b.svg)](http://arxiv.org/abs/2306.14435) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rysOFTpDBhc) |
| [Dynamic Policy-Driven Adaptive Multi-Instance Learning for whole Slide Image Classification](https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Dynamic_Policy-Driven_Adaptive_Multi-Instance_Learning_for_Whole_Slide_Image_Classification_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://vilab.hit.edu.cn/projects/pamil/) <br /> [![GitHub](https://img.shields.io/github/stars/titizheng/PAMIL?style=flat)](https://github.com/titizheng/PAMIL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_Dynamic_Policy-Driven_Adaptive_Multi-Instance_Learning_for_Whole_Slide_Image_Classification_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.07939-b31b1b.svg)](http://arxiv.org/abs/2403.07939) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ThDuM2tJPzs) |
| [Fast ODE-based Sampling for Diffusion Models in Around 5 Steps](https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Fast_ODE-based_Sampling_for_Diffusion_Models_in_Around_5_Steps_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/zju-pi/diff-sampler?style=flat)](https://github.com/zju-pi/diff-sampler) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Fast_ODE-based_Sampling_for_Diffusion_Models_in_Around_5_Steps_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.00094-b31b1b.svg)](http://arxiv.org/abs/2312.00094) | :heavy_minus_sign: |
| [FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis](https://openaccess.thecvf.com/content/CVPR2024/html/Liang_FlowVid_Taming_Imperfect_Optical_Flows_for_Consistent_Video-to-Video_Synthesis_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jeff-liangf.github.io/projects/flowvid/) <br /> [![GitHub](https://img.shields.io/github/stars/Jeff-LiangF/FlowVid?style=flat)](https://github.com/Jeff-LiangF/FlowVid) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_FlowVid_Taming_Imperfect_Optical_Flows_for_Consistent_Video-to-Video_Synthesis_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.17681-b31b1b.svg)](http://arxiv.org/abs/2312.17681) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=y5IlgGl8Y24) |
| [Generative Powers of Ten](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Generative_Powers_of_Ten_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://powers-of-10.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/aryanmikaeili/generative_powers_of_ten?style=flat)](https://github.com/aryanmikaeili/generative_powers_of_ten) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Generative_Powers_of_Ten_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02149-b31b1b.svg)](http://arxiv.org/abs/2312.02149) | :heavy_minus_sign: |
| [HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_HumanGaussian_Text-Driven_3D_Human_Generation_with_Gaussian_Splatting_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://alvinliu0.github.io/projects/HumanGaussian) <br /> [![GitHub](https://img.shields.io/github/stars/alvinliu0/HumanGaussian?style=flat)](https://github.com/alvinliu0/HumanGaussian) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_HumanGaussian_Text-Driven_3D_Human_Generation_with_Gaussian_Splatting_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.17061-b31b1b.svg)](http://arxiv.org/abs/2311.17061) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=S3djzHoqPKY) |
| [Image Neural Field Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Image_Neural_Field_Diffusion_Models_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yinboc.github.io/infd/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Image_Neural_Field_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2406.07480-b31b1b.svg)](http://arxiv.org/abs/2406.07480) | :heavy_minus_sign: |
| [Learning Adaptive Spatial Coherent Correlations for Speech-Preserving Facial Expression Manipulation](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Learning_Adaptive_Spatial_Coherent_Correlations_for_Speech-Preserving_Facial_Expression_Manipulation_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/jianmanlincjx/ASCCL?style=flat)](https://github.com/jianmanlincjx/ASCCL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Learning_Adaptive_Spatial_Coherent_Correlations_for_Speech-Preserving_Facial_Expression_Manipulation_CVPR_2024_paper.pdf) | :heavy_minus_sign: |
| [LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching](https://openaccess.thecvf.com/content/CVPR2024/html/Liang_LucidDreamer_Towards_High-Fidelity_Text-to-3D_Generation_via_Interval_Score_Matching_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/EnVision-Research/LucidDreamer?style=flat)](https://github.com/EnVision-Research/LucidDreamer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_LucidDreamer_Towards_High-Fidelity_Text-to-3D_Generation_via_Interval_Score_Matching_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.11284-b31b1b.svg)](http://arxiv.org/abs/2311.11284) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zToxGi_hNWs) |
| [MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_MicroCinema_A_Divide-and-Conquer_Approach_for_Text-to-Video_Generation_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://wangyanhui666.github.io/MicroCinema.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_MicroCinema_A_Divide-and-Conquer_Approach_for_Text-to-Video_Generation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.18829-b31b1b.svg)](http://arxiv.org/abs/2311.18829) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=H7O-Ku_lqPA) |
| [MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis](https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_MIGC_Multi-Instance_Generation_Controller_for_Text-to-Image_Synthesis_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://migcproject.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/limuloo/MIGC?style=flat)](https://github.com/limuloo/MIGC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_MIGC_Multi-Instance_Generation_Controller_for_Text-to-Image_Synthesis_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2402.05408-b31b1b.svg)](http://arxiv.org/abs/2402.05408) | :heavy_minus_sign: |
| [One-Dimensional Adapter to Rule them All: Concepts Diffusion Models and Erasing Applications](https://openaccess.thecvf.com/content/CVPR2024/html/Lyu_One-dimensional_Adapter_to_Rule_Them_All_Concepts_Diffusion_Models_and_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lyumengyao.github.io/projects/spm) <br /> [![GitHub](https://img.shields.io/github/stars/Con6924/SPM?style=flat)](https://github.com/Con6924/SPM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Lyu_One-dimensional_Adapter_to_Rule_Them_All_Concepts_Diffusion_Models_and_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.16145-b31b1b.svg)](http://arxiv.org/abs/2312.16145) | :heavy_minus_sign: |
| [Orthogonal Adaptation for Modular Customization of Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Po_Orthogonal_Adaptation_for_Modular_Customization_of_Diffusion_Models_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://ryanpo.com/ortha/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Po_Orthogonal_Adaptation_for_Modular_Customization_of_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02432-b31b1b.svg)](http://arxiv.org/abs/2312.02432) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=4lVEFBtYE4A) |
| [PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis](https://openaccess.thecvf.com/content/CVPR2024/html/Lv_PLACE_Adaptive_Layout-Semantic_Fusion_for_Semantic_Image_Synthesis_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cszy98.github.io/PLACE/) <br /> [![GitHub](https://img.shields.io/github/stars/cszy98/PLACE?style=flat)](https://github.com/cszy98/PLACE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Lv_PLACE_Adaptive_Layout-Semantic_Fusion_for_Semantic_Image_Synthesis_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.01852-b31b1b.svg)](http://arxiv.org/abs/2403.01852) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=47mMAmclPWw) |
| [Emu Edit: Precise Image Editing via Recognition and Generation Tasks](https://openaccess.thecvf.com/content/CVPR2024/html/Sheynin_Emu_Edit_Precise_Image_Editing_via_Recognition_and_Generation_Tasks_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://emu-edit.metademolab.com/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Sheynin_Emu_Edit_Precise_Image_Editing_via_Recognition_and_Generation_Tasks_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.10089-b31b1b.svg)](http://arxiv.org/abs/2311.10089) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=UNDR55ehSYM) |
| [Predicated Diffusion: Predicate Logic-based Attention Guidance for Text-to-Image Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Sueyoshi_Predicated_Diffusion_Predicate_Logic-Based_Attention_Guidance_for_Text-to-Image_Diffusion_Models_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Sueyoshi_Predicated_Diffusion_Predicate_Logic-Based_Attention_Guidance_for_Text-to-Image_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.16117-b31b1b.svg)](http://arxiv.org/abs/2311.16117) | :heavy_minus_sign: |
| [RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Kara_RAVE_Randomized_Noise_Shuffling_for_Fast_and_Consistent_Video_Editing_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rave-video.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/RehgLab/RAVE?style=flat)](https://github.com/RehgLab/RAVE) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-demo-FFD21F.svg)](https://huggingface.co/spaces/ozgurkara/RAVE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Kara_RAVE_Randomized_Noise_Shuffling_for_Fast_and_Consistent_Video_Editing_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.04524-b31b1b.svg)](http://arxiv.org/abs/2312.04524) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2hQho5AC9T0) |
| [Readout Guidance: Learning Control from Diffusion Features](https://openaccess.thecvf.com/content/CVPR2024/html/Luo_Readout_Guidance_Learning_Control_from_Diffusion_Features_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://readout-guidance.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/google-research/readout_guidance?style=flat)](https://github.com/google-research/readout_guidance) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Luo_Readout_Guidance_Learning_Control_from_Diffusion_Features_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02150-b31b1b.svg)](http://arxiv.org/abs/2312.02150) | :heavy_minus_sign: |
| [Real-Time 3D-Aware Portrait Video Relighting](https://openaccess.thecvf.com/content/CVPR2024/html/Cai_Real-time_3D-aware_Portrait_Video_Relighting_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](http://geometrylearning.com/VideoRelighting/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Cai_Real-time_3D-aware_Portrait_Video_Relighting_CVPR_2024_paper.pdf) | :heavy_minus_sign: |
| [Residual Learning in Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Residual_Learning_in_Diffusion_Models_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Residual_Learning_in_Diffusion_Models_CVPR_2024_paper.pdf) | :heavy_minus_sign: |
| [Rethinking FID: Towards a Better Evaluation Metric for Image Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Jayasumana_Rethinking_FID_Towards_a_Better_Evaluation_Metric_for_Image_Generation_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/google-research/google-research/tree/master/cmmd) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Jayasumana_Rethinking_FID_Towards_a_Better_Evaluation_Metric_for_Image_Generation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.09603-b31b1b.svg)](http://arxiv.org/abs/2401.09603) | :heavy_minus_sign: |
| [Sat2Scene: 3D Urban Scene Generation from Satellite Images with Diffusion](https://openaccess.thecvf.com/content/CVPR2024/html/Li_Sat2Scene_3D_Urban_Scene_Generation_from_Satellite_Images_with_Diffusion_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Sat2Scene_3D_Urban_Scene_Generation_from_Satellite_Images_with_Diffusion_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.10786-b31b1b.svg)](http://arxiv.org/abs/2401.10786) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=NqFy20zjFHU) |
| [SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing](https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_SCEdit_Efficient_and_Controllable_Image_Diffusion_Generation_via_Skip_Connection_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://scedit.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/ali-vilab/SCEdit?style=flat)](https://github.com/ali-vilab/SCEdit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_SCEdit_Efficient_and_Controllable_Image_Diffusion_Generation_via_Skip_Connection_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.11392-b31b1b.svg)](http://arxiv.org/abs/2312.11392) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=XJrK3-NgB1Q) |
| [SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models](https://openaccess.thecvf.com/content/CVPR2024/html/Huang_SmartEdit_Exploring_Complex_Instruction-based_Image_Editing_with_Multimodal_Large_Language_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yuzhou914.github.io/SmartEdit/) <br /> [![GitHub](https://img.shields.io/github/stars/TencentARC/SmartEdit?style=flat)](https://github.com/TencentARC/SmartEdit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_SmartEdit_Exploring_Complex_Instruction-based_Image_Editing_with_Multimodal_Large_Language_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.06739-b31b1b.svg)](http://arxiv.org/abs/2312.06739) | :heavy_minus_sign: |
| [Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis](https://openaccess.thecvf.com/content/CVPR2024/html/Menapace_Snap_Video_Scaled_Spatiotemporal_Transformers_for_Text-to-Video_Synthesis_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://snap-research.github.io/snapvideo/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Menapace_Snap_Video_Scaled_Spatiotemporal_Transformers_for_Text-to-Video_Synthesis_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2402.14797-b31b1b.svg)](http://arxiv.org/abs/2402.14797) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=aL2zq_IKSBg) |
| [Style Injection in Diffusion: A Training-Free Approach for Adapting Large-Scale Diffusion Models for Style Transfer](https://openaccess.thecvf.com/content/CVPR2024/html/Chung_Style_Injection_in_Diffusion_A_Training-free_Approach_for_Adapting_Large-scale_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jiwoogit.github.io/StyleID_site/) <br /> [![GitHub](https://img.shields.io/github/stars/jiwoogit/StyleID?style=flat)](https://github.com/jiwoogit/StyleID) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Chung_Style_Injection_in_Diffusion_A_Training-free_Approach_for_Adapting_Large-scale_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.09008-b31b1b.svg)](http://arxiv.org/abs/2312.09008) | :heavy_minus_sign: |
| [Tackling the Singularities at the Endpoints of Time Intervals in Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Tackling_the_Singularities_at_the_Endpoints_of_Time_Intervals_in_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://pangzecheung.github.io/SingDiffusion/) <br /> [![GitHub](https://img.shields.io/github/stars/PangzeCheung/SingDiffusion?style=flat)](https://github.com/PangzeCheung/SingDiffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Tackling_the_Singularities_at_the_Endpoints_of_Time_Intervals_in_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.08381-b31b1b.svg)](http://arxiv.org/abs/2403.08381) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3Rt17MnHEJQ) |
| [Taming Stable Diffusion for Text to 360 Panorama Image Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Taming_Stable_Diffusion_for_Text_to_360_Panorama_Image_Generation_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://chengzhag.github.io/publication/panfusion/) <br /> [![GitHub](https://img.shields.io/github/stars/chengzhag/PanFusion?style=flat)](https://github.com/chengzhag/PanFusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Taming_Stable_Diffusion_for_Text_to_360_Panorama_Image_Generation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2404.07949-b31b1b.svg)](http://arxiv.org/abs/2404.07949) | :heavy_minus_sign: |
| [TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Huang_TFMQ-DM_Temporal_Feature_Maintenance_Quantization_for_Diffusion_Models_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://modeltc.github.io/TFMQ-DM/) <br /> [![GitHub](https://img.shields.io/github/stars/ModelTC/TFMQ-DM?style=flat)](https://github.com/ModelTC/TFMQ-DM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_TFMQ-DM_Temporal_Feature_Maintenance_Quantization_for_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.16503-b31b1b.svg)](http://arxiv.org/abs/2311.16503) | :heavy_minus_sign: |
| [Total Selfie: Generating Full-Body Selfies](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Total_Selfie_Generating_Full-Body_Selfies_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://homes.cs.washington.edu/~boweiche/project_page/totalselfie/) <br /> [![GitHub](https://img.shields.io/github/stars/ArmastusChen/total_selfie?style=flat)](https://github.com/ArmastusChen/total_selfie) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Total_Selfie_Generating_Full-Body_Selfies_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14740-b31b1b.svg)](http://arxiv.org/abs/2308.14740) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Aoq6BLbynWM) |
| [UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs](https://openaccess.thecvf.com/content/CVPR2024/html/Xu_UFOGen_You_Forward_Once_Large_Scale_Text-to-Image_Generation_via_Diffusion_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_UFOGen_You_Forward_Once_Large_Scale_Text-to-Image_Generation_via_Diffusion_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.09257-b31b1b.svg)](http://arxiv.org/abs/2311.09257) | :heavy_minus_sign: |
| [VecFusion: Vector Font Generation with Diffusion](https://openaccess.thecvf.com/content/CVPR2024/html/Thamizharasan_VecFusion_Vector_Font_Generation_with_Diffusion_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://vikastmz.github.io/VecFusion/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Thamizharasan_VecFusion_Vector_Font_Generation_with_Diffusion_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.10540-b31b1b.svg)](http://arxiv.org/abs/2312.10540) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=w4-h-t7XEKI) |
| [ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Kwak_ViVid-1-to-3_Novel_View_Synthesis_with_Video_Diffusion_Models_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ubc-vision.github.io/vivid123/) <br /> [![GitHub](https://img.shields.io/github/stars/ubc-vision/vivid123?style=flat)](https://github.com/ubc-vision/vivid123) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Kwak_ViVid-1-to-3_Novel_View_Synthesis_with_Video_Diffusion_Models_CVPR_2024_paper.pdf) | :heavy_minus_sign: |
| [3D Geometry-Aware Deformable Gaussian Splatting for Dynamic View Synthesis](https://openaccess.thecvf.com/content/CVPR2024/html/Lu_3D_Geometry-Aware_Deformable_Gaussian_Splatting_for_Dynamic_View_Synthesis_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://npucvr.github.io/GaGS/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_3D_Geometry-Aware_Deformable_Gaussian_Splatting_for_Dynamic_View_Synthesis_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2404.06270-b31b1b.svg)](http://arxiv.org/abs/2404.06270) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=gU0z0k9Ta0Y) |
| [3D Multi-Frame Fusion for Video Stabilization](https://openaccess.thecvf.com/content/CVPR2024/html/Peng_3D_Multi-frame_Fusion_for_Video_Stabilization_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Peng_3D_Multi-frame_Fusion_for_Video_Stabilization_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2404.12887-b31b1b.svg)](http://arxiv.org/abs/2404.12887) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-dpI1CFcM7A) |
| [4D-fy: Text-to-4D Generation using Hybrid Score Distillation Sampling](https://openaccess.thecvf.com/content/CVPR2024/html/Bahmani_4D-fy_Text-to-4D_Generation_Using_Hybrid_Score_Distillation_Sampling_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sherwinbahmani.github.io/4dfy/) <br /> [![GitHub](https://img.shields.io/github/stars/sherwinbahmani/4dfy?style=flat)](https://github.com/sherwinbahmani/4dfy) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Bahmani_4D-fy_Text-to-4D_Generation_Using_Hybrid_Score_Distillation_Sampling_CVPR_2024_paper.pdf) | :heavy_minus_sign: |
| [360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_360DVD_Controllable_Panorama_Video_Generation_with_360-Degree_Video_Diffusion_Model_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://akaneqwq.github.io/360DVD/) <br /> [![GitHub](https://img.shields.io/github/stars/Akaneqwq/360DVD?style=flat)](https://github.com/Akaneqwq/360DVD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_360DVD_Controllable_Panorama_Video_Generation_with_360-Degree_Video_Diffusion_Model_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.06578-b31b1b.svg)](http://arxiv.org/abs/2401.06578) | :heavy_minus_sign: |
| [RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization](https://openaccess.thecvf.com/content/CVPR2024/html/Huang_RealCustom_Narrowing_Real_Text_Word_for_Real-Time_Open-Domain_Text-to-Image_Customization_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://corleone-huang.github.io/realcustom/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_RealCustom_Narrowing_Real_Text_Word_for_Real-Time_Open-Domain_Text-to-Image_Customization_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.00483-b31b1b.svg)](http://arxiv.org/abs/2403.00483) | :heavy_minus_sign: |
| [Æµ<sup>*</sup>: Zero-Shot Style Transfer via Attention Reweighting](https://openaccess.thecvf.com/content/CVPR2024/html/Deng_Z_Zero-shot_Style_Transfer_via_Attention_Reweighting_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/HolmesShuan/Zero-shot-Style-Transfer-via-Attention-Rearrangement?style=flat)](https://github.com/HolmesShuan/Zero-shot-Style-Transfer-via-Attention-Rearrangement) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Deng_Z_Zero-shot_Style_Transfer_via_Attention_Reweighting_CVPR_2024_paper.pdf) | :heavy_minus_sign: |
| [A Recipe for Scaling up Text-to-Video Generation with Text-Free Videos](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_A_Recipe_for_Scaling_up_Text-to-Video_Generation_with_Text-free_Videos_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tf-t2v.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/ali-vilab/VGen?style=flat)](https://github.com/ali-vilab/VGen) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_A_Recipe_for_Scaling_up_Text-to-Video_Generation_with_Text-free_Videos_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.15770-b31b1b.svg)](http://arxiv.org/abs/2312.15770) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Cx34ky0PxOU) |
| [A Unified Approach for Text- and Image-guided 4D Scene Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_A_Unified_Approach_for_Text-_and_Image-guided_4D_Scene_Generation_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://research.nvidia.com/labs/nxp/dream-in-4d/) <br /> [![GitHub](https://img.shields.io/github/stars/NVlabs/dream-in-4d?style=flat)](https://github.com/NVlabs/dream-in-4d) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_A_Unified_Approach_for_Text-_and_Image-guided_4D_Scene_Generation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.16854-b31b1b.svg)](http://arxiv.org/abs/2311.16854) | :heavy_minus_sign: |
| [A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing](https://openaccess.thecvf.com/content/CVPR2024/html/Li_A_Video_is_Worth_256_Bases_Spatial-Temporal_Expectation-Maximization_Inversion_for_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://stem-inv.github.io/page/) <br /> [![GitHub](https://img.shields.io/github/stars/STEM-Inv/stem-inv?style=flat)](https://github.com/STEM-Inv/stem-inv) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_A_Video_is_Worth_256_Bases_Spatial-Temporal_Expectation-Maximization_Inversion_for_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.05856-b31b1b.svg)](http://arxiv.org/abs/2312.05856) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FZnDn4wBPqY) |
| [Accelerating Diffusion Sampling with Optimized Time Steps](https://openaccess.thecvf.com/content/CVPR2024/html/Xue_Accelerating_Diffusion_Sampling_with_Optimized_Time_Steps_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/scxue/DM-NonUniform?style=flat)](https://github.com/scxue/DM-NonUniform) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Xue_Accelerating_Diffusion_Sampling_with_Optimized_Time_Steps_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2402.17376-b31b1b.svg)](http://arxiv.org/abs/2402.17376) | :heavy_minus_sign: |
| [ACT-Diffusion: Efficient Adversarial Consistency Training for One-Step Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Kong_ACT-Diffusion_Efficient_Adversarial_Consistency_Training_for_One-step_Diffusion_Models_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/kong13661/ACT?style=flat)](https://github.com/kong13661/ACT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Kong_ACT-Diffusion_Efficient_Adversarial_Consistency_Training_for_One-step_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.14097-b31b1b.svg)](http://arxiv.org/abs/2311.14097) | :heavy_minus_sign: |
| [Adversarial Score Distillation: When Score Distillation Meets GAN](https://openaccess.thecvf.com/content/CVPR2024/html/Wei_Adversarial_Score_Distillation_When_score_distillation_meets_GAN_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://2y7c3.github.io/ASD/asd.html) <br /> [![GitHub](https://img.shields.io/github/stars/2y7c3/ASD?style=flat)](https://github.com/2y7c3/ASD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wei_Adversarial_Score_Distillation_When_score_distillation_meets_GAN_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.00739-b31b1b.svg)](http://arxiv.org/abs/2312.00739) | :heavy_minus_sign: |
| [Adversarial Text to Continuous Image Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Haydarov_Adversarial_Text_to_Continuous_Image_Generation_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://kilichbek.github.io/webpage/hypercgan/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Haydarov_Adversarial_Text_to_Continuous_Image_Generation_CVPR_2024_paper.pdf) | :heavy_minus_sign: |
| [AEROBLADE: Training-Free Detection of Latent Diffusion Images using Autoencoder Reconstruction Error](https://openaccess.thecvf.com/content/CVPR2024/html/Ricker_AEROBLADE_Training-Free_Detection_of_Latent_Diffusion_Images_Using_Autoencoder_Reconstruction_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/jonasricker/aeroblade?style=flat)](https://github.com/jonasricker/aeroblade) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Ricker_AEROBLADE_Training-Free_Detection_of_Latent_Diffusion_Images_Using_Autoencoder_Reconstruction_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.17879-b31b1b.svg)](http://arxiv.org/abs/2401.17879) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=P9OvpqQN_Js) |
| [Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation](https://openaccess.thecvf.com/content/CVPR2024/html/Hu_Animate_Anyone_Consistent_and_Controllable_Image-to-Video_Synthesis_for_Character_Animation_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://humanaigc.github.io/animate-anyone/) <br /> [![GitHub](https://img.shields.io/github/stars/HumanAIGC/AnimateAnyone?style=flat)](https://github.com/HumanAIGC/AnimateAnyone) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_Animate_Anyone_Consistent_and_Controllable_Image-to-Video_Synthesis_for_Character_Animation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.17117-b31b1b.svg)](http://arxiv.org/abs/2311.17117) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8PCn5hLKNu4) |
| [Animating General Image with Large Visual Motion Model](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Animating_General_Image_with_Large_Visual_Motion_Model_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://densechen.github.io/LVMM/) <br /> [![GitHub](https://img.shields.io/github/stars/densechen/LVMM?style=flat)](https://github.com/densechen/LVMM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Animating_General_Image_with_Large_Visual_Motion_Model_CVPR_2024_paper.pdf) | :heavy_minus_sign: |
| [Anomaly Score: Evaluating Generative Models and Individual Generated Images based on Complexity and Vulnerability](https://openaccess.thecvf.com/content/CVPR2024/html/Hwang_Anomaly_Score_Evaluating_Generative_Models_and_Individual_Generated_Images_based_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Hwang_Anomaly_Score_Evaluating_Generative_Models_and_Individual_Generated_Images_based_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.10634-b31b1b.svg)](http://arxiv.org/abs/2312.10634) | :heavy_minus_sign: |
| [AnyDoor: Zero-Shot Object-Level Image Customization](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_AnyDoor_Zero-shot_Object-level_Image_Customization_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ali-vilab.github.io/AnyDoor-Page/) <br /> [![GitHub](https://img.shields.io/github/stars/ali-vilab/AnyDoor?style=flat)](https://github.com/ali-vilab/AnyDoor) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-demo-FFD21F.svg)](https://huggingface.co/spaces/xichenhku/AnyDoor-online) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_AnyDoor_Zero-shot_Object-level_Image_Customization_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09481-b31b1b.svg)](http://arxiv.org/abs/2307.09481) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=D4Goc3-BmGI) |
| [AnyScene: Customized Image Synthesis with Composited Foreground](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_AnyScene_Customized_Image_Synthesis_with_Composited_Foreground_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_AnyScene_Customized_Image_Synthesis_with_Composited_Foreground_CVPR_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_EYrW0w7r7U) |
| [Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural Decoder](https://openaccess.thecvf.com/content/CVPR2024/html/Kim_Arbitrary-Scale_Image_Generation_and_Upsampling_using_Latent_Diffusion_Model_and_CVPR_2024_paper.html) |  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Arbitrary-Scale_Image_Generation_and_Upsampling_using_Latent_Diffusion_Model_and_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.10255-b31b1b.svg)](http://arxiv.org/abs/2403.10255) | :heavy_minus_sign: |
| [ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder and Explicit Adaptation](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_ArtAdapter_Text-to-Image_Style_Transfer_using_Multi-Level_Style_Encoder_and_Explicit_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cardinalblue.github.io/artadapter.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/cardinalblue/ArtAdapter?style=flat)](https://github.com/cardinalblue/ArtAdapter) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_ArtAdapter_Text-to-Image_Style_Transfer_using_Multi-Level_Style_Encoder_and_Explicit_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02109-b31b1b.svg)](http://arxiv.org/abs/2312.02109) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6e6TeV7wlRU) |
| [AVID: Any-Length Video Inpainting with Diffusion Model](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_AVID_Any-Length_Video_Inpainting_with_Diffusion_Model_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zhang-zx.github.io/AVID/) <br /> [![GitHub](https://img.shields.io/github/stars/zhang-zx/AVID?style=flat)](https://github.com/zhang-zx/AVID) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_AVID_Any-Length_Video_Inpainting_with_Diffusion_Model_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.03816-b31b1b.svg)](http://arxiv.org/abs/2312.03816) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=p8-xTfOe9Tw) |
| [Balancing Act: Distribution-Guided Debiasing in Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Parihar_Balancing_Act_Distribution-Guided_Debiasing_in_Diffusion_Models_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ab-34.github.io/balancing_act/) <br /> [![GitHub](https://img.shields.io/github/stars/rishubhpar/debiasing_gen_models?style=flat)](https://github.com/rishubhpar/debiasing_gen_models) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Parihar_Balancing_Act_Distribution-Guided_Debiasing_in_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2402.18206-b31b1b.svg)](http://arxiv.org/abs/2402.18206) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ppkJyw7t_UE) |
| [BerfScene: Bev-Conditioned Equivariant Radiance Fields for Infinite 3D Scene Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_BerfScene_Bev-conditioned_Equivariant_Radiance_Fields_for_Infinite_3D_Scene_Generation_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zqh0253.github.io/BerfScene/) <br /> [![GitHub](https://img.shields.io/github/stars/zqh0253/BerfScene?style=flat)](https://github.com/zqh0253/BerfScene) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-demo-FFD21F.svg)](https://huggingface.co/spaces/qihang/BerfScene) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_BerfScene_Bev-conditioned_Equivariant_Radiance_Fields_for_Infinite_3D_Scene_Generation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02136-b31b1b.svg)](http://arxiv.org/abs/2312.02136) | :heavy_minus_sign: |
| [Beyond First-Order Tweedie: Solving Inverse Problems using Latent Diffusion](https://openaccess.thecvf.com/content/CVPR2024/html/Rout_Beyond_First-Order_Tweedie_Solving_Inverse_Problems_using_Latent_Diffusion_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Rout_Beyond_First-Order_Tweedie_Solving_Inverse_Problems_using_Latent_Diffusion_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.00852-b31b1b.svg)](http://arxiv.org/abs/2312.00852) | :heavy_minus_sign: |
| [Beyond Textual Constraints: Learning Novel Diffusion Conditions with Fewer Examples](https://openaccess.thecvf.com/content/CVPR2024/html/Yu_Beyond_Textual_Constraints_Learning_Novel_Diffusion_Conditions_with_Fewer_Examples_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/Yuyan9Yu/BeyondTextConstraint?style=flat)](https://github.com/Yuyan9Yu/BeyondTextConstraint) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_Beyond_Textual_Constraints_Learning_Novel_Diffusion_Conditions_with_Fewer_Examples_CVPR_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=BkJejVktIWs) |
| [BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Shi_BIVDiff_A_Training-Free_Framework_for_General-Purpose_Video_Synthesis_via_Bridging_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://bivdiff.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/MCG-NJU/BIVDiff?style=flat)](https://github.com/MCG-NJU/BIVDiff) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Shi_BIVDiff_A_Training-Free_Framework_for_General-Purpose_Video_Synthesis_via_Bridging_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02813-b31b1b.svg)](http://arxiv.org/abs/2312.02813) | :heavy_minus_sign: |
| [Boosting Diffusion Models with Moving Average Sampling in Frequency Domain](https://openaccess.thecvf.com/content/CVPR2024/html/Qian_Boosting_Diffusion_Models_with_Moving_Average_Sampling_in_Frequency_Domain_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Qian_Boosting_Diffusion_Models_with_Moving_Average_Sampling_in_Frequency_Domain_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.17870-b31b1b.svg)](http://arxiv.org/abs/2403.17870) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=25wh9-MGtYw) |
| [C3: High-Performance and Low-Complexity Neural Compression from a Single Image or Video](https://openaccess.thecvf.com/content/CVPR2024/html/Kim_C3_High-Performance_and_Low-Complexity_Neural_Compression_from_a_Single_Image_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://c3-neural-compression.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/google-deepmind/c3_neural_compression?style=flat)](https://github.com/google-deepmind/c3_neural_compression) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_C3_High-Performance_and_Low-Complexity_Neural_Compression_from_a_Single_Image_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02753-b31b1b.svg)](http://arxiv.org/abs/2312.02753) | :heavy_minus_sign: |
| [Cache Me if You Can: Accelerating Diffusion Models through Block Caching](https://openaccess.thecvf.com/content/CVPR2024/html/Wimbauer_Cache_Me_if_You_Can_Accelerating_Diffusion_Models_through_Block_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://fwmb.github.io/blockcaching/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wimbauer_Cache_Me_if_You_Can_Accelerating_Diffusion_Models_through_Block_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.03209-b31b1b.svg)](http://arxiv.org/abs/2312.03209) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pQjJmDltIDk) |
| [CAMEL: CAusal Motion Enhancement Tailored for Lifting Text-Driven Video Editing](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_CAMEL_CAusal_Motion_Enhancement_Tailored_for_Lifting_Text-driven_Video_Editing_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/zhangguiwei610/CAMEL?style=flat)](https://github.com/zhangguiwei610/CAMEL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_CAMEL_CAusal_Motion_Enhancement_Tailored_for_Lifting_Text-driven_Video_Editing_CVPR_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=c_TZEz4tj2A) |
| [CapHuman: Capture Your Moments in Parallel Universes](https://openaccess.thecvf.com/content/CVPR2024/html/Liang_CapHuman_Capture_Your_Moments_in_Parallel_Universes_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://caphuman.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/VamosC/CapHuman?style=flat)](https://github.com/VamosC/CapHuman) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_CapHuman_Capture_Your_Moments_in_Parallel_Universes_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2402.00627-b31b1b.svg)](http://arxiv.org/abs/2402.00627) | :heavy_minus_sign: |
| [Carve3D: Improving Multi-View Reconstruction Consistency for Diffusion Models with RL Finetuning](https://openaccess.thecvf.com/content/CVPR2024/html/Xie_Carve3D_Improving_Multi-view_Reconstruction_Consistency_for_Diffusion_Models_with_RL_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://desaixie.github.io/carve-3d/) <br /> [![GitHub](https://img.shields.io/github/stars/desaixie/carve3d?style=flat)](https://github.com/desaixie/carve3d) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Xie_Carve3D_Improving_Multi-view_Reconstruction_Consistency_for_Diffusion_Models_with_RL_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.13980-b31b1b.svg)](http://arxiv.org/abs/2312.13980) | :heavy_minus_sign: |
| [CAT-DM: Controllable Accelerated Virtual Try-On with Diffusion Model](https://openaccess.thecvf.com/content/CVPR2024/html/Zeng_CAT-DM_Controllable_Accelerated_Virtual_Try-on_with_Diffusion_Model_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zengjianhao.github.io/CAT-DM/) <br /> [![GitHub](https://img.shields.io/github/stars/zengjianhao/CAT-DM?style=flat)](https://github.com/zengjianhao/CAT-DM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zeng_CAT-DM_Controllable_Accelerated_Virtual_Try-on_with_Diffusion_Model_CVPR_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZIkow2fmMNk) |
| [CCEdit: Creative and Controllable Video Editing via Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Feng_CCEdit_Creative_and_Controllable_Video_Editing_via_Diffusion_Models_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ruoyufeng.github.io/CCEdit.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/RuoyuFeng/CCEdit?style=flat)](https://github.com/RuoyuFeng/CCEdit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Feng_CCEdit_Creative_and_Controllable_Video_Editing_via_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.16496-b31b1b.svg)](http://arxiv.org/abs/2309.16496) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=UQw4jq-igN4) |
| [CDFormer: When Degradation Prediction Embraces Diffusion Model for Blind Image Super-Resolution](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_CDFormer_When_Degradation_Prediction_Embraces_Diffusion_Model_for_Blind_Image_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/I2-Multimedia-Lab/CDFormer?style=flat)](https://github.com/I2-Multimedia-Lab/CDFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_CDFormer_When_Degradation_Prediction_Embraces_Diffusion_Model_for_Blind_Image_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2405.07648-b31b1b.svg)](http://arxiv.org/abs/2405.07648) | :heavy_minus_sign: |
| [CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization](https://openaccess.thecvf.com/content/CVPR2024/html/Ni_CHAIN_Enhancing_Generalization_in_Data-Efficient_GANs_via_lipsCHitz_continuity_constrAIned_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/MaxwellYaoNi/CHAIN?style=flat)](https://github.com/MaxwellYaoNi/CHAIN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Ni_CHAIN_Enhancing_Generalization_in_Data-Efficient_GANs_via_lipsCHitz_continuity_constrAIned_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2404.00521-b31b1b.svg)](http://arxiv.org/abs/2404.00521) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_f7X5_zT_lA) |
| [Check Locate Rectify: A Training-Free Layout Calibration System for Text-to-Image Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Gong_Check_Locate_Rectify_A_Training-Free_Layout_Calibration_System_for_Text-to-Image_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://simm-t2i.github.io/SimM/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Gong_Check_Locate_Rectify_A_Training-Free_Layout_Calibration_System_for_Text-to-Image_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.15773-b31b1b.svg)](http://arxiv.org/abs/2311.15773) | :heavy_minus_sign: |
| [Cinematic Behavior Transfer via NeRF-based Differentiable Filming](https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_Cinematic_Behavior_Transfer_via_NeRF-based_Differentiable_Filming_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://virtualfilmstudio.github.io/projects/cinetransfer/) <br /> [![GitHub](https://img.shields.io/github/stars/VirtualFilmStudio/Cinetransfer?style=flat)](https://github.com/VirtualFilmStudio/Cinetransfer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_Cinematic_Behavior_Transfer_via_NeRF-based_Differentiable_Filming_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.17754-b31b1b.svg)](http://arxiv.org/abs/2311.17754) | :heavy_minus_sign: |
| [Codebook Transfer with Part-of-Speech for Vector-Quantized Image Modeling](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Codebook_Transfer_with_Part-of-Speech_for_Vector-Quantized_Image_Modeling_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Codebook_Transfer_with_Part-of-Speech_for_Vector-Quantized_Image_Modeling_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.10071-b31b1b.svg)](http://arxiv.org/abs/2403.10071) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=N6M0jcMP9lo) |
| [CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Mei_CoDi_Conditional_Diffusion_Distillation_for_Higher-Fidelity_and_Faster_Image_Generation_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://fast-codi.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/fast-codi/CoDi?style=flat)](https://github.com/fast-codi/CoDi) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Mei_CoDi_Conditional_Diffusion_Distillation_for_Higher-Fidelity_and_Faster_Image_Generation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.01407-b31b1b.svg)](http://arxiv.org/abs/2310.01407) | :heavy_minus_sign: |
| [Combining Frame and GOP Embeddings for Neural Video Representation](https://openaccess.thecvf.com/content/CVPR2024/html/Saethre_Combining_Frame_and_GOP_Embeddings_for_Neural_Video_Representation_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Saethre_Combining_Frame_and_GOP_Embeddings_for_Neural_Video_Representation_CVPR_2024_paper.pdf) | :heavy_minus_sign: |
| [CommonCanvas: Open Diffusion Models Trained on Creative-Commons Images](https://openaccess.thecvf.com/content/CVPR2024/html/Gokaslan_CommonCanvas_Open_Diffusion_Models_Trained_on_Creative-Commons_Images_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/mosaicml/diffusion/blob/main/assets/common-canvas.md) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-demo-FFD21F.svg)](https://huggingface.co/spaces/common-canvas/CommonCanvas) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Gokaslan_CommonCanvas_Open_Diffusion_Models_Trained_on_Creative-Commons_Images_CVPR_2024_paper.pdf) | :heavy_minus_sign: |
| [Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models](https://openaccess.thecvf.com/content/CVPR2024/html/Kwon_Concept_Weaver_Enabling_Multi-Concept_Fusion_in_Text-to-Image_Models_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Kwon_Concept_Weaver_Enabling_Multi-Concept_Fusion_in_Text-to-Image_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2404.03913-b31b1b.svg)](http://arxiv.org/abs/2404.03913) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zfrZVyiPp2o) |
| [Condition-Aware Neural Network for Controlled Image Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Cai_Condition-Aware_Neural_Network_for_Controlled_Image_Generation_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/mit-han-lab/efficientvit?style=flat)](https://github.com/mit-han-lab/efficientvit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Cai_Condition-Aware_Neural_Network_for_Controlled_Image_Generation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2404.01143-b31b1b.svg)](http://arxiv.org/abs/2404.01143) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=uYQR06yG_S4) |
| [CONFORM: Contrast is All You Need for High-Fidelity Text-to-Image Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Meral_CONFORM_Contrast_is_All_You_Need_for_High-Fidelity_Text-to-Image_Diffusion_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://conform-diffusion.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/gemlab-vt/CONFORM?style=flat)](https://github.com/gemlab-vt/CONFORM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Meral_CONFORM_Contrast_is_All_You_Need_for_High-Fidelity_Text-to-Image_Diffusion_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.06059-b31b1b.svg)](http://arxiv.org/abs/2312.06059) | :heavy_minus_sign: |
| [ConsistNet: Enforcing 3D Consistency for Multi-View Images Diffusion](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_ConsistNet_Enforcing_3D_Consistency_for_Multi-view_Images_Diffusion_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jiayuyang.github.io/Consist_Net/) <br /> [![GitHub](https://img.shields.io/github/stars/JiayuYANG/ConsistNet?style=flat)](https://github.com/JiayuYANG/ConsistNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_ConsistNet_Enforcing_3D_Consistency_for_Multi-view_Images_Diffusion_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.10343-b31b1b.svg)](http://arxiv.org/abs/2310.10343) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=yXrtv3yzeks) |
| [Content-Style Decoupling for Unsupervised Makeup Transfer without Generating Pseudo Ground Truth](https://openaccess.thecvf.com/content/CVPR2024/html/Sun_Content-Style_Decoupling_for_Unsupervised_Makeup_Transfer_without_Generating_Pseudo_Ground_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/Snowfallingplum/CSD-MT?style=flat)](https://github.com/Snowfallingplum/CSD-MT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_Content-Style_Decoupling_for_Unsupervised_Makeup_Transfer_without_Generating_Pseudo_Ground_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2405.17240-b31b1b.svg)](http://arxiv.org/abs/2405.17240) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_RKWygsMy5g) |
| [Contrastive Denoising Score for Text-Guided Latent Diffusion Image Editing](https://openaccess.thecvf.com/content/CVPR2024/html/Nam_Contrastive_Denoising_Score_for_Text-guided_Latent_Diffusion_Image_Editing_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hyelinnam.github.io/CDS/) <br /> [![GitHub](https://img.shields.io/github/stars/HyelinNAM/ContrastiveDenoisingScore?style=flat)](https://github.com/HyelinNAM/ContrastiveDenoisingScore) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Nam_Contrastive_Denoising_Score_for_Text-guided_Latent_Diffusion_Image_Editing_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.18608-b31b1b.svg)](http://arxiv.org/abs/2311.18608) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jLaIOSE9nkw) |
| [ControlRoom3D: Room Generation using Semantic Proxy Rooms](https://openaccess.thecvf.com/content/CVPR2024/html/Schult_ControlRoom3D_Room_Generation_using_Semantic_Proxy_Rooms_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jonasschult.github.io/ControlRoom3D/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Schult_ControlRoom3D_Room_Generation_using_Semantic_Proxy_Rooms_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.05208-b31b1b.svg)](http://arxiv.org/abs/2312.05208) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=g1h9f2cjgd8) |
| [Cross Initialization for Face Personalization of Text-to-Image Models](https://openaccess.thecvf.com/content/CVPR2024/html/Pang_Cross_Initialization_for_Face_Personalization_of_Text-to-Image_Models_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/lyuPang/CrossInitialization?style=flat)](https://github.com/lyuPang/CrossInitialization) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Pang_Cross_Initialization_for_Face_Personalization_of_Text-to-Image_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.15905-b31b1b.svg)](http://arxiv.org/abs/2312.15905) | :heavy_minus_sign: |
| [Customization Assistant for Text-to-Image Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Customization_Assistant_for_Text-to-Image_Generation_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Customization_Assistant_for_Text-to-Image_Generation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.03045-b31b1b.svg)](http://arxiv.org/abs/2312.03045) | :heavy_minus_sign: |
| [Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training](https://openaccess.thecvf.com/content/CVPR2024/html/He_Customize_your_NeRF_Adaptive_Source_Driven_3D_Scene_Editing_via_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://customnerf.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/hrz2000/CustomNeRF?style=flat)](https://github.com/hrz2000/CustomNeRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/He_Customize_your_NeRF_Adaptive_Source_Driven_3D_Scene_Editing_via_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.01663-b31b1b.svg)](http://arxiv.org/abs/2312.01663) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=B8dW4Jpwerg) |
| [DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_DanceCamera3D_3D_Camera_Movement_Synthesis_with_Music_and_Dance_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://carmenw1203.github.io/DanceCamera3D.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/Carmenw1203/DanceCamera3D-Official?style=flat)](https://github.com/Carmenw1203/DanceCamera3D-Official) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_DanceCamera3D_3D_Camera_Movement_Synthesis_with_Music_and_Dance_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.13667-b31b1b.svg)](http://arxiv.org/abs/2403.13667) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=GSUyx1yWjaU) |
| [Dancing with Still Images: Video Distillation via Static-Dynamic Disentanglement](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Dancing_with_Still_Images_Video_Distillation_via_Static-Dynamic_Disentanglement_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/yuz1wan/video_distillation?style=flat)](https://github.com/yuz1wan/video_distillation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Dancing_with_Still_Images_Video_Distillation_via_Static-Dynamic_Disentanglement_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.00362-b31b1b.svg)](http://arxiv.org/abs/2312.00362) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OMC9WmM_SwE) |
| [Deformable One-Shot Face Stylization via DINO Semantic Guidance](https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Deformable_One-shot_Face_Stylization_via_DINO_Semantic_Guidance_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://vcc.tech/research/2024/DoesFS) <br /> [![GitHub](https://img.shields.io/github/stars/zichongc/DoesFS?style=flat)](https://github.com/zichongc/DoesFS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Deformable_One-shot_Face_Stylization_via_DINO_Semantic_Guidance_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.00459-b31b1b.svg)](http://arxiv.org/abs/2403.00459) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mzWbVlibYBs) |
| [DemoCaricature: Democratising Caricature Generation with a Rough Sketch](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_DemoCaricature_Democratising_Caricature_Generation_with_a_Rough_Sketch_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://democaricature.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/ChenDarYen/DemoCaricature?style=flat)](https://github.com/ChenDarYen/DemoCaricature) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_DemoCaricature_Democratising_Caricature_Generation_with_a_Rough_Sketch_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.04364-b31b1b.svg)](http://arxiv.org/abs/2312.04364) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=X5xE7FSOXA4) |
| [DemoFusion: Democratising High-Resolution Image Generation with No $$$](https://openaccess.thecvf.com/content/CVPR2024/html/Du_DemoFusion_Democratising_High-Resolution_Image_Generation_With_No__CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ruoyidu.github.io/demofusion/demofusion.html) <br /> [![GitHub](https://img.shields.io/github/stars/PRIS-CV/DemoFusion?style=flat)](https://github.com/PRIS-CV/DemoFusion) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-demo-FFD21F.svg)](https://huggingface.co/spaces/radames/Enhance-This-DemoFusion-SDXL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_DemoFusion_Democratising_High-Resolution_Image_Generation_With_No__CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.16973-b31b1b.svg)](http://arxiv.org/abs/2311.16973) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VM4xXJRcxwU) |
| [DetDiffusion: Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_DetDiffusion_Synergizing_Generative_and_Perceptive_Models_for_Enhanced_Data_Generation_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_DetDiffusion_Synergizing_Generative_and_Perceptive_Models_for_Enhanced_Data_Generation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.13304-b31b1b.svg)](http://arxiv.org/abs/2403.13304) | :heavy_minus_sign: |
| [DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model](https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_DiffAgent_Fast_and_Accurate_Text-to-Image_API_Selection_with_Large_Language_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/OpenGVLab/DiffAgent?style=flat)](https://github.com/OpenGVLab/DiffAgent) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_DiffAgent_Fast_and_Accurate_Text-to-Image_API_Selection_with_Large_Language_CVPR_2024_paper.pdf) | :heavy_minus_sign: |
| [DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image Editing](https://openaccess.thecvf.com/content/CVPR2024/html/Mou_DiffEditor_Boosting_Accuracy_and_Flexibility_on_Diffusion-based_Image_Editing_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mc-e.github.io/project/DragonDiffusion/) <br /> [![GitHub](https://img.shields.io/github/stars/MC-E/DragonDiffusion?style=flat)](https://github.com/MC-E/DragonDiffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Mou_DiffEditor_Boosting_Accuracy_and_Flexibility_on_Diffusion-based_Image_Editing_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2402.02583-b31b1b.svg)](http://arxiv.org/abs/2402.02583) | :heavy_minus_sign: |
| [DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_DiffMorpher_Unleashing_the_Capability_of_Diffusion_Models_for_Image_Morphing_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://kevin-thu.github.io/DiffMorpher_page/) <br /> [![GitHub](https://img.shields.io/github/stars/Kevin-thu/DiffMorpher?style=flat)](https://github.com/Kevin-thu/DiffMorpher) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-demo-FFD21F.svg)](https://huggingface.co/spaces/Kevin-thu/DiffMorpher) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_DiffMorpher_Unleashing_the_Capability_of_Diffusion_Models_for_Image_Morphing_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.07409-b31b1b.svg)](http://arxiv.org/abs/2312.07409) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hv_JgrBIK68) |
| [DiffPerformer: Iterative Learning of Consistent Latent Guidance for Diffusion-based Human Video Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_DiffPerformer_Iterative_Learning_of_Consistent_Latent_Guidance_for_Diffusion-based_Human_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_DiffPerformer_Iterative_Learning_of_Consistent_Latent_Guidance_for_Diffusion-based_Human_CVPR_2024_paper.pdf) | :heavy_minus_sign: |
| [DiffSHEG: A Diffusion-based Approach for Real-Time Speech-Driven Holistic 3D Expression and Gesture Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_DiffSHEG_A_Diffusion-Based_Approach_for_Real-Time_Speech-driven_Holistic_3D_Expression_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jeremycjm.github.io/proj/DiffSHEG/) <br /> [![GitHub](https://img.shields.io/github/stars/JeremyCJM/DiffSHEG?style=flat)](https://github.com/JeremyCJM/DiffSHEG) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_DiffSHEG_A_Diffusion-Based_Approach_for_Real-Time_Speech-driven_Holistic_3D_Expression_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.04747-b31b1b.svg)](http://arxiv.org/abs/2401.04747) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=HFaSd5do-zI) |
| [Diffusion Model Alignment using Direct Preference Optimization](https://openaccess.thecvf.com/content/CVPR2024/html/Wallace_Diffusion_Model_Alignment_Using_Direct_Preference_Optimization_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/SalesforceAIResearch/DiffusionDPO?style=flat)](https://github.com/SalesforceAIResearch/DiffusionDPO) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wallace_Diffusion_Model_Alignment_Using_Direct_Preference_Optimization_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.12908-b31b1b.svg)](http://arxiv.org/abs/2311.12908) | :heavy_minus_sign: |
| [Diffusion Models without Attention](https://openaccess.thecvf.com/content/CVPR2024/html/Yan_Diffusion_Models_Without_Attention_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Diffusion_Models_Without_Attention_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.18257-b31b1b.svg)](http://arxiv.org/abs/2311.18257) | :heavy_minus_sign: |
| [Direct2.5: Diverse Text-to-3D Generation via Multi-View 2.5D Diffusion](https://openaccess.thecvf.com/content/CVPR2024/html/Lu_Direct2.5_Diverse_Text-to-3D_Generation_via_Multi-view_2.5D_Diffusion_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://nju-3dv.github.io/projects/direct25/) <br /> [![GitHub](https://img.shields.io/github/stars/apple/ml-direct2.5?style=flat)](https://github.com/apple/ml-direct2.5) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_Direct2.5_Diverse_Text-to-3D_Generation_via_Multi-view_2.5D_Diffusion_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.15980-b31b1b.svg)](http://arxiv.org/abs/2311.15980) | :heavy_minus_sign: |
| [DIRECT-3D: Learning Direct Text-to-3D Generation on Massive Noisy 3D Data](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_DIRECT-3D_Learning_Direct_Text-to-3D_Generation_on_Massive_Noisy_3D_Data_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://direct-3d.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/qihao067/direct3d?style=flat)](https://github.com/qihao067/direct3d) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_DIRECT-3D_Learning_Direct_Text-to-3D_Generation_on_Massive_Noisy_3D_Data_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2406.04322-b31b1b.svg)](http://arxiv.org/abs/2406.04322) | :heavy_minus_sign: |
| [DisCo: Disentangled Control for Realistic Human Dance Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_DisCo_Disentangled_Control_for_Realistic_Human_Dance_Generation_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://disco-dance.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/Wangt-CN/DisCo?style=flat)](https://github.com/Wangt-CN/DisCo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_DisCo_Disentangled_Control_for_Realistic_Human_Dance_Generation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.00040-b31b1b.svg)](http://arxiv.org/abs/2307.00040) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=D_mPPjUCDjE) |
| [Discriminative Probing and Tuning for Text-to-Image Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Qu_Discriminative_Probing_and_Tuning_for_Text-to-Image_Generation_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dpt-t2i.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/LgQu/DPT-T2I?style=flat)](https://github.com/LgQu/DPT-T2I) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Qu_Discriminative_Probing_and_Tuning_for_Text-to-Image_Generation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.04321-b31b1b.svg)](http://arxiv.org/abs/2403.04321) | :heavy_minus_sign: |
| [Distilling ODE Solvers of Diffusion Models into Smaller Steps](https://openaccess.thecvf.com/content/CVPR2024/html/Kim_Distilling_ODE_Solvers_of_Diffusion_Models_into_Smaller_Steps_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/kim-sanghwan/D-ODE-Solvers?style=flat)](https://github.com/kim-sanghwan/D-ODE-Solvers) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Distilling_ODE_Solvers_of_Diffusion_Models_into_Smaller_Steps_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.16421-b31b1b.svg)](http://arxiv.org/abs/2309.16421) | :heavy_minus_sign: |
| [Diversity-Aware Channel Pruning for StyleGAN Compression](https://openaccess.thecvf.com/content/CVPR2024/html/Chung_Diversity-aware_Channel_Pruning_for_StyleGAN_Compression_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jiwoogit.github.io/DCP-GAN_site/) <br /> [![GitHub](https://img.shields.io/github/stars/jiwoogit/DCP-GAN?style=flat)](https://github.com/jiwoogit/DCP-GAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Chung_Diversity-aware_Channel_Pruning_for_StyleGAN_Compression_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.13548-b31b1b.svg)](http://arxiv.org/abs/2403.13548) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FM_0xaO0mEI) |
| [Don't Look into the Dark: Latent Codes for Pluralistic Image Inpainting](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Dont_Look_into_the_Dark_Latent_Codes_for_Pluralistic_Image_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/nintendops/latent-code-inpainting?style=flat)](https://github.com/nintendops/latent-code-inpainting) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Dont_Look_into_the_Dark_Latent_Codes_for_Pluralistic_Image_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.18186-b31b1b.svg)](http://arxiv.org/abs/2403.18186) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=y6O-u6u0oPc) |
| [Doubly Abductive Counterfactual Inference for Text-based Image Editing](https://openaccess.thecvf.com/content/CVPR2024/html/Song_Doubly_Abductive_Counterfactual_Inference_for_Text-based_Image_Editing_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/xuesong39/DAC?style=flat)](https://github.com/xuesong39/DAC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Song_Doubly_Abductive_Counterfactual_Inference_for_Text-based_Image_Editing_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.02981-b31b1b.svg)](http://arxiv.org/abs/2403.02981) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=yFdUchtMZnI) |
| [Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic Propagation](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Drag_Your_Noise_Interactive_Point-based_Editing_via_Diffusion_Semantic_Propagation_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/haofengl/DragNoise?style=flat)](https://github.com/haofengl/DragNoise) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Drag_Your_Noise_Interactive_Point-based_Editing_via_Diffusion_Semantic_Propagation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2404.01050-b31b1b.svg)](http://arxiv.org/abs/2404.01050) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=gKq0s_CvCAg) |
| [DREAM: Diffusion Rectification and Estimation-Adaptive Models](https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_DREAM_Diffusion_Rectification_and_Estimation-Adaptive_Models_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.tianyuding.com/projects/DREAM/) <br /> [![GitHub](https://img.shields.io/github/stars/jinxinzhou/DREAM?style=flat)](https://github.com/jinxinzhou/DREAM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_DREAM_Diffusion_Rectification_and_Estimation-Adaptive_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.00210-b31b1b.svg)](http://arxiv.org/abs/2312.00210) | :heavy_minus_sign: |
| [DreamComposer: Controllable 3D Object Generation via Multi-View Conditions](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_DreamComposer_Controllable_3D_Object_Generation_via_Multi-View_Conditions_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yhyang-myron.github.io/DreamComposer/) <br /> [![GitHub](https://img.shields.io/github/stars/yhyang-myron/DreamComposer?style=flat)](https://github.com/yhyang-myron/DreamComposer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_DreamComposer_Controllable_3D_Object_Generation_via_Multi-View_Conditions_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.03611-b31b1b.svg)](http://arxiv.org/abs/2312.03611) | :heavy_minus_sign: |
| [DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization](https://openaccess.thecvf.com/content/CVPR2024/html/Nam_DreamMatcher_Appearance_Matching_Self-Attention_for_Semantically-Consistent_Text-to-Image_Personalization_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ku-cvlab.github.io/DreamMatcher/) <br /> [![GitHub](https://img.shields.io/github/stars/KU-CVLAB/DreamMatcher?style=flat)](https://github.com/KU-CVLAB/DreamMatcher) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Nam_DreamMatcher_Appearance_Matching_Self-Attention_for_Semantically-Consistent_Text-to-Image_Personalization_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2402.09812-b31b1b.svg)](http://arxiv.org/abs/2402.09812) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vtq2muizj-c) |
| [DreamSalon: A Staged Diffusion Framework for Preserving Identity-Context in Editable Face Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Lin_DreamSalon_A_Staged_Diffusion_Framework_for_Preserving_Identity-Context_in_Editable_CVPR_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_DreamSalon_A_Staged_Diffusion_Framework_for_Preserving_Identity-Context_in_Editable_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.19235-b31b1b.svg)](http://arxiv.org/abs/2403.19235) | :heavy_minus_sign: |
| [DreamVideo: Composing Your Dream Videos with Customized Subject and Motion](https://openaccess.thecvf.com/content/CVPR2024/html/Wei_DreamVideo_Composing_Your_Dream_Videos_with_Customized_Subject_and_Motion_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dreamvideo-t2v.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/ali-vilab/VGen?style=flat)](https://github.com/ali-vilab/VGen) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wei_DreamVideo_Composing_Your_Dream_Videos_with_Customized_Subject_and_Motion_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.04433-b31b1b.svg)](http://arxiv.org/abs/2312.04433) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wiuKiVWHd6I) |
| [DyBluRF: Dynamic Neural Radiance Fields from Blurry Monocular Video](https://openaccess.thecvf.com/content/CVPR2024/html/Sun_DyBluRF_Dynamic_Neural_Radiance_Fields_from_Blurry_Monocular_Video_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://huiqiang-sun.github.io/dyblurf/) <br /> [![GitHub](https://img.shields.io/github/stars/huiqiang-sun/DyBluRF?style=flat)](https://github.com/huiqiang-sun/DyBluRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_DyBluRF_Dynamic_Neural_Radiance_Fields_from_Blurry_Monocular_Video_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.10103-b31b1b.svg)](http://arxiv.org/abs/2403.10103) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0cwJyDC40vw) |
| [DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_DynVideo-E_Harnessing_Dynamic_NeRF_for_Large-Scale_Motion-_and_View-Change_Human-Centric_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://showlab.github.io/DynVideo-E/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_DynVideo-E_Harnessing_Dynamic_NeRF_for_Large-Scale_Motion-_and_View-Change_Human-Centric_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.10624-b31b1b.svg)](http://arxiv.org/abs/2310.10624) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xiRH4Q6B3Yk) |
| [Dysen-VDM: Empowering Dynamics-Aware Text-to-Video Diffusion with LLMs](https://openaccess.thecvf.com/content/CVPR2024/html/Fei_Dysen-VDM_Empowering_Dynamics-aware_Text-to-Video_Diffusion_with_LLMs_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/scofield7419/Dysen?style=flat)](https://github.com/scofield7419/Dysen) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Fei_Dysen-VDM_Empowering_Dynamics-aware_Text-to-Video_Diffusion_with_LLMs_CVPR_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ADyBODiIppQ) |
| [EasyDrag: Efficient Point-based Manipulation on Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Hou_EasyDrag_Efficient_Point-based_Manipulation_on_Diffusion_Models_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/Ace-Pegasus/EasyDrag?style=flat)](https://github.com/Ace-Pegasus/EasyDrag) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Hou_EasyDrag_Efficient_Point-based_Manipulation_on_Diffusion_Models_CVPR_2024_paper.pdf) | :heavy_minus_sign: |
| [ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations](https://openaccess.thecvf.com/content/CVPR2024/html/Patel_ECLIPSE_A_Resource-Efficient_Text-to-Image_Prior_for_Image_Generations_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://eclipse-t2i.vercel.app/) <br /> [![GitHub](https://img.shields.io/github/stars/eclipse-t2i/eclipse-inference?style=flat)](https://github.com/eclipse-t2i/eclipse-inference) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-demo-FFD21F.svg)](https://huggingface.co/spaces/ECLIPSE-Community/ECLIPSE-Kandinsky-v2.2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Patel_ECLIPSE_A_Resource-Efficient_Text-to-Image_Prior_for_Image_Generations_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.04655-b31b1b.svg)](http://arxiv.org/abs/2312.04655) | :heavy_minus_sign: |
| [Edit One for All: Interactive Batch Image Editing](https://openaccess.thecvf.com/content/CVPR2024/html/Nguyen_Edit_One_for_All_Interactive_Batch_Image_Editing_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://thaoshibe.github.io/edit-one-for-all/) <br /> [![GitHub](https://img.shields.io/github/stars/WisconsinAIVision/edit-one-for-all?style=flat)](https://github.com/WisconsinAIVision/edit-one-for-all) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Nguyen_Edit_One_for_All_Interactive_Batch_Image_Editing_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.10219-b31b1b.svg)](http://arxiv.org/abs/2401.10219) | :heavy_minus_sign: |
| [ElasticDiffusion: Training-free Arbitrary Size Image Generation through Global-Local Content Separation](https://openaccess.thecvf.com/content/CVPR2024/html/Haji-Ali_ElasticDiffusion_Training-free_Arbitrary_Size_Image_Generation_through_Global-Local_Content_Separation_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://elasticdiffusion.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/MoayedHajiAli/ElasticDiffusion-official?style=flat)](https://github.com/MoayedHajiAli/ElasticDiffusion-official) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Haji-Ali_ElasticDiffusion_Training-free_Arbitrary_Size_Image_Generation_through_Global-Local_Content_Separation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.18822-b31b1b.svg)](http://arxiv.org/abs/2311.18822) | :heavy_minus_sign: |
| [EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_EmoGen_Emotional_Image_Content_Generation_with_Text-to-Image_Diffusion_Models_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://vcc.tech/research/2024/EmoGen) <br /> [![GitHub](https://img.shields.io/github/stars/JingyuanYY/EmoGen?style=flat)](https://github.com/JingyuanYY/EmoGen) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_EmoGen_Emotional_Image_Content_Generation_with_Text-to-Image_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.04608-b31b1b.svg)](http://arxiv.org/abs/2401.04608) | :heavy_minus_sign: |
| [EMOPortraits: Emotion-enhanced Multimodal One-Shot Head Avatars](https://openaccess.thecvf.com/content/CVPR2024/html/Drobyshev_EMOPortraits_Emotion-enhanced_Multimodal_One-shot_Head_Avatars_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://neeek2303.github.io/EMOPortraits/) <br /> [![GitHub](https://img.shields.io/github/stars/neeek2303/EMOPortraits?style=flat)](https://github.com/neeek2303/EMOPortraits) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Drobyshev_EMOPortraits_Emotion-enhanced_Multimodal_One-shot_Head_Avatars_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2404.19110-b31b1b.svg)](http://arxiv.org/abs/2404.19110) | :heavy_minus_sign: |
| [Exact Fusion via Feature Distribution Matching for Few-Shot Image Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Exact_Fusion_via_Feature_Distribution_Matching_for_Few-shot_Image_Generation_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/ZYBOBO/F2DGAN?style=flat)](https://github.com/ZYBOBO/F2DGAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Exact_Fusion_via_Feature_Distribution_Matching_for_Few-shot_Image_Generation_CVPR_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=A4uKRH3g3NE) |
| [Exploiting Diffusion Prior for Generalizable Dense Prediction](https://openaccess.thecvf.com/content/CVPR2024/html/Lee_Exploiting_Diffusion_Prior_for_Generalizable_Dense_Prediction_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://shinying.github.io/dmp/) <br /> [![GitHub](https://img.shields.io/github/stars/shinying/dmp?style=flat)](https://github.com/shinying/dmp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Lee_Exploiting_Diffusion_Prior_for_Generalizable_Dense_Prediction_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.18832-b31b1b.svg)](http://arxiv.org/abs/2311.18832) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=z4W0LmI-4C8) |
| [Face2Diffusion for Fast and Editable Face Personalization](https://openaccess.thecvf.com/content/CVPR2024/html/Shiohara_Face2Diffusion_for_Fast_and_Editable_Face_Personalization_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mapooon.github.io/Face2DiffusionPage/) <br /> [![GitHub](https://img.shields.io/github/stars/mapooon/Face2Diffusion?style=flat)](https://github.com/mapooon/Face2Diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Shiohara_Face2Diffusion_for_Fast_and_Editable_Face_Personalization_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.05094-b31b1b.svg)](http://arxiv.org/abs/2403.05094) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=O5w5yjXeW2c) |
| [FaceChain-SuDe: Building Derived Class to Inherit Category Attributes for One-Shot Subject-Driven Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Qiao_FaceChain-SuDe_Building_Derived_Class_to_Inherit_Category_Attributes_for_One-shot_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/modelscope/facechain?style=flat)](https://github.com/modelscope/facechain) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Qiao_FaceChain-SuDe_Building_Derived_Class_to_Inherit_Category_Attributes_for_One-shot_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.06775-b31b1b.svg)](http://arxiv.org/abs/2403.06775) | :heavy_minus_sign: |
| [Faces that Speak: Jointly Synthesising Talking Face and Speech from Text](https://openaccess.thecvf.com/content/CVPR2024/html/Jang_Faces_that_Speak_Jointly_Synthesising_Talking_Face_and_Speech_from_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://mm.kaist.ac.kr/projects/faces-that-speak/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Jang_Faces_that_Speak_Jointly_Synthesising_Talking_Face_and_Speech_from_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2405.10272-b31b1b.svg)](http://arxiv.org/abs/2405.10272) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=4NU53iE62f0) |
| [Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis](https://openaccess.thecvf.com/content/CVPR2024/html/Wu_Fairy_Fast_Parallelized_Instruction-Guided_Video-to-Video_Synthesis_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://fairy-video2video.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Fairy_Fast_Parallelized_Instruction-Guided_Video-to-Video_Synthesis_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.13834-b31b1b.svg)](http://arxiv.org/abs/2312.13834) | :heavy_minus_sign: |
| [Fixed Point Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lukemelas.github.io/fixed-point-diffusion-models/) <br /> [![GitHub](https://img.shields.io/github/stars/lukemelas/fixed-point-diffusion-models?style=flat)](https://github.com/lukemelas/fixed-point-diffusion-models) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.08741-b31b1b.svg)](http://arxiv.org/abs/2401.08741) | :heavy_minus_sign: |
| [Focus on Your Instruction: Fine-grained and Multi-Instruction Image Editing by Attention Modulation](https://openaccess.thecvf.com/content/CVPR2024/html/Guo_Focus_on_Your_Instruction_Fine-grained_and_Multi-instruction_Image_Editing_by_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/guoqincode/Focus-on-Your-Instruction?style=flat)](https://github.com/guoqincode/Focus-on-Your-Instruction) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_Focus_on_Your_Instruction_Fine-grained_and_Multi-instruction_Image_Editing_by_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.10113-b31b1b.svg)](http://arxiv.org/abs/2312.10113) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rPknqOJsxkg) |
| [FreeControl: Training-Free Spatial Control of any Text-to-Image Diffusion Model with any Condition](https://openaccess.thecvf.com/content/CVPR2024/html/Mo_FreeControl_Training-Free_Spatial_Control_of_Any_Text-to-Image_Diffusion_Model_with_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://genforce.github.io/freecontrol/) <br /> [![GitHub](https://img.shields.io/github/stars/genforce/freecontrol?style=flat)](https://github.com/genforce/freecontrol) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Mo_FreeControl_Training-Free_Spatial_Control_of_Any_Text-to-Image_Diffusion_Model_with_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.07536-b31b1b.svg)](http://arxiv.org/abs/2312.07536) | :heavy_minus_sign: |
| [FreeCustom: Tuning-Free Customized Image Generation for Multi-Concept Composition](https://openaccess.thecvf.com/content/CVPR2024/html/Ding_FreeCustom_Tuning-Free_Customized_Image_Generation_for_Multi-Concept_Composition_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://aim-uofa.github.io/FreeCustom/) <br /> [![GitHub](https://img.shields.io/github/stars/aim-uofa/FreeCustom?style=flat)](https://github.com/aim-uofa/FreeCustom) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Ding_FreeCustom_Tuning-Free_Customized_Image_Generation_for_Multi-Concept_Composition_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2405.13870-b31b1b.svg)](http://arxiv.org/abs/2405.13870) | :heavy_minus_sign: |
| [FreeDrag: Feature Dragging for Reliable Point-based Image Editing](https://openaccess.thecvf.com/content/CVPR2024/html/Ling_FreeDrag_Feature_Dragging_for_Reliable_Point-based_Image_Editing_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://lin-chen.site/projects/freedrag/) <br /> [![GitHub](https://img.shields.io/github/stars/LPengYang/FreeDrag?style=flat)](https://github.com/LPengYang/FreeDrag) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Ling_FreeDrag_Feature_Dragging_for_Reliable_Point-based_Image_Editing_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.04684-b31b1b.svg)](http://arxiv.org/abs/2307.04684) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dKPqMQG1CeE) |
| [FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_FRESCO_Spatial-Temporal_Correspondence_for_Zero-Shot_Video_Translation_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.mmlab-ntu.com/project/fresco/) <br /> [![GitHub](https://img.shields.io/github/stars/williamyang1991/FRESCO?style=flat)](https://github.com/williamyang1991/FRESCO) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_FRESCO_Spatial-Temporal_Correspondence_for_Zero-Shot_Video_Translation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.12962-b31b1b.svg)](http://arxiv.org/abs/2403.12962) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jLnGx5H-wLw) |
| [FSRT: Facial Scene Representation Transformer for Face Reenactment from Factorized Appearance Head-Pose and Facial Expression Features](https://openaccess.thecvf.com/content/CVPR2024/html/Rochow_FSRT_Facial_Scene_Representation_Transformer_for_Face_Reenactment_from_Factorized_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://andrerochow.github.io/fsrt/) <br /> [![GitHub](https://img.shields.io/github/stars/andrerochow/fsrt?style=flat)](https://github.com/andrerochow/fsrt) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Rochow_FSRT_Facial_Scene_Representation_Transformer_for_Face_Reenactment_from_Factorized_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2404.09736-b31b1b.svg)](http://arxiv.org/abs/2404.09736) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=GIoZ8QoshcM) |
| [Gaussian Shell Maps for Efficient 3D Human Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/computational-imaging/GSM?style=flat)](https://github.com/computational-imaging/GSM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.17857-b31b1b.svg)](http://arxiv.org/abs/2311.17857) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WSTBftn7N3s) |
| [GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Yi_GaussianDreamer_Fast_Generation_from_Text_to_3D_Gaussians_by_Bridging_CVPR_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://taoranyi.com/gaussiandreamer/) <br /> [![GitHub](https://img.shields.io/github/stars/hustvl/GaussianDreamer?style=flat)](https://github.com/hustvl/GaussianDreamer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Yi_GaussianDreamer_Fast_Generation_from_Text_to_3D_Gaussians_by_Bridging_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.08529-b31b1b.svg)](http://arxiv.org/abs/2310.08529) | :heavy_minus_sign: |
| [GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image](https://openaccess.thecvf.com/content/CVPR2024/html/Bao_GeneAvatar_Generic_Expression-Aware_Volumetric_Head_Avatar_Editing_from_a_Single_CVPR_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zju3dv.github.io/geneavatar/) <br /> [![GitHub](https://img.shields.io/github/stars/zju3dv/GeneAvatar?style=flat)](https://github.com/zju3dv/GeneAvatar) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Bao_GeneAvatar_Generic_Expression-Aware_Volumetric_Head_Avatar_Editing_from_a_Single_CVPR_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2404.02152-b31b1b.svg)](http://arxiv.org/abs/2404.02152) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=4zfbfPivtVU) |
