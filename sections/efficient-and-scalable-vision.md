# CVPR-2023-Papers

<div align="center">
 <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/adversarial-attack-and-defense.md">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
 </a>
 <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
 </a>
 <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/computational-imaging.md">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
 </a>
</div>

## Efficient and Scalable Vision

![Section Papers](https://img.shields.io/badge/Section%20Papers-48-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-36-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-30-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-27-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| DisWOT: Student Architecture Search for Distillation WithOut Training | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lilujunai.github.io/DisWOT-CVPR2023/) <br /> [![GitHub](https://img.shields.io/github/stars/lilujunai/DisWOT-CVPR2023)](https://github.com/lilujunai/DisWOT-CVPR2023) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_DisWOT_Student_Architecture_Search_for_Distillation_WithOut_Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15678-b31b1b.svg)](http://arxiv.org/abs/2303.15678) | :heavy_minus_sign: |
| Stitchable Neural Networks <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://snnet.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/ziplab/SN-Net)](https://github.com/ziplab/SN-Net) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Stitchable_Neural_Networks_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.06586-b31b1b.svg)](http://arxiv.org/abs/2302.06586) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=SfpHVWWLREM) |
| NIRVANA: Neural Implicit Representations of Videos with Adaptive Networks and Autoregressive Patch-Wise Modeling | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.cs.umd.edu/~shishira/Nirvana/nirvana.html) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Maiya_NIRVANA_Neural_Implicit_Representations_of_Videos_With_Adaptive_Networks_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.14593-b31b1b.svg)](http://arxiv.org/abs/2212.14593) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=cJOt3p2WLj0) |
| ResFormer: Scaling ViTs with Multi-Resolution Training | [![GitHub](https://img.shields.io/github/stars/ruitian12/resformer)](https://github.com/ruitian12/resformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_ResFormer_Scaling_ViTs_With_Multi-Resolution_Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.00776-b31b1b.svg)](http://arxiv.org/abs/2212.00776) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tx5bSpeRcrE) |
| PD-Quant: Post-Training Quantization based on Prediction Difference Metric | [![GitHub](https://img.shields.io/github/stars/hustvl/PD-Quant)](https://github.com/hustvl/PD-Quant) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_PD-Quant_Post-Training_Quantization_Based_on_Prediction_Difference_Metric_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.07048-b31b1b.svg)](https://arxiv.org/abs/2212.07048) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5OEmjFcqNPo) |
| DepGraph: Towards any Structural Pruning | [![GitHub](https://img.shields.io/github/stars/VainF/Torch-Pruning)](https://github.com/VainF/Torch-Pruning) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.12900-b31b1b.svg)](http://arxiv.org/abs/2301.12900) | :heavy_minus_sign: |
| Towards Professional Level Crowd Annotation of Expert Domain Data | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Towards_Professional_Level_Crowd_Annotation_of_Expert_Domain_Data_CVPR_2023_paper.pdf) |[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jRQVuAytdwA) |
| GENIE: Show Me the Data for Quantization | [![GitHub](https://img.shields.io/github/stars/SamsungLabs/Genie)](https://github.com/SamsungLabs/Genie) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jeon_Genie_Show_Me_the_Data_for_Quantization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04780-b31b1b.svg)](http://arxiv.org/abs/2212.04780) | :heavy_minus_sign: |
| Boost Vision Transformer with GPU-Friendly Sparsity and Quantization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Boost_Vision_Transformer_With_GPU-Friendly_Sparsity_and_Quantization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.10727-b31b1b.svg)](http://arxiv.org/abs/2305.10727) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=gilS4ELhi9M) |
| MobileOne: An Improved One Millisecond Mobile Backbone | [![GitHub](https://img.shields.io/github/stars/apple/ml-mobileone)](https://github.com/apple/ml-mobileone) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Vasu_MobileOne_An_Improved_One_Millisecond_Mobile_Backbone_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.04040-b31b1b.svg)](http://arxiv.org/abs/2206.04040) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=SDOUtpEvTgg) |
| 1% VS 100%: Parameter-Efficient Low Rank Adapter for Dense Predictions | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_1_VS_100_Parameter-Efficient_Low_Rank_Adapter_for_Dense_Predictions_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Discriminator-Cooperated Feature Map Distillation for GAN Compression | [![GitHub](https://img.shields.io/github/stars/poopit/DCD-official)](https://github.com/poopit/DCD-official) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Discriminator-Cooperated_Feature_Map_Distillation_for_GAN_Compression_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.14169-b31b1b.svg)](http://arxiv.org/abs/2212.14169) | :heavy_minus_sign: |
| EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/microsoft/Cream/tree/main/EfficientViT) <br /> [![GitHub](https://img.shields.io/github/stars/microsoft/Cream)](https://github.com/microsoft/Cream) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_EfficientViT_Memory_Efficient_Vision_Transformer_With_Cascaded_Group_Attention_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.07027-b31b1b.svg)](http://arxiv.org/abs/2305.07027) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZYATsJboyhM) |
| itKD: Interchange Transfer-based Knowledge Distillation for 3D Object Detection | [![GitHub](https://img.shields.io/github/stars/hyeon-jo/interchange-transfer-KD)](https://github.com/hyeon-jo/interchange-transfer-KD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2205.15531-b31b1b.svg)](http://arxiv.org/abs/2205.15531) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=a4oBLiJ62XI) |
| Slimmable Dataset Condensation <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Slimmable_Dataset_Condensation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zvcdG80p1-s) |
| Dynamic Inference with Grounding based Vision and Language Models | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Uzkent_Dynamic_Inference_With_Grounding_Based_Vision_and_Language_Models_CVPR_2023_paper.pdf) <br /> [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/dynamic-inference-with-grounding-based-vision-and-language-models) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3uoHUR25Ht4) |
| ScaleDet: A Scalable Multi-Dataset Object Detector | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_ScaleDet_A_Scalable_Multi-Dataset_Object_Detector_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.04849-b31b1b.svg)](https://arxiv.org/abs/2306.04849) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vy3DVdlSTaw) |
| Learning to Zoom and Unzoom | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tchittesh.github.io/lzu/) <br /> [![GitHub](https://img.shields.io/github/stars/tchittesh/lzu)](https://github.com/tchittesh/lzu) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Thavamani_Learning_To_Zoom_and_Unzoom_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15390-b31b1b.svg)](http://arxiv.org/abs/2303.15390) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wALSrBZiUgc) |
| Generic-to-Specific Distillation of Masked Autoencoders | [![GitHub](https://img.shields.io/github/stars/pengzhiliang/G2SD)](https://github.com/pengzhiliang/G2SD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Generic-to-Specific_Distillation_of_Masked_Autoencoders_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.14771-b31b1b.svg)](http://arxiv.org/abs/2302.14771) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=k3-eJiuRxBE) |
| Post-Training Quantization on Diffusion Models | [![GitHub](https://img.shields.io/github/stars/42Shawn/PTQ4DM)](https://github.com/42Shawn/PTQ4DM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Shang_Post-Training_Quantization_on_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.15736-b31b1b.svg)](http://arxiv.org/abs/2211.15736) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EkCvrRcLzOc) |
| Global Vision Transformer Pruning with Hessian-Aware Saliency | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Global_Vision_Transformer_Pruning_With_Hessian-Aware_Saliency_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2110.04869-b31b1b.svg)](http://arxiv.org/abs/2110.04869) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0D8O7yBw4h4) |
| Network Expansion for Practical Training Acceleration | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/huawei-noah/Efficient-Computing/tree/master/TrainingAcceleration/NetworkExpansion) <br /> [![GitHub](https://img.shields.io/github/stars/huawei-noah/Efficient-Computing)](https://github.com/huawei-noah/Efficient-Computing) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_Network_Expansion_for_Practical_Training_Acceleration_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Compacting Binary Neural Networks by Sparse Kernel Selection | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yikaiw.github.io/projects/CVPR23-Sparks/slides.pdf) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Compacting_Binary_Neural_Networks_by_Sparse_Kernel_Selection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14470-b31b1b.svg)](http://arxiv.org/abs/2303.14470) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wFfW5dUZo-Y) |
| PointDistiller: Structured Knowledge Distillation Towards Efficient and Compact 3D Detection | [![GitHub](https://img.shields.io/github/stars/RunpeiDong/PointDistiller)](https://github.com/RunpeiDong/PointDistiller) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_PointDistiller_Structured_Knowledge_Distillation_Towards_Efficient_and_Compact_3D_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2205.11098-b31b1b.svg)](http://arxiv.org/abs/2205.11098) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_zWyXXZwDVc) |
| Practical Network Acceleration with Tiny Sets | [![GitHub](https://img.shields.io/github/stars/DoctorKey/Practise)](https://github.com/DoctorKey/Practise) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Practical_Network_Acceleration_With_Tiny_Sets_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2202.07861-b31b1b.svg)](http://arxiv.org/abs/2202.07861) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZMjT9YZDhTw) |
| Memory-Friendly Scalable Super-Resolution via Rewinding Lottery Ticket Hypothesis | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Memory-Friendly_Scalable_Super-Resolution_via_Rewinding_Lottery_Ticket_Hypothesis_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Fast Point Cloud Generation with Straight Flows | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Fast_Point_Cloud_Generation_With_Straight_Flows_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.01747-b31b1b.svg)](http://arxiv.org/abs/2212.01747) | :heavy_minus_sign: |
| Rethinking Federated Learning with Domain Shift: A Prototype View | [![GitHub](https://img.shields.io/github/stars/WenkeHuang/RethinkFL)](https://github.com/WenkeHuang/RethinkFL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=UGitdEfrPk0) |
| Solving Oscillation Problem in Post-Training Quantization through a Theoretical Perspective | [![GitHub](https://img.shields.io/github/stars/bytedance/MRECG)](https://github.com/bytedance/MRECG) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_Solving_Oscillation_Problem_in_Post-Training_Quantization_Through_a_Theoretical_Perspective_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11906-b31b1b.svg)](http://arxiv.org/abs/2303.11906) | :heavy_minus_sign: |
| ScaleKD: Distilling Scale-Aware Knowledge in Small Object Detector | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_ScaleKD_Distilling_Scale-Aware_Knowledge_in_Small_Object_Detector_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Adaptive Channel Sparsity for Federated Learning under System Heterogeneity | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_Adaptive_Channel_Sparsity_for_Federated_Learning_Under_System_Heterogeneity_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://m.youtube.com/watch?v=GRUA0y_kzN0) |
| A-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Compacting_Binary_Neural_Networks_by_Sparse_Kernel_Selection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.07994-b31b1b.svg)](https://arxiv.org/abs/2302.07994) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FQ8s-0HDtTE) |
| NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_NoisyQuant_Noisy_Bias-Enhanced_Post-Training_Activation_Quantization_for_Vision_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.16056-b31b1b.svg)](http://arxiv.org/abs/2211.16056) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-b2aBYOdsSU) |
| NIPQ: Noise Proxy-based Integrated Pseudo-Quantization | [![GitHub](https://img.shields.io/github/stars/ECoLab-POSTECH/NIPQ)](https://github.com/ECoLab-POSTECH/NIPQ) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Shin_NIPQ_Noise_Proxy-Based_Integrated_Pseudo-Quantization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.00820-b31b1b.svg)](https://arxiv.org/abs/2206.00820)|:heavy_minus_sign: |
| FlatFormer: <u>Flat</u>tened Window Attention for Efficient Point Cloud Trans<u>former</u> | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://flatformer.mit.edu/) <br /> [![GitHub](https://img.shields.io/github/stars/mit-han-lab/flatformer)](https://github.com/mit-han-lab/flatformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_FlatFormer_Flattened_Window_Attention_for_Efficient_Point_Cloud_Transformer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.08739-b31b1b.svg)](http://arxiv.org/abs/2301.08739) | :heavy_minus_sign: |
| SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer | [![GitHub](https://img.shields.io/github/stars/mit-han-lab/sparsevit)](https://github.com/mit-han-lab/sparsevit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_SparseViT_Revisiting_Activation_Sparsity_for_Efficient_High-Resolution_Vision_Transformer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17605-b31b1b.svg)](http://arxiv.org/abs/2303.17605) | :heavy_minus_sign: |
| Efficient On-Device Training via Gradient Filtering | [![GitHub](https://img.shields.io/github/stars/SLDGroup/GradientFilter-CVPR23)](https://github.com/SLDGroup/GradientFilter-CVPR23) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Efficient_On-Device_Training_via_Gradient_Filtering_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.00330-b31b1b.svg)](http://arxiv.org/abs/2301.00330) | :heavy_minus_sign: |
| Towards High-Quality and Efficient Video Super-Resolution via Spatial-Temporal Data Overfitting <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/coulsonlee/STDO-CVPR2023)](https://github.com/coulsonlee/STDO-CVPR2023) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Towards_High-Quality_and_Efficient_Video_Super-Resolution_via_Spatial-Temporal_Data_Overfitting_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08331-b31b1b.svg)](http://arxiv.org/abs/2303.08331) | :heavy_minus_sign: |
| You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model | [![GitHub](https://img.shields.io/github/stars/OFA-Sys/OFA)](https://github.com/OFA-Sys/OFA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_You_Need_Multiple_Exiting_Dynamic_Early_Exiting_for_Accelerating_Unified_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11152-b31b1b.svg)](http://arxiv.org/abs/2211.11152) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2op5v3-0EA4) |
| Adaptive Data-Free Quantization | [![GitHub](https://img.shields.io/github/stars/hfutqian/AdaDFQ)](https://github.com/hfutqian/AdaDFQ) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Qian_Adaptive_Data-Free_Quantization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06869-b31b1b.svg)](http://arxiv.org/abs/2303.06869) | :heavy_minus_sign: |
| Train-Once-for-All Personalization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Train-Once-for-All_Personalization_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Neural Rate Estimator and Unsupervised Learning for Efficient Distributed Image Analytics in Split-DNN Models | [![GitHub](https://img.shields.io/github/stars/intellabs/spic)](https://github.com/intellabs/spic) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ahuja_Neural_Rate_Estimator_and_Unsupervised_Learning_for_Efficient_Distributed_Image_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers | [![GitHub](https://img.shields.io/github/stars/lim142857/Sparsifiner)](https://github.com/lim142857/Sparsifiner) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Sparsifiner_Learning_Sparse_Instance-Dependent_Attention_for_Efficient_Vision_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13755-b31b1b.svg)](http://arxiv.org/abs/2303.13755) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pUISIydZFHU) |
| FFCV: Accelerating Training by Removing Data Bottlenecks | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://ffcv.io/) <br /> [![GitHub](https://img.shields.io/github/stars/libffcv/ffcv)](https://github.com/libffcv/ffcv) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Leclerc_FFCV_Accelerating_Training_by_Removing_Data_Bottlenecks_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.12517-b31b1b.svg)](https://arxiv.org/abs/2306.12517)| :heavy_minus_sign: |
| Samples with Low Loss Curvature Improve Data Efficiency | [![GitHub](https://img.shields.io/github/stars/isha-garg/SLo-Curves)](https://github.com/isha-garg/SLo-Curves) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Garg_Samples_With_Low_Loss_Curvature_Improve_Data_Efficiency_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mGagNmTaRy8) |
| Decentralized Learning with Multi-Headed Distillation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhmoginov_Decentralized_Learning_With_Multi-Headed_Distillation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.15774-b31b1b.svg)](http://arxiv.org/abs/2211.15774) | :heavy_minus_sign: |
| Bit-Shrinking: Limiting Instantaneous Sharpness for Improving Post-Training Quantization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Bit-Shrinking_Limiting_Instantaneous_Sharpness_for_Improving_Post-Training_Quantization_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Masked Autoencoders Enable Efficient Knowledge Distillers | [![GitHub](https://img.shields.io/github/stars/UCSC-VLAA/DMAE)](https://github.com/UCSC-VLAA/DMAE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_Masked_Autoencoders_Enable_Efficient_Knowledge_Distillers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.12256-b31b1b.svg)](http://arxiv.org/abs/2208.12256) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=63CMleVH9oY) |
