# CVPR-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/datasets-and-evaluation.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/adversarial-attack-and-defense.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Scene Analysis and Understanding

![Section Papers](https://img.shields.io/badge/Section%20Papers-54-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-42-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-45-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-42-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| You Only Segment Once: Towards Real-Time Panoptic Segmentation | [![GitHub](https://img.shields.io/github/stars/hujiecpp/YOSO?style=flat)](https://github.com/hujiecpp/YOSO) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_You_Only_Segment_Once_Towards_Real-Time_Panoptic_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14651-b31b1b.svg)](http://arxiv.org/abs/2303.14651) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wQCKerE_NmQ) |
| IS-GGT: Iterative Scene Graph Generation with Generative Transformers | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://saakur.github.io/Projects/IS_GGT/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kundu_IS-GGT_Iterative_Scene_Graph_Generation_With_Generative_Transformers_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CZ9aCrqgY9E) |
| Disentangling Orthogonal Planes for Indoor Panoramic Room Layout Estimation with Cross-Scale Distortion Awareness | [![GitHub](https://img.shields.io/github/stars/zhijieshen-bjtu/DOPNet?style=flat)](https://github.com/zhijieshen-bjtu/DOPNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_Disentangling_Orthogonal_Planes_for_Indoor_Panoramic_Room_Layout_Estimation_With_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.00971-b31b1b.svg)](http://arxiv.org/abs/2303.00971) | :heavy_minus_sign: |
| Panoptic Video Scene Graph Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jingkang50.github.io/PVSG/) <br /> [![GitHub](https://img.shields.io/github/stars/LilyDaytoy/OpenPVSG?style=flat)](https://github.com/LilyDaytoy/OpenPVSG) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Panoptic_Video_Scene_Graph_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.17058-b31b1b.svg)](http://arxiv.org/abs/2311.17058) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZvKeIZ8LHVw) |
| 3D Spatial Multimodal Knowledge Accumulation for Scene Graph Prediction in Point Cloud | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_3D_Spatial_Multimodal_Knowledge_Accumulation_for_Scene_Graph_Prediction_in_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=srvsnfCZTQI) |
| JacobiNeRF: NeRF Shaping with Mutual Information Gradients | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://xxm19.github.io/jnerf/) <br /> [![GitHub](https://img.shields.io/github/stars/xxm19/jacobinerf?style=flat)](https://github.com/xxm19/jacobinerf) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_JacobiNeRF_NeRF_Shaping_With_Mutual_Information_Gradients_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00341-b31b1b.svg)](http://arxiv.org/abs/2304.00341) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=uKU9UdVL6GQ) |
| Learning Geometric-Aware Properties in 2D Representation using Lightweight CAD Models, or Zero Real 3D Pairs | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://geoaware2drepusingcad.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Arsomngern_Learning_Geometric-Aware_Properties_in_2D_Representation_Using_Lightweight_CAD_Models_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nTb1RC9T-0I) |
| Learning and Aggregating Lane Graphs for Urban Automated Driving | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](http://urbanlanegraph.cs.uni-freiburg.de/) <br /> [![GitHub](https://img.shields.io/github/stars/jzuern/lanegnn?style=flat)](https://github.com/jzuern/lanegnn) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Buchner_Learning_and_Aggregating_Lane_Graphs_for_Urban_Automated_Driving_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.06175-b31b1b.svg)](http://arxiv.org/abs/2302.06175) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zHkbF_pjfdg) |
| MIME: Human-Aware 3D Scene Generation | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://mime.is.tue.mpg.de/) <br /> [![GitHub](https://img.shields.io/github/stars/yhw-yhw/MIME?style=flat)](https://github.com/yhw-yhw/MIME) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_MIME_Human-Aware_3D_Scene_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04360-b31b1b.svg)](http://arxiv.org/abs/2212.04360) | :heavy_minus_sign: |
| Connecting the Dots: Floorplan Reconstruction using Two-Level Queries | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ywyue.github.io/RoomFormer/) <br /> [![GitHub](https://img.shields.io/github/stars/ywyue/RoomFormer?style=flat)](https://github.com/ywyue/RoomFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yue_Connecting_the_Dots_Floorplan_Reconstruction_Using_Two-Level_Queries_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.15658-b31b1b.svg)](http://arxiv.org/abs/2211.15658) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=yzYe4yVN1NU) |
| NeRF-RPN: A General Framework for Object Detection in NeRFs | [![GitHub](https://img.shields.io/github/stars/lyclyc52/NeRF_RPN?style=flat)](https://github.com/lyclyc52/NeRF_RPN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_NeRF-RPN_A_General_Framework_for_Object_Detection_in_NeRFs_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11646-b31b1b.svg)](http://arxiv.org/abs/2211.11646) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=M8_4Ih1CJjE) |
| Relational Context Learning for Human-Object Interaction Detection | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://cvlab.postech.ac.kr/research/MUREN/) <br /> [![GitHub](https://img.shields.io/github/stars/OreoChocolate/MUREN?style=flat)](https://github.com/OreoChocolate/MUREN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Relational_Context_Learning_for_Human-Object_Interaction_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04997-b31b1b.svg)](http://arxiv.org/abs/2304.04997) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=u-5EmiGWbsA) |
| Symmetric Shape-Preserving Autoencoder for Unsupervised Real Scene Point Cloud Completion | [![GitHub](https://img.shields.io/github/stars/murcherful/USSPA?style=flat)](https://github.com/murcherful/USSPA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_Symmetric_Shape-Preserving_Autoencoder_for_Unsupervised_Real_Scene_Point_Cloud_Completion_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=1iWvKcR9DzA) |
| Token Contrast for Weakly-Supervised Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/rulixiang/ToCo?style=flat)](https://github.com/rulixiang/ToCo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ru_Token_Contrast_for_Weakly-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.01267-b31b1b.svg)](http://arxiv.org/abs/2303.01267) | :heavy_minus_sign: |
| MM-3DScene: 3D Scene Understanding by Customizing Masked Modeling with Informative-Preserved Reconstruction and Self-Distilled Consistency | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mingyexu.github.io/mm3dscene/) <br /> [![GitHub](https://img.shields.io/github/stars/MingyeXu/mm-3dscene?style=flat)](https://github.com/MingyeXu/mm-3dscene) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_MM-3DScene_3D_Scene_Understanding_by_Customizing_Masked_Modeling_With_Informative-Preserved_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09948-b31b1b.svg)](http://arxiv.org/abs/2212.09948) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=MxRJG9EUR5Q) |
| Primitive Generation and Semantic-related Alignment for Universal Zero-Shot Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://henghuiding.github.io/PADing/) <br /> [![GitHub](https://img.shields.io/github/stars/heshuting555/PADing?style=flat)](https://github.com/heshuting555/PADing) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/He_Primitive_Generation_and_Semantic-Related_Alignment_for_Universal_Zero-Shot_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.11087-b31b1b.svg)](http://arxiv.org/abs/2306.11087) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=yZp-i7ZgU_M) |
| CLIP2Scene: Towards Label-Efficient 3D Scene Understanding by CLIP | [![GitHub](https://img.shields.io/github/stars/runnanchen/CLIP2Scene?style=flat)](https://github.com/runnanchen/CLIP2Scene) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.04926-b31b1b.svg)](http://arxiv.org/abs/2301.04926) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OT6AvAVlNNs) |
| Multispectral Video Semantic Segmentation: A Benchmark Dataset and Baseline | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jiwei0921.github.io/Multispectral-Video-Semantic-Segmentation/) <br /> [![GitHub](https://img.shields.io/github/stars/jiwei0921/MVSS-Baseline?style=flat)](https://github.com/jiwei0921/MVSS-Baseline) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Multispectral_Video_Semantic_Segmentation_A_Benchmark_Dataset_and_Baseline_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Optimal Transport Minimization: Crowd Localization on Density Maps for Semi-Supervised Counting <br/> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/Elin24/OT-M?style=flat)](https://github.com/Elin24/OT-M) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Optimal_Transport_Minimization_Crowd_Localization_on_Density_Maps_for_Semi-Supervised_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=415yW0YBC0I) |
| Indiscernible Object Counting in Underwater Scenes | [![GitHub](https://img.shields.io/github/stars/GuoleiSun/Indiscernible-Object-Counting?style=flat)](https://github.com/GuoleiSun/Indiscernible-Object-Counting) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Indiscernible_Object_Counting_in_Underwater_Scenes_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.11677-b31b1b.svg)](http://arxiv.org/abs/2304.11677) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3-XRKmGh154) |
| Long Range Pooling for 3D Large-Scale Scene Understanding | [![GitHub](https://img.shields.io/github/stars/Visual-Attention-Network/LRPNet?style=flat)](https://github.com/Visual-Attention-Network/LRPNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Long_Range_Pooling_for_3D_Large-Scale_Scene_Understanding_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.06962-b31b1b.svg)](http://arxiv.org/abs/2301.06962) | :heavy_minus_sign: |
| Delivering Arbitrary-Modal Semantic Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jamycheung.github.io/DELIVER.html) <br /> [![GitHub](https://img.shields.io/github/stars/jamycheung/DELIVER?style=flat)](https://github.com/jamycheung/DELIVER) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Delivering_Arbitrary-Modal_Semantic_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.01480-b31b1b.svg)](https://arxiv.org/abs/2303.01480) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=X-VeSLsEToA) |
| Images Speak in Images: A Generalist Painter for In-Context Visual Learning | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg?style=flat)](https://github.com/baaivision/Painter/tree/main/Painter) <br /> [![GitHub](https://img.shields.io/github/stars/baaivision/Painter?style=flat)](https://github.com/baaivision/Painter) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Images_Speak_in_Images_A_Generalist_Painter_for_In-Context_Visual_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.02499-b31b1b.svg)](https://arxiv.org/abs/2212.02499) | :heavy_minus_sign: |
| SCPNet: Semantic Scene Completion on Point Cloud <br/> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/SCPNet/Codes-for-SCPNet?style=flat)](https://github.com/SCPNet/Codes-for-SCPNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xia_SCPNet_Semantic_Scene_Completion_on_Point_Cloud_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06884-b31b1b.svg)](https://arxiv.org/abs/2303.06884) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=X2mHXxPM5hg) |
| Content-Aware Token Sharing for Efficient Semantic Segmentation with Vision Transformers | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tue-mps.github.io/CTS/) <br /> [![GitHub](https://img.shields.io/github/stars/tue-mps/cts-segmenter?style=flat)](https://github.com/tue-mps/cts-segmenter) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Content-Aware_Token_Sharing_for_Efficient_Semantic_Segmentation_With_Vision_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.02095-b31b1b.svg)](https://arxiv.org/abs/2306.02095) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=omkEFGBmcqI) |
| OpenScene: 3D Scene Understanding with Open Vocabularies | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://pengsongyou.github.io/openscene) <br /> [![GitHub](https://img.shields.io/github/stars/pengsongyou/openscene?style=flat)](https://github.com/pengsongyou/openscene) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Peng_OpenScene_3D_Scene_Understanding_With_Open_Vocabularies_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.15654-b31b1b.svg)](https://arxiv.org/abs/2211.15654) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jZxCLHyDJf8) |
| Devil's on the Edges: Selective Quad Attention for Scene Graph Generation | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://cvlab.postech.ac.kr/research/SQUAT/) <br /> [![GitHub](https://img.shields.io/github/stars/hesedjds/SQUAT?style=flat)](https://github.com/hesedjds/SQUAT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jung_Devils_on_the_Edges_Selective_Quad_Attention_for_Scene_Graph_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03495-b31b1b.svg)](https://arxiv.org/abs/2304.03495) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jwYNS1EM6v4) |
| Delving into Shape-Aware Zero-Shot Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/Liuxinyv/SAZS?style=flat)](https://github.com/Liuxinyv/SAZS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Delving_Into_Shape-Aware_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08491-b31b1b.svg)](https://arxiv.org/abs/2304.08491) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PxQ4hzGETZQ) |
| Category Query Learning for Human-Object Interaction Classification | [![GitHub](https://img.shields.io/github/stars/Charles-Xie/CQL?style=flat)](https://github.com/Charles-Xie/CQL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Category_Query_Learning_for_Human-Object_Interaction_Classification_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14005-b31b1b.svg)](https://arxiv.org/abs/2303.14005) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=UbV5wtOs4DM) |
| Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jetd1.github.io/nerflets-web/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Nerflets_Local_Radiance_Fields_for_Efficient_Structure-Aware_3D_Scene_Representation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03361-b31b1b.svg)](https://arxiv.org/abs/2303.03361) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rfPVNXVV_Ns) |
| DejaVu: Conditional Regenerative Learning to Enhance Dense Prediction | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Borse_DejaVu_Conditional_Regenerative_Learning_To_Enhance_Dense_Prediction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.01573-b31b1b.svg)](https://arxiv.org/abs/2303.01573) | :heavy_minus_sign: |
| SCOOP: Self-Supervised Correspondence and Optimization-based Scene Flow | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://itailang.github.io/SCOOP/) <br /> [![GitHub](https://img.shields.io/github/stars/itailang/SCOOP?style=flat)](https://github.com/itailang/SCOOP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lang_SCOOP_Self-Supervised_Correspondence_and_Optimization-Based_Scene_Flow_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14020-b31b1b.svg)](https://arxiv.org/abs/2211.14020) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=b8MVWGU7V4E) |
| Incremental 3D Semantic Scene Graph Prediction from RGB Sequences | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://shunchengwu.github.io/MonoSSG) <br /> [![GitHub](https://img.shields.io/github/stars/ShunChengWu/MonoSSG?style=flat)](https://github.com/ShunChengWu/MonoSSG) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Incremental_3D_Semantic_Scene_Graph_Prediction_From_RGB_Sequences_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.02743-b31b1b.svg)](https://arxiv.org/abs/2305.02743) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2uALwmw6WbU) |
| PanelNet: Understanding 360 Indoor Environment via Panel Representation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_PanelNet_Understanding_360_Indoor_Environment_via_Panel_Representation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.09078-b31b1b.svg)](https://arxiv.org/abs/2305.09078) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=urGHusgV8Z4) |
| Perspective Fields for Single Image Camera Calibration <br/> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jinlinyi.github.io/PerspectiveFields/) <br /> [![GitHub](https://img.shields.io/github/stars/jinlinyi/PerspectiveFields?style=flat)](https://github.com/jinlinyi/PerspectiveFields) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Perspective_Fields_for_Single_Image_Camera_Calibration_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03239-b31b1b.svg)](https://arxiv.org/abs/2212.03239) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=sN5B_ZvMva8) |
| Open-Category Human-Object Interaction Pre-Training via Language Modeling Framework | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Open-Category_Human-Object_Interaction_Pre-Training_via_Language_Modeling_Framework_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Fast Contextual Scene Graph Generation with Unbiased Context Augmentation | [![GitHub](https://img.shields.io/github/stars/moshuilanting/fast-context-scene-graph-generation?style=flat)](https://github.com/moshuilanting/fast-context-scene-graph-generation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Fast_Contextual_Scene_Graph_Generation_With_Unbiased_Context_Augmentation_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Diffusion-based Generation, Optimization, and Planning in 3D Scenes | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://scenediffuser.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/scenediffuser/Scene-Diffuser?style=flat)](https://github.com/scenediffuser/Scene-Diffuser) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Diffusion-Based_Generation_Optimization_and_Planning_in_3D_Scenes_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.06015-b31b1b.svg)](https://arxiv.org/abs/2301.06015) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=a0SSprBoVV4) |
| TopNet: Transformer-based Object Placement Network for Image Compositing | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_TopNet_Transformer-Based_Object_Placement_Network_for_Image_Compositing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03372-b31b1b.svg)](https://arxiv.org/abs/2304.03372) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KbcnGFeg-V8) |
| Computational Flash Photography through Intrinsics | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yaksoy.github.io/intrinsicFlash/) <br /> [![GitHub](https://img.shields.io/github/stars/compphoto/IntrinsicFlashPhotography?style=flat)](https://github.com/compphoto/IntrinsicFlashPhotography) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Maralan_Computational_Flash_Photography_Through_Intrinsics_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.06089-b31b1b.svg)](https://arxiv.org/abs/2306.06089) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Zs23PKgJCO8) |
| Probing Neural Representations of Scene Perception in a Hippocampally Dependent Task using Artificial Neural Networks | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Frey_Probing_Neural_Representations_of_Scene_Perception_in_a_Hippocampally_Dependent_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06367-b31b1b.svg)](https://arxiv.org/abs/2303.06367) | :heavy_minus_sign: |
| DeepSolo: Let Transformer Decoder with Explicit Points Solo for Text Spotting | [![GitHub](https://img.shields.io/github/stars/ViTAE-Transformer/DeepSolo?style=flat)](https://github.com/ViTAE-Transformer/DeepSolo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_DeepSolo_Let_Transformer_Decoder_With_Explicit_Points_Solo_for_Text_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.10772-b31b1b.svg)](https://arxiv.org/abs/2211.10772) | :heavy_minus_sign: |
| LEGO-Net: Learning Regular Rearrangements of Objects in Rooms | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://ivl.cs.brown.edu/research/lego-net.html) <br /> [![GitHub](https://img.shields.io/github/stars/QiuhongAnnaWei/LEGO-Net?style=flat)](https://github.com/QiuhongAnnaWei/LEGO-Net) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_LEGO-Net_Learning_Regular_Rearrangements_of_Objects_in_Rooms_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.09629-b31b1b.svg)](https://arxiv.org/abs/2301.09629) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Wzy7wdyc4cI) |
| Open-Vocabulary Point-Cloud Object Detection without 3D Annotation | [![GitHub](https://img.shields.io/github/stars/lyhdet/OV-3DET?style=flat)](https://github.com/lyhdet/OV-3DET) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00788-b31b1b.svg)](https://arxiv.org/abs/2304.00788) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-rUBCX4sHII) |
| Weakly-Supervised Domain Adaptive Semantic Segmentation with Prototypical Contrastive Learning | [![GitHub](https://img.shields.io/github/stars/anurag-198/WDASS?style=flat)](https://github.com/anurag-198/WDASS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Das_Weakly-Supervised_Domain_Adaptive_Semantic_Segmentation_With_Prototypical_Contrastive_Learning_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Arg8p0Zrf9A) |
| ScanDMM: A Deep Markov Model of Scanpath Prediction for 360&deg; Images | [![GitHub](https://img.shields.io/github/stars/xiangjieSui/ScanDMM?style=flat)](https://github.com/xiangjieSui/ScanDMM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Sui_ScanDMM_A_Deep_Markov_Model_of_Scanpath_Prediction_for_360deg_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=noXCcFvXY2k) |
| Canonical Fields: Self-Supervised Learning of Pose-Canonicalized Neural Fields <br/> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://ivl.cs.brown.edu/research/canonical-fields.html) <br /> [![GitHub](https://img.shields.io/github/stars/brown-ivl/Cafi-Net?style=flat)](https://github.com/brown-ivl/Cafi-Net) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Agaram_Canonical_Fields_Self-Supervised_Learning_of_Pose-Canonicalized_Neural_Fields_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.02493-b31b1b.svg)](https://arxiv.org/abs/2212.02493) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vKHbky6Wcxk) |
| TempSAL - Uncovering Temporal Information for Deep Saliency Prediction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ivrl.github.io/Tempsal/) <br /> [![GitHub](https://img.shields.io/github/stars/IVRL/Tempsal?style=flat)](https://github.com/IVRL/Tempsal) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Aydemir_TempSAL_-_Uncovering_Temporal_Information_for_Deep_Saliency_Prediction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.02315-b31b1b.svg)](https://arxiv.org/abs/2301.02315) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=1CrgRjzfjFQ) |
| Probabilistic Debiasing of Scene Graphs | [![GitHub](https://img.shields.io/github/stars/bashirulazam/within-triplet-debias?style=flat)](https://github.com/bashirulazam/within-triplet-debias) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Biswas_Probabilistic_Debiasing_of_Scene_Graphs_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.06444-b31b1b.svg)](https://arxiv.org/abs/2211.06444) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=4tDAW0D1YE4) |
| Towards Unified Scene Text Spotting based on Sequence Generation | [![GitHub](https://img.shields.io/github/stars/clovaai/units?style=flat)](https://github.com/clovaai/units) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kil_Towards_Unified_Scene_Text_Spotting_Based_on_Sequence_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03435-b31b1b.svg)](https://arxiv.org/abs/2304.03435) | :heavy_minus_sign: |
| Learning to Generate Language-Supervised and Open-Vocabulary Scene Graph using Pre-trained Visual-Semantic Space | [![GitHub](https://img.shields.io/github/stars/zyong812/VS3_CVPR23?style=flat)](https://github.com/zyong812/VS3_CVPR23) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_To_Generate_Language-Supervised_and_Open-Vocabulary_Scene_Graph_Using_Pre-Trained_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jWTxcHC6ccQ) |
| Modular Memorability: Tiered Representations for Video Memorability Prediction | [![GitHub](https://img.shields.io/github/stars/theodumont/modular-memorability?style=flat)](https://github.com/theodumont/modular-memorability) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Dumont_Modular_Memorability_Tiered_Representations_for_Video_Memorability_Prediction_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=1ysNNMHQb1o) |
| Where we are and what we're Looking at: Query based Worldwide Image Geo-Localization using Hierarchies and Scenes | [![GitHub](https://img.shields.io/github/stars/AHKerrigan/GeoGuessNet?style=flat)](https://github.com/AHKerrigan/GeoGuessNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Clark_Where_We_Are_and_What_Were_Looking_At_Query_Based_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.04249-b31b1b.svg)](https://arxiv.org/abs/2303.04249) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=fp3hZGbwPqk) |
| HRDFuse: Monocular 360&deg; Depth Estimation by Collaboratively Learning Holistic-with-Regional Depth Distributions | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://vlis2022.github.io/HRDFuse/) <br /> [![GitHub](https://img.shields.io/github/stars/haoai-1997/HRDFuse?style=flat)](https://github.com/haoai-1997/HRDFuse) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ai_HRDFuse_Monocular_360deg_Depth_Estimation_by_Collaboratively_Learning_Holistic-With-Regional_Depth_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11616-b31b1b.svg)](https://arxiv.org/abs/2303.11616) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Qgxdar_MdIc) |
