# CVPR-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
    <tr>
        <td><strong>New collections</strong></td>
        <td>
            <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README.md">
                <img src="http://img.shields.io/badge/CVPR-2024-0073AE.svg" alt="Conference">
            </a>
        </td>
    </tr>
</table>

<div align="center">
  <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/sections/2023/main/transparency-fairness-accountability-privacy-ethics-in-vision.md">
    <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
  </a>
  <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README_2023.md">
    <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
  </a>
  <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/sections/2023/main/embodied-vision-active-agents-simulation.md">
    <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
  </a>
</div>

## Explainable AI for CV

![Section Papers](https://img.shields.io/badge/Section%20Papers-24-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-21-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-18-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-19-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Are Data-Driven Explanations Robust Against Out-of-Distribution Data? | [![GitHub](https://img.shields.io/github/stars/tangli-udel/DRE?style=flat)](https://github.com/tangli-udel/DRE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Are_Data-Driven_Explanations_Robust_Against_Out-of-Distribution_Data_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16390-b31b1b.svg)](http://arxiv.org/abs/2303.16390) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=logVgiC4x54) |
| Uncertainty-Aware Unsupervised Image Deblurring with Deep Residual Prior | [![GitHub](https://img.shields.io/github/stars/xl-tang3/UAUDeblur?style=flat)](https://github.com/xl-tang3/UAUDeblur) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Uncertainty-Aware_Unsupervised_Image_Deblurring_With_Deep_Residual_Prior_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.05361-b31b1b.svg)](http://arxiv.org/abs/2210.05361) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9ZfN5Jt7vVA) |
| Teaching Matters: Investigating the Role of Supervision in Vision Transformers | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](http://www.cs.umd.edu/~sakshams/vit_analysis/) <br /> [![GitHub](https://img.shields.io/github/stars/mwalmer-umd/vit_analysis?style=flat)](https://github.com/mwalmer-umd/vit_analysis) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Walmer_Teaching_Matters_Investigating_the_Role_of_Supervision_in_Vision_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03862-b31b1b.svg)](http://arxiv.org/abs/2212.03862) | :heavy_minus_sign: |
| Adversarial Counterfactual Visual Explanations | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://guillaumejs2403.github.io/projects/ace.html) <br /> [![GitHub](https://img.shields.io/github/stars/guillaumejs2403/ACE?style=flat)](https://github.com/guillaumejs2403/ACE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jeanneret_Adversarial_Counterfactual_Visual_Explanations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09962-b31b1b.svg)](http://arxiv.org/abs/2303.09962) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ykTUSSTZOME) |
| SketchXAI: A First Look at Explainability for Human Sketches | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sketchxai.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/WinKawaks/SketchXAI?style=flat)](https://github.com/WinKawaks/SketchXAI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_SketchXAI_A_First_Look_at_Explainability_for_Human_Sketches_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.11744-b31b1b.svg)](http://arxiv.org/abs/2304.11744) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=RcY3NJlTGyE) |
| Doubly Right Object Recognition: A why Prompt for Visual Rationales | [![GitHub](https://img.shields.io/github/stars/cvlab-columbia/DoubleRight?style=flat)](https://github.com/cvlab-columbia/DoubleRight) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Mao_Doubly_Right_Object_Recognition_A_Why_Prompt_for_Visual_Rationales_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.06202-b31b1b.svg)](http://arxiv.org/abs/2212.06202) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kp47I79-o38) |
| Overlooked Factors in Concept-based Explanations: Dataset Choice, Concept Learnability, and Human Capability | [![GitHub](https://img.shields.io/github/stars/princetonvisualai/OverlookedFactors?style=flat)](https://github.com/princetonvisualai/OverlookedFactors) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ramaswamy_Overlooked_Factors_in_Concept-Based_Explanations_Dataset_Choice_Concept_Learnability_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2207.09615-b31b1b.svg)](http://arxiv.org/abs/2207.09615) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LnvyAmNW918) |
| Initialization Noise in Image Gradients and Saliency Maps | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.visualcomputing.informatik.uni-mainz.de/initialization-noise-in-image-gradients-and-saliency-maps/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Woerl_Initialization_Noise_in_Image_Gradients_and_Saliency_Maps_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=4qshf2hZiis) |
| Learning Bottleneck Concepts in Image Classification | [![GitHub](https://img.shields.io/github/stars/wbw520/BotCL?style=flat)](https://github.com/wbw520/BotCL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Learning_Bottleneck_Concepts_in_Image_Classification_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10131-b31b1b.svg)](http://arxiv.org/abs/2304.10131) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=W8Lsas0FJ6w) |
| Zero-Shot Model Diagnosis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zero-shot-model-diagnosis.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/humansensinglab/ZOOM?style=flat)](https://github.com/humansensinglab/ZOOM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Zero-Shot_Model_Diagnosis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15441-b31b1b.svg)](http://arxiv.org/abs/2303.15441) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mmiR1UxKbPg) |
| OCTET: Object-Aware Counterfactual Explanations | [![GitHub](https://img.shields.io/github/stars/valeoai/OCTET?style=flat)](https://github.com/valeoai/OCTET) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zemni_OCTET_Object-Aware_Counterfactual_Explanations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12380-b31b1b.svg)](http://arxiv.org/abs/2211.12380) | :heavy_minus_sign: |
| X-Pruner: eXplainable Pruning for Vision Transformers | [![GitHub](https://img.shields.io/github/stars/vickyyu90/XPruner?style=flat)](https://github.com/vickyyu90/XPruner) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_X-Pruner_eXplainable_Pruning_for_Vision_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.04935-b31b1b.svg)](https://arxiv.org/abs/2303.04935) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=IWzC3tBL-Fo) |
| Don't Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Fel_Dont_Lie_to_Me_Robust_and_Efficient_Explainability_With_Verified_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2202.07728-b31b1b.svg)](https://arxiv.org/abs/2202.07728) | :heavy_minus_sign: |
| CRAFT: Concept Recursive Activation FacTorization for Explainability | [![GitHub](https://img.shields.io/github/stars/deel-ai/Craft?style=flat)](https://github.com/deel-ai/Craft) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Fel_CRAFT_Concept_Recursive_Activation_FacTorization_for_Explainability_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.10154-b31b1b.svg)](https://arxiv.org/abs/2211.10154) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=z12IffeD7yw) |
| Grounding Counterfactual Explanation of Image Classifiers to Textual Concept Space | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Grounding_Counterfactual_Explanation_of_Image_Classifiers_to_Textual_Concept_Space_CVPR_2023_paper.pdf) <br /> [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/grounding-counterfactual-explanation-of-image-classifier-to-textual-concept-space) | :heavy_minus_sign: |
| Explaining Image Classifiers with Multiscale Directional Image Representation | [![GitHub](https://img.shields.io/github/stars/skmda37/ShearletX?style=flat)](https://github.com/skmda37/ShearletX) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kolek_Explaining_Image_Classifiers_With_Multiscale_Directional_Image_Representation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12857-b31b1b.svg)](https://arxiv.org/abs/2211.12857) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=524MaHHewoo) |
| IDGI: A Framework to Eliminate Explanation Noise from Integrated Gradients | [![GitHub](https://img.shields.io/github/stars/yangruo1226/idgi?style=flat)](https://github.com/yangruo1226/idgi) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_IDGI_A_Framework_To_Eliminate_Explanation_Noise_From_Integrated_Gradients_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14242-b31b1b.svg)](http://arxiv.org/abs/2303.14242) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3L7wbMdOQiY) |
| Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification | [![GitHub](https://img.shields.io/github/stars/YueYANG1996/LaBo?style=flat)](https://github.com/YueYANG1996/LaBo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Language_in_a_Bottle_Language_Model_Guided_Concept_Bottlenecks_for_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11158-b31b1b.svg)](http://arxiv.org/abs/2211.11158) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nOPy4BBm4Tw) |
| Gradient-based Uncertainty Attribution for Explainable Bayesian Deep Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Gradient-Based_Uncertainty_Attribution_for_Explainable_Bayesian_Deep_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04824-b31b1b.svg)](https://arxiv.org/abs/2304.04824) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lvJ_grAfwFw) |
| PIP-Net: Patch-based Intuitive Prototypes for Interpretable Image Classification | [![GitHub](https://img.shields.io/github/stars/M-Nauta/PIPNet?style=flat)](https://github.com/M-Nauta/PIPNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Nauta_PIP-Net_Patch-Based_Intuitive_Prototypes_for_Interpretable_Image_Classification_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=GfQQFQ62SLU) |
| Shortcomings of Top-Down Randomization-based Sanity Checks for Evaluations of Deep Neural Network Explanations | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Binder_Shortcomings_of_Top-Down_Randomization-Based_Sanity_Checks_for_Evaluations_of_Deep_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12486-b31b1b.svg)](http://arxiv.org/abs/2211.12486) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JKBQKBlSEJM) |
| Spatial-Temporal Concept based Explanation of 3D ConvNets | [![GitHub](https://img.shields.io/github/stars/yingji425/STCE?style=flat)](https://github.com/yingji425/STCE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Spatial-Temporal_Concept_Based_Explanation_of_3D_ConvNets_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.05275-b31b1b.svg)](http://arxiv.org/abs/2206.05275) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-53laIT9ytM) |
| A Practical Upper Bound for the Worst-Case Attribution Deviations | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_A_Practical_Upper_Bound_for_the_Worst-Case_Attribution_Deviations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.00340-b31b1b.svg)](http://arxiv.org/abs/2303.00340) | :heavy_minus_sign: |
| Adversarial Normalization: I Can Visualize Everything (ICE) | [![GitHub](https://img.shields.io/github/stars/Hanyang-HCC-Lab/ICE?style=flat)](https://github.com/Hanyang-HCC-Lab/ICE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Adversarial_Normalization_I_Can_Visualize_Everything_ICE_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZeZYWmrHqIw) |
