# CVPR-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/2023/main/3d-from-multi-view-and-sensors.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/2023/main/humans-face-body-pose-gesture-movement.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Image and Video Synthesis and Generation

![Section Papers](https://img.shields.io/badge/Section%20Papers-186-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-159-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-135-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-142-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Towards Universal Fake Image Detectors That Generalize Across Generative Models | [![GitHub](https://img.shields.io/github/stars/Yuheng-Li/UniversalFakeDetect?style=flat)](https://github.com/Yuheng-Li/UniversalFakeDetect) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ojha_Towards_Universal_Fake_Image_Detectors_That_Generalize_Across_Generative_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.10174-b31b1b.svg)](http://arxiv.org/abs/2302.10174)|:heavy_minus_sign: |
| Implicit Diffusion Models for Continuous Super-Resolution | [![GitHub](https://img.shields.io/github/stars/Ree1s/IDM?style=flat)](https://github.com/Ree1s/IDM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Implicit_Diffusion_Models_for_Continuous_Super-Resolution_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16491-b31b1b.svg)](http://arxiv.org/abs/2303.16491)| :heavy_minus_sign: |
| High-Fidelity Guided Image Synthesis With Latent Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://1jsingh.github.io/gradop) <br /> [![GitHub](https://img.shields.io/github/stars/1jsingh/GradOP-Guided-Image-Synthesis?style=flat)](https://github.com/1jsingh/GradOP-Guided-Image-Synthesis) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Singh_High-Fidelity_Guided_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.17084-b31b1b.svg)](http://arxiv.org/abs/2211.17084) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Yk83RPCOa2o) |
| DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields | [![GitHub](https://img.shields.io/github/stars/AIBluefisher/dbarf?style=flat)](https://github.com/AIBluefisher/dbarf) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_DBARF_Deep_Bundle-Adjusting_Generalizable_Neural_Radiance_Fields_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14478-b31b1b.svg)](http://arxiv.org/abs/2303.14478) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wFPO403wtAg) |
| Deep Arbitrary-Scale Image Super-Resolution via Scale-Equivariance Pursuit | [![GitHub](https://img.shields.io/github/stars/neuralchen/EQSR?style=flat)](https://github.com/neuralchen/EQSR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Deep_Arbitrary-Scale_Image_Super-Resolution_via_Scale-Equivariance_Pursuit_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Pq9eI5kxqUE) |
| Balanced Spherical Grid for Egocentric View Synthesis | [![GitHub](https://img.shields.io/github/stars/changwoonchoi/EgoNeRF?style=flat)](https://github.com/changwoonchoi/EgoNeRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Balanced_Spherical_Grid_for_Egocentric_View_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12408-b31b1b.svg)](http://arxiv.org/abs/2303.12408) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=D-lsBhVP8zw) |
| SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation | [![GitHub](https://img.shields.io/github/stars/yccyenchicheng/SDFusion?style=flat)](https://github.com/yccyenchicheng/SDFusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_SDFusion_Multimodal_3D_Shape_Completion_Reconstruction_and_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04493-b31b1b.svg)](http://arxiv.org/abs/2212.04493) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6EvHJRlUMFQ) |
| DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation <br /> ![CVPR - Award](https://img.shields.io/badge/CVPR-Award-294A7C) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dreambooth.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.12242-b31b1b.svg)](http://arxiv.org/abs/2208.12242) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=82x4XTSFwBQ) |
| Self-Guided Diffusion Models | [![GitHub](https://img.shields.io/github/stars/dongzhuoyao/self-guided-diffusion-models?style=flat)](https://github.com/dongzhuoyao/self-guided-diffusion-models) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Self-Guided_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.06462-b31b1b.svg)](http://arxiv.org/abs/2210.06462) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zwkn640t-u8) |
| Multi-Concept Customization of Text-to-Image Diffusion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://www.cs.cmu.edu/~custom-diffusion/dataset.html) <br /> [![GitHub](https://img.shields.io/github/stars/adobe-research/custom-diffusion?style=flat)](https://github.com/adobe-research/custom-diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04488-b31b1b.svg)](http://arxiv.org/abs/2212.04488) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WWNA_IPLO84) |
| 3D-Aware Conditional Image Synthesis | [![GitHub](https://img.shields.io/github/stars/dunbar12138/pix2pix3D?style=flat)](https://github.com/dunbar12138/pix2pix3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Deng_3D-Aware_Conditional_Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.08509-b31b1b.svg)](http://arxiv.org/abs/2302.08509) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dDRoI1gjbzk) |
| QuantArt: Quantizing Image Style Transfer Towards High Visual Fidelity | [![GitHub](https://img.shields.io/github/stars/siyuhuang/QuantArt?style=flat)](https://github.com/siyuhuang/QuantArt) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_QuantArt_Quantizing_Image_Style_Transfer_Towards_High_Visual_Fidelity_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.10431-b31b1b.svg)](http://arxiv.org/abs/2212.10431) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WWNA_IPLO84) |
| SceneComposer: Any-Level Semantic Image Synthesis <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zengyu.me/scenec/) <br /> [![GitHub](https://img.shields.io/github/stars/zengxianyu/scenec?style=flat)](https://github.com/zengxianyu/scenec) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_SceneComposer_Any-Level_Semantic_Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11742-b31b1b.svg)](http://arxiv.org/abs/2211.11742) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ky0LWZ_USRA) |
| DiffCollage: Parallel Generation of Large Content With Diffusion Models | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://research.nvidia.com/labs/dir/diffcollage/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_DiffCollage_Parallel_Generation_of_Large_Content_With_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17076-b31b1b.svg)](http://arxiv.org/abs/2303.17076) | :heavy_minus_sign: |
| Putting People in Their Place: Affordance-Aware Human Insertion Into Scenes | [![GitHub](https://img.shields.io/github/stars/adobe-research/affordance-insertion?style=flat)](https://github.com/adobe-research/affordance-insertion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kulal_Putting_People_in_Their_Place_Affordance-Aware_Human_Insertion_Into_Scenes_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.14406-b31b1b.svg)](http://arxiv.org/abs/2304.14406) | :heavy_minus_sign: |
| Hybrid Neural Rendering for Large-Scale Scenes With Motion Blur | [![GitHub](https://img.shields.io/github/stars/CVMI-Lab/HybridNeuralRendering?style=flat)](https://github.com/CVMI-Lab/HybridNeuralRendering) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Dai_Hybrid_Neural_Rendering_for_Large-Scale_Scenes_With_Motion_Blur_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.12652-b31b1b.svg)](http://arxiv.org/abs/2304.12652) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hAhFfKRqDgE) |
| Binary Latent Diffusion | [![GitHub](https://img.shields.io/github/stars/ZeWang95/BinaryLatentDiffusion?style=flat)](https://github.com/ZeWang95/BinaryLatentDiffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Binary_Latent_Diffusion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04820-b31b1b.svg)](http://arxiv.org/abs/2304.04820) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/channel/UCzkUNNsV1TYuf6U_wGnMlnw) |
| StyleRes: Transforming the Residuals for Real Image Editing With StyleGAN | [![GitHub](https://img.shields.io/github/stars/hamzapehlivan/StyleRes?style=flat)](https://github.com/hamzapehlivan/StyleRes) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Pehlivan_StyleRes_Transforming_the_Residuals_for_Real_Image_Editing_With_StyleGAN_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.14359-b31b1b.svg)](http://arxiv.org/abs/2212.14359) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=S9ZswKv8enw) |
| KD-DLGAN: Data Limited Image Generation via Knowledge Distillation | [![GitHub](https://img.shields.io/github/stars/cuikaiwen18/KD_DLGAN?style=flat)](https://github.com/cuikaiwen18/KD_DLGAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cui_KD-DLGAN_Data_Limited_Image_Generation_via_Knowledge_Distillation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17158-b31b1b.svg)](https://arxiv.org/abs/2303.17158) | :heavy_minus_sign: |
| SeaThru-NeRF: Neural Radiance Fields in Scattering Media | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sea-thru-nerf.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/deborahLevy130/seathru_NeRF?style=flat)](https://github.com/deborahLevy130/seathru_NeRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Levy_SeaThru-NeRF_Neural_Radiance_Fields_in_Scattering_Media_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.07743-b31b1b.svg)](https://arxiv.org/abs/2304.07743) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dXKCJS4cscg) |
| PointAvatar: Deformable Point-Based Head Avatars From Videos | [![GitHub](https://img.shields.io/github/stars/zhengyuf/PointAvatar?style=flat)](https://github.com/zhengyuf/PointAvatar) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_PointAvatar_Deformable_Point-Based_Head_Avatars_From_Videos_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08377-b31b1b.svg)](http://arxiv.org/abs/2212.08377) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wll_XtgpU7U) |
| 3DAvatarGAN: Bridging Domains for Personalized Editable Avatars | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rameenabdal.github.io/3DAvatarGAN/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Abdal_3DAvatarGAN_Bridging_Domains_for_Personalized_Editable_Avatars_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.02700-b31b1b.svg)](http://arxiv.org/abs/2301.02700) | :heavy_minus_sign: |
| Neural Preset for Color Style Transfer | [![GitHub](https://img.shields.io/github/stars/ZHKKKe/NeuralPreset?style=flat)](https://github.com/ZHKKKe/NeuralPreset) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ke_Neural_Preset_for_Color_Style_Transfer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13511-b31b1b.svg)](http://arxiv.org/abs/2303.13511) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=x6fLAvTPesk) |
| Zero-Shot Generative Model Adaptation via Image-Specific Prompt Learning | [![GitHub](https://img.shields.io/github/stars/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation?style=flat)](https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Zero-Shot_Generative_Model_Adaptation_via_Image-Specific_Prompt_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03119-b31b1b.svg)](http://arxiv.org/abs/2304.03119) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vw9-C3Sz5nM) |
| DyNCA: Real-Time Dynamic Texture Synthesis Using Neural Cellular Automata | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dynca.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/IVRL/DyNCA?style=flat)](https://github.com/IVRL/DyNCA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Pajouheshgar_DyNCA_Real-Time_Dynamic_Texture_Synthesis_Using_Neural_Cellular_Automata_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11417-b31b1b.svg)](http://arxiv.org/abs/2211.11417) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ELZC2mX5Z9U) |
| Exploring Incompatible Knowledge Transfer in Few-Shot Image Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yunqing-me.github.io/RICK/) <br /> [![GitHub](https://img.shields.io/github/stars/yunqing-me/RICK?style=flat)](https://github.com/yunqing-me/RICK) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Exploring_Incompatible_Knowledge_Transfer_in_Few-Shot_Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.07574-b31b1b.svg)](http://arxiv.org/abs/2304.07574) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=s14bA8filtw) |
| HouseDiffusion: Vector Floorplan Generation via a Diffusion Model With Discrete and Continuous Denoising | [![GitHub](https://img.shields.io/github/stars/aminshabani/house_diffusion?style=flat)](https://github.com/aminshabani/house_diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Shabani_HouseDiffusion_Vector_Floorplan_Generation_via_a_Diffusion_Model_With_Discrete_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13287-b31b1b.svg)](http://arxiv.org/abs/2211.13287) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ku6_gr94n5Q) |
| Towards Accurate Image Coding: Improved Autoregressive Image Generation With Dynamic Vector Quantization <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/CrossmodalGroup/DynamicVectorQuantization?style=flat)](https://github.com/CrossmodalGroup/DynamicVectorQuantization) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Towards_Accurate_Image_Coding_Improved_Autoregressive_Image_Generation_With_Dynamic_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.11718-b31b1b.svg)](http://arxiv.org/abs/2305.11718) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ir60YW9JCjU) |
| RiDDLE: Reversible and Diversified De-Identification With Latent Encryptor | [![GitHub](https://img.shields.io/github/stars/ldz666666/RiDDLE?style=flat)](https://github.com/ldz666666/RiDDLE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_RiDDLE_Reversible_and_Diversified_De-Identification_With_Latent_Encryptor_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05171-b31b1b.svg)](http://arxiv.org/abs/2303.05171) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=gqs9Q6ReEn0) |
| LayoutDiffusion: Controllable Diffusion Model for Layout-to-Image Generation | [![GitHub](https://img.shields.io/github/stars/ZGCTroy/LayoutDiffusion?style=flat)](https://github.com/ZGCTroy/LayoutDiffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_LayoutDiffusion_Controllable_Diffusion_Model_for_Layout-to-Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17189-b31b1b.svg)](http://arxiv.org/abs/2303.17189) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bJOpJnvhw3s) |
| LipFormer: High-Fidelity and Generalizable Talking Face Generation With a Pre-Learned Facial Codebook | [![GitHub](https://img.shields.io/github/stars/DaddyJin/awesome-faceReenactment?style=flat)](https://github.com/DaddyJin/awesome-faceReenactment) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_LipFormer_High-Fidelity_and_Generalizable_Talking_Face_Generation_With_a_Pre-Learned_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Not All Image Regions Matter: Masked Vector Quantization for Autoregressive Image Generation | [![GitHub](https://img.shields.io/github/stars/CrossmodalGroup/MaskedVectorQuantization?style=flat)](https://github.com/CrossmodalGroup/MaskedVectorQuantization) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Not_All_Image_Regions_Matter_Masked_Vector_Quantization_for_Autoregressive_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.13607-b31b1b.svg)](http://arxiv.org/abs/2305.13607) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=o2eyRscEejw) |
| GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis | [![GitHub](https://img.shields.io/github/stars/tobran/GALIP?style=flat)](https://github.com/tobran/GALIP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tao_GALIP_Generative_Adversarial_CLIPs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.12959-b31b1b.svg)](http://arxiv.org/abs/2301.12959) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lwhTDY4du_g) |
| High-Fidelity Generalized Emotional Talking Face Generation With Multi-Modal Emotion Space Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_High-Fidelity_Generalized_Emotional_Talking_Face_Generation_With_Multi-Modal_Emotion_Space_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.02572-b31b1b.svg)](http://arxiv.org/abs/2305.02572) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=eQG6ql83T0w) |
| Consistent View Synthesis With Pose-Guided Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://poseguided-diffusion.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tseng_Consistent_View_Synthesis_With_Pose-Guided_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17598-b31b1b.svg)](http://arxiv.org/abs/2303.17598) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://youtu.be/eV1jwq14lE0) |
| StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-Based Generator | [![GitHub](https://img.shields.io/github/stars/guanjz20/StyleSync?style=flat)](https://github.com/guanjz20/StyleSync) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Guan_StyleSync_High-Fidelity_Generalized_and_Personalized_Lip_Sync_in_Style-Based_Generator_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.05445-b31b1b.svg)](http://arxiv.org/abs/2305.05445) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=yAPDl2dVonY) |
| Imagic: Text-Based Real Image Editing With Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://poseguided-diffusion.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kawar_Imagic_Text-Based_Real_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.09276-b31b1b.svg)](http://arxiv.org/abs/2210.09276) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lOZvBGz47wQ) |
| Large-Capacity and Flexible Video Steganography via Invertible Neural Network | [![GitHub](https://img.shields.io/github/stars/MC-E/LF-VSN?style=flat)](https://github.com/MC-E/LF-VSN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Mou_Large-Capacity_and_Flexible_Video_Steganography_via_Invertible_Neural_Network_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.12300-b31b1b.svg)](http://arxiv.org/abs/2304.12300) | :heavy_minus_sign: |
| Quantitative Manipulation of Custom Attributes on 3D-Aware Image Synthesis | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Do_Quantitative_Manipulation_of_Custom_Attributes_on_3D-Aware_Image_Synthesis_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent Portrait Synthesis From Monocular Image | [![GitHub](https://img.shields.io/github/stars/YuDeng/GRAMInverter?style=flat)](https://github.com/YuDeng/GRAMInverter) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Deng_Learning_Detailed_Radiance_Manifolds_for_High-Fidelity_and_3D-Consistent_Portrait_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13901-b31b1b.svg)](http://arxiv.org/abs/2211.13901) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nclBOg_CiJo) |
| CF-Font: Content Fusion for Few-Shot Font Generation | [![GitHub](https://img.shields.io/github/stars/wangchi95/CF-Font?style=flat)](https://github.com/wangchi95/CF-Font) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_CF-Font_Content_Fusion_for_Few-Shot_Font_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14017-b31b1b.svg)](https://arxiv.org/abs/2303.14017) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=biwFd0K3X9o) |
| One-Shot High-Fidelity Talking-Head Synthesis With Deformable Neural Radiance Field | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.waytron.net/hidenerf/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_One-Shot_High-Fidelity_Talking-Head_Synthesis_With_Deformable_Neural_Radiance_Field_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05097-b31b1b.svg)](http://arxiv.org/abs/2304.05097) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=opLdLY8_VYQ) |
| Unsupervised Domain Adaption With Pixel-Level Discriminator for Image-Aware Layout Generation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Unsupervised_Domain_Adaption_With_Pixel-Level_Discriminator_for_Image-Aware_Layout_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14377-b31b1b.svg)](http://arxiv.org/abs/2303.14377) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DBHFzw02T1I) |
| Diffusion Probabilistic Model Made Slim | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Diffusion_Probabilistic_Model_Made_Slim_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.17106-b31b1b.svg)](http://arxiv.org/abs/2211.17106) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=82N6FsRUfr4) |
| Collaborative Diffusion for Multi-Modal Face Generation and Editing | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ziqihuangg.github.io/projects/collaborative-diffusion.html) <br /> [![GitHub](https://img.shields.io/github/stars/ziqihuangg/Collaborative-Diffusion?style=flat)](https://github.com/ziqihuangg/Collaborative-Diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Collaborative_Diffusion_for_Multi-Modal_Face_Generation_and_Editing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10530-b31b1b.svg)](http://arxiv.org/abs/2304.10530)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=inLK4c8sNhc) |
| High-Fidelity Facial Avatar Reconstruction From Monocular Video With Generative Priors | [![GitHub](https://img.shields.io/github/stars/bbaaii/HFA-GP?style=flat)](https://github.com/bbaaii/HFA-GP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_High-Fidelity_Facial_Avatar_Reconstruction_From_Monocular_Video_With_Generative_Priors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.15064-b31b1b.svg)](http://arxiv.org/abs/2211.15064)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZWdF8ASl0BQ) |
| Network-Free, Unsupervised Semantic Segmentation With Synthetic Images | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Network-Free_Unsupervised_Semantic_Segmentation_With_Synthetic_Images_CVPR_2023_paper.pdf) <br /> [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/network-free-unsupervised-semantic-segmentation-with-synthetic-images) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mfFZdvaF1Tw) |
| Visual Prompt Tuning for Generative Transfer Learning | [![GitHub](https://img.shields.io/github/stars/google-research/generative_transfer?style=flat)](https://github.com/google-research/generative_transfer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Sohn_Visual_Prompt_Tuning_for_Generative_Transfer_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.00990-b31b1b.svg)](http://arxiv.org/abs/2210.00990) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kOza8-xop_g) |
| Specialist Diffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models To Learn Any Unseen Style | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://specialist-diffusion.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/Picsart-AI-Research/Specialist-Diffusion?style=flat)](https://github.com/Picsart-AI-Research/Specialist-Diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Specialist_Diffusion_Plug-and-Play_Sample-Efficient_Fine-Tuning_of_Text-to-Image_Diffusion_Models_To_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5-hkImpVsNI) |
| Catch Missing Details: Image Reconstruction With Frequency Augmented Variational Autoencoder | [![GitHub](https://img.shields.io/github/stars/JiauZhang/FA-VAE?style=flat)](https://github.com/JiauZhang/FA-VAE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Catch_Missing_Details_Image_Reconstruction_With_Frequency_Augmented_Variational_Autoencoder_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.02541-b31b1b.svg)](http://arxiv.org/abs/2305.02541) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wJ9U7tAnQEo) |
| Towards Bridging the Performance Gaps of Joint Energy-Based Models | [![GitHub](https://img.shields.io/github/stars/sndnyang/sadajem?style=flat)](https://github.com/sndnyang/sadajem) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Towards_Bridging_the_Performance_Gaps_of_Joint_Energy-Based_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.07959-b31b1b.svg)](http://arxiv.org/abs/2209.07959) | :heavy_minus_sign: |
| GLeaD: Improving GANs With a Generator-Leading Task | [![GitHub](https://img.shields.io/github/stars/EzioBy/glead?style=flat)](https://github.com/EzioBy/glead) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_GLeaD_Improving_GANs_With_a_Generator-Leading_Task_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03752-b31b1b.svg)](http://arxiv.org/abs/2212.03752) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bP8Iq_qLuU0) |
| Structural Multiplane Image: Bridging Neural View Synthesis and 3D Reconstruction | [![GitHub](https://img.shields.io/github/stars/mf-zhang/Structural-MPI?style=flat)](https://github.com/mf-zhang/Structural-MPI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Structural_Multiplane_Image_Bridging_Neural_View_Synthesis_and_3D_Reconstruction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05937-b31b1b.svg)](http://arxiv.org/abs/2303.05937) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8Bbl8oZKAOs) |
| SPARF: Neural Radiance Fields From Sparse and Noisy Poses <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/google-research/sparf?style=flat)](https://github.com/google-research/sparf) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Truong_SPARF_Neural_Radiance_Fields_From_Sparse_and_Noisy_Poses_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11738-b31b1b.svg)](http://arxiv.org/abs/2211.11738) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_s3_p2Brd_8) |
| DeltaEdit: Exploring Text-Free Training for Text-Driven Image Manipulation | [![GitHub](https://img.shields.io/github/stars/Yueming6568/DeltaEdit?style=flat)](https://github.com/Yueming6568/DeltaEdit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lyu_DeltaEdit_Exploring_Text-Free_Training_for_Text-Driven_Image_Manipulation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06285-b31b1b.svg)](http://arxiv.org/abs/2303.06285) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8cdZSbhDMIA) |
| Inferring and Leveraging Parts From Object Shape for Improving Semantic Image Synthesis | [![GitHub](https://img.shields.io/github/stars/csyxwei/iPOSE?style=flat)](https://github.com/csyxwei/iPOSE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Inferring_and_Leveraging_Parts_From_Object_Shape_for_Improving_Semantic_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.19547-b31b1b.svg)](https://arxiv.org/abs/2305.19547) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=yVUmjQU9-v4) |
| VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation|:heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_VideoFusion_Decomposed_Diffusion_Models_for_High-Quality_Video_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08320-b31b1b.svg)](http://arxiv.org/abs/2303.08320) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hxA0DTZScg0) |
| MaskSketch: Unpaired Structure-Guided Masked Image Generation <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) |:heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Bashkirova_MaskSketch_Unpaired_Structure-Guided_Masked_Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.05496-b31b1b.svg)](http://arxiv.org/abs/2302.05496) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2tBzEGASeo0) |
| Affordance Diffusion: Synthesizing Hand-Object Interactions | [![GitHub](https://img.shields.io/github/stars/NVlabs/affordance_diffusion?style=flat)](https://github.com/NVlabs/affordance_diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_Affordance_Diffusion_Synthesizing_Hand-Object_Interactions_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12538-b31b1b.svg)](http://arxiv.org/abs/2303.12538) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=omhEoLzsopo) |
| Interactive Cartoonization With Controllable Perceptual Factors|:heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ahn_Interactive_Cartoonization_With_Controllable_Perceptual_Factors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09555-b31b1b.svg)](http://arxiv.org/abs/2212.09555) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=z8B2RiB4DyM) |
| MetaPortrait: Identity-Preserving Talking Head Generation With Fast Personalized Adaptation | [![GitHub](https://img.shields.io/github/stars/Meta-Portrait/MetaPortrait?style=flat)](https://github.com/Meta-Portrait/MetaPortrait) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_MetaPortrait_Identity-Preserving_Talking_Head_Generation_With_Fast_Personalized_Adaptation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08062-b31b1b.svg)](http://arxiv.org/abs/2212.08062) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DDEnjbCNNY4) |
| Paint by Example: Exemplar-Based Image Editing With Diffusion Models | [![GitHub](https://img.shields.io/github/stars/Fantasy-Studio/Paint-by-Example?style=flat)](https://github.com/Fantasy-Studio/Paint-by-Example) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Paint_by_Example_Exemplar-Based_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13227-b31b1b.svg)](http://arxiv.org/abs/2211.13227) | :heavy_minus_sign: |
| GLIGEN: Open-Set Grounded Text-to-Image Generation | [![GitHub](https://img.shields.io/github/stars/gligen/GLIGEN?style=flat)](https://github.com/gligen/GLIGEN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_GLIGEN_Open-Set_Grounded_Text-to-Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.07093-b31b1b.svg)](http://arxiv.org/abs/2301.07093) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-MCkU7IAGKs) |
| L-CoIns: Language-Based Colorization With Instance Awareness | [![GitHub](https://img.shields.io/github/stars/changzheng123/L-CoIns?style=flat)](https://github.com/changzheng123/L-CoIns) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_L-CoIns_Language-Based_Colorization_With_Instance_Awareness_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation | [![GitHub](https://img.shields.io/github/stars/sstzal/DiffTalk?style=flat)](https://github.com/sstzal/DiffTalk) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_DiffTalk_Crafting_Diffusion_Models_for_Generalized_Audio-Driven_Portraits_Animation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.03786-b31b1b.svg)](http://arxiv.org/abs/2301.03786) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tup5kbsOJXc) |
| Evading DeepFake Detectors via Adversarial Statistical Consistency | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hou_Evading_DeepFake_Detectors_via_Adversarial_Statistical_Consistency_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.11670-b31b1b.svg)](http://arxiv.org/abs/2304.11670) | :heavy_minus_sign: |
| GlassesGAN: Eyewear Personalization Using Synthetic Appearance Discovery and Targeted Subspace Modeling | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Plesh_GlassesGAN_Eyewear_Personalization_Using_Synthetic_Appearance_Discovery_and_Targeted_Subspace_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.14145-b31b1b.svg)](http://arxiv.org/abs/2210.14145) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=oMiV__LWV4A) |
| GP-VTON: Towards General Purpose Virtual Try-On via Collaborative Local-Flow Global-Parsing Learning | [![GitHub](https://img.shields.io/github/stars/xiezhy6/GP-VTON?style=flat)](https://github.com/xiezhy6/GP-VTON) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_GP-VTON_Towards_General_Purpose_Virtual_Try-On_via_Collaborative_Local-Flow_Global-Parsing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13756-b31b1b.svg)](https://arxiv.org/abs/2303.13756) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=b-FDMJ0jrw0) |
| Where Is My Spot? Few-Shot Image Generation via Latent Subspace Optimization | [![GitHub](https://img.shields.io/github/stars/chansey0529/LSO?style=flat)](https://github.com/chansey0529/LSO) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Where_Is_My_Spot_Few-Shot_Image_Generation_via_Latent_Subspace_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Regularized Vector Quantization for Tokenized Image Synthesis | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Regularized_Vector_Quantization_for_Tokenized_Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06424-b31b1b.svg)](http://arxiv.org/abs/2303.06424) | :heavy_minus_sign: |
| EDICT: Exact Diffusion Inversion via Coupled Transformations | [![GitHub](https://img.shields.io/github/stars/salesforce/EDICT?style=flat)](https://github.com/salesforce/EDICT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wallace_EDICT_Exact_Diffusion_Inversion_via_Coupled_Transformations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12446-b31b1b.svg)](http://arxiv.org/abs/2211.12446) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2k6DiE_h1eY) |
| Scaling Up GANs for Text-to-Image Synthesis <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mingukkang.github.io/GigaGAN/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Scaling_Up_GANs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05511-b31b1b.svg)](http://arxiv.org/abs/2303.05511) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZjxtuDQkOPY) |
| Shape-Aware Text-Driven Layered Video Editing | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://text-video-edit.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Shape-Aware_Text-Driven_Layered_Video_Editing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.13173-b31b1b.svg)](http://arxiv.org/abs/2301.13173) | :heavy_minus_sign: |
| A Unified Pyramid Recurrent Network for Video Frame Interpolation | [![GitHub](https://img.shields.io/github/stars/srcn-ivl/UPR-Net?style=flat)](https://github.com/srcn-ivl/UPR-Net) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_A_Unified_Pyramid_Recurrent_Network_for_Video_Frame_Interpolation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.03456-b31b1b.svg)](http://arxiv.org/abs/2211.03456) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=clvrjUKgfhI) |
| TAPS3D: Text-Guided 3D Textured Shape Generation From Pseudo Supervision | [![GitHub](https://img.shields.io/github/stars/plusmultiply/TAPS3D?style=flat)](https://github.com/plusmultiply/TAPS3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_TAPS3D_Text-Guided_3D_Textured_Shape_Generation_From_Pseudo_Supervision_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13273-b31b1b.svg)](http://arxiv.org/abs/2303.13273) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-eWBEwAkThA) |
| Fine-Grained Face Swapping via Regional GAN Inversion | [![GitHub](https://img.shields.io/github/stars/e4s2022/e4s?style=flat)](https://github.com/e4s2022/e4s) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Fine-Grained_Face_Swapping_via_Regional_GAN_Inversion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14068-b31b1b.svg)](http://arxiv.org/abs/2211.14068) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Z-cmKVeXHvY) |
| OTAvatar: One-Shot Talking Face Avatar With Controllable Tri-Plane Rendering | [![GitHub](https://img.shields.io/github/stars/theEricMa/OTAvatar?style=flat)](https://github.com/theEricMa/OTAvatar) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_OTAvatar_One-Shot_Talking_Face_Avatar_With_Controllable_Tri-Plane_Rendering_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14662-b31b1b.svg)](http://arxiv.org/abs/2303.14662) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qpIoMYFr7Aw) |
| Deep Stereo Video Inpainting | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Deep_Stereo_Video_Inpainting_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| StyleGAN Salon: Multi-View Latent Optimization for Pose-Invariant Hairstyle Transfer | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://stylegan-salon.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Khwanmuang_StyleGAN_Salon_Multi-View_Latent_Optimization_for_Pose-Invariant_Hairstyle_Transfer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02744-b31b1b.svg)](http://arxiv.org/abs/2304.02744) | :heavy_minus_sign: |
| Cross-GAN Auditing: Unsupervised Identification of Attribute Level Similarities and Differences Between Pretrained Generative Models | [![GitHub](https://img.shields.io/github/stars/mattolson93/cross_gan_auditing?style=flat)](https://github.com/mattolson93/cross_gan_auditing) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Olson_Cross-GAN_Auditing_Unsupervised_Identification_of_Attribute_Level_Similarities_and_Differences_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10774-b31b1b.svg)](http://arxiv.org/abs/2303.10774) | :heavy_minus_sign: |
| Unsupervised Volumetric Animation | [![GitHub](https://img.shields.io/github/stars/snap-research/unsupervised-volumetric-animation?style=flat)](https://github.com/snap-research/unsupervised-volumetric-animation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Siarohin_Unsupervised_Volumetric_Animation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.11326-b31b1b.svg)](http://arxiv.org/abs/2301.11326) | :heavy_minus_sign: |
| SINE: SINgle Image Editing With Text-to-Image Diffusion Models | [![GitHub](https://img.shields.io/github/stars/zhang-zx/SINE?style=flat)](https://github.com/zhang-zx/SINE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_SINE_SINgle_Image_Editing_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04489-b31b1b.svg)](http://arxiv.org/abs/2212.04489) | :heavy_minus_sign: |
| Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis | [![GitHub](https://img.shields.io/github/stars/Dorniwang/PD-FGC-inference?style=flat)](https://github.com/Dorniwang/PD-FGC-inference) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Progressive_Disentangled_Representation_Learning_for_Fine-Grained_Controllable_Talking_Head_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14506-b31b1b.svg)](http://arxiv.org/abs/2211.14506) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PdSQt_zNbC4) |
| CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer | [![GitHub](https://img.shields.io/github/stars/linfengWen98/CAP-VSTNet?style=flat)](https://github.com/linfengWen98/CAP-VSTNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_CAP-VSTNet_Content_Affinity_Preserved_Versatile_Style_Transfer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17867-b31b1b.svg)](https://arxiv.org/abs/2303.17867) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://m.youtube.com/watch?v=OTJ1wEe29Hc) |
| DeepVecFont-v2: Exploiting Transformers To Synthesize Vector Fonts With Higher Quality | [![GitHub](https://img.shields.io/github/stars/yizhiwang96/deepvecfont-v2?style=flat)](https://github.com/yizhiwang96/deepvecfont-v2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_DeepVecFont-v2_Exploiting_Transformers_To_Synthesize_Vector_Fonts_With_Higher_Quality_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14585-b31b1b.svg)](https://arxiv.org/abs/2303.14585) | :heavy_minus_sign: |
| LEMaRT: Label-Efficient Masked Region Transform for Image Harmonization | [![GitHub](https://img.shields.io/github/stars/yizhiwang96/deepvecfont-v2?style=flat)](https://github.com/yizhiwang96/deepvecfont-v2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_LEMaRT_Label-Efficient_Masked_Region_Transform_for_Image_Harmonization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.13166-b31b1b.svg)](http://arxiv.org/abs/2304.13166) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xSS4RChu7zk) |
| SINE: Semantic-Driven Image-Based NeRF Editing With Prior-Guided Editing Field | [![GitHub](https://img.shields.io/github/stars/zju3dv/SINE?style=flat)](https://github.com/zju3dv/SINE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Bao_SINE_Semantic-Driven_Image-Based_NeRF_Editing_With_Prior-Guided_Editing_Field_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13277-b31b1b.svg)](http://arxiv.org/abs/2303.13277) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=AfAR-PoZ8SM) |
| Exploring Intra-Class Variation Factors With Learnable Cluster Prompts for Semi-Supervised Image Synthesis | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Exploring_Intra-Class_Variation_Factors_With_Learnable_Cluster_Prompts_for_Semi-Supervised_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Image Cropping With Spatial-Aware Feature and Rank Consistency | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Image_Cropping_With_Spatial-Aware_Feature_and_Rank_Consistency_CVPR_2023_paper.pdf) <br /> [![thecvf](https://img.shields.io/badge/sup-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/supplemental/Wang_Image_Cropping_With_CVPR_2023_supplemental.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=J8ImNnEWwGQ) |
| Picture That Sketch: Photorealistic Image Generation From Abstract Sketches | [![GitHub](https://img.shields.io/github/stars/subhadeepkoley/PictureThatSketch?style=flat)](https://github.com/subhadeepkoley/PictureThatSketch) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Koley_Picture_That_Sketch_Photorealistic_Image_Generation_From_Abstract_Sketches_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11162-b31b1b.svg)](http://arxiv.org/abs/2303.11162) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=k7xFbELpnv4) |
| MonoHuman: Animatable Human Neural Field From Monocular Video | [![GitHub](https://img.shields.io/github/stars/Yzmblog/MonoHuman?style=flat)](https://github.com/Yzmblog/MonoHuman) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_MonoHuman_Animatable_Human_Neural_Field_From_Monocular_Video_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02001-b31b1b.svg)](http://arxiv.org/abs/2304.02001) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=T91fXw9dOmM) |
| PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Sheng_PixHt-Lab_Pixel_Height_Based_Light_Effect_Generation_for_Image_Compositing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.00137-b31b1b.svg)](https://arxiv.org/abs/2303.00137) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=H2B0yrEf86I) |
| Neural Pixel Composition for 3D-4D View Synthesis From Multi-Views | [![Web Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.aayushbansal.xyz/npc/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Bansal_Neural_Pixel_Composition_for_3D-4D_View_Synthesis_From_Multi-Views_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2207.10663-b31b1b.svg)](https://arxiv.org/abs/2207.10663) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=oTJyUUH2uCk) |
| SpaText: Spatio-Textual Representation for Controllable Image Generation | [![Web Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://omriavrahami.com/spatext/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Avrahami_SpaText_Spatio-Textual_Representation_for_Controllable_Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14305-b31b1b.svg)](http://arxiv.org/abs/2211.14305) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VlieNoCwHO4) |
| Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Exploring_Motion_Ambiguity_and_Alignment_for_High-Quality_Video_Frame_Interpolation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2203.10291-b31b1b.svg)](http://arxiv.org/abs/2203.10291) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WoAyz1S_nTI) |
| MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation | [![GitHub](https://img.shields.io/github/stars/researchmm/MM-Diffusion?style=flat)](https://github.com/researchmm/MM-Diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ruan_MM-Diffusion_Learning_Multi-Modal_Diffusion_Models_for_Joint_Audio_and_Video_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09478-b31b1b.svg)](https://arxiv.org/abs/2212.09478) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DthMxv2VogU) |
| Synthesizing Photorealistic Virtual Humans Through Cross-Modal Disentanglement | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ondrejtexler.github.io/synthesizing_humans/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ravichandran_Synthesizing_Photorealistic_Virtual_Humans_Through_Cross-Modal_Disentanglement_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.01320-b31b1b.svg)](http://arxiv.org/abs/2209.01320) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://youtu.be/oNYy2-_xuhM) |
| Video Probabilistic Diffusion Models in Projected Latent Space | [![Web Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sihyun.me/PVDM/) <br /> [![GitHub](https://img.shields.io/github/stars/sihyun-yu/PVDM?style=flat)](https://github.com/sihyun-yu/PVDM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Video_Probabilistic_Diffusion_Models_in_Projected_Latent_Space_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.07685-b31b1b.svg)](http://arxiv.org/abs/2302.07685) | :heavy_minus_sign: |
| Variational Distribution Learning for Unsupervised Text-to-Image Generation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Variational_Distribution_Learning_for_Unsupervised_Text-to-Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16105-b31b1b.svg)](http://arxiv.org/abs/2303.16105) | :heavy_minus_sign: |
| Linking Garment With Person via Semantically Associated Landmarks for Virtual Try-On | [![Web Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://modelscope.cn/datasets/damo/SAL-HG/summary) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_Linking_Garment_With_Person_via_Semantically_Associated_Landmarks_for_Virtual_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| UV Volumes for Real-Time Rendering of Editable Free-View Human Performance | [![GitHub](https://img.shields.io/github/stars/fanegg/UV-Volumes?style=flat)](https://github.com/fanegg/UV-Volumes) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_UV_Volumes_for_Real-Time_Rendering_of_Editable_Free-View_Human_Performance_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2203.14402-b31b1b.svg)](http://arxiv.org/abs/2203.14402) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=v3PsN-rMAUw) |
| NULL-Text Inversion for Editing Real Images Using Guided Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://null-text-inversion.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/google/prompt-to-prompt?style=flat)](https://github.com/google/prompt-to-prompt/#null-text-inversion-for-editing-real-images) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Mokady_NULL-Text_Inversion_for_Editing_Real_Images_Using_Guided_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.09794-b31b1b.svg)](http://arxiv.org/abs/2211.09794) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qzTlzrMWU2M) |
| Polynomial Implicit Neural Representations for Large Diverse Datasets <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/Rajhans0/Poly_INR?style=flat)](https://github.com/Rajhans0/Poly_INR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Singh_Polynomial_Implicit_Neural_Representations_for_Large_Diverse_Datasets_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11424-b31b1b.svg)](http://arxiv.org/abs/2303.11424) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=YdFpzITgV8M) |
| Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation | [![GitHub](https://img.shields.io/github/stars/MichalGeyer/plug-and-play?style=flat)](https://github.com/MichalGeyer/plug-and-play) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tumanyan_Plug-and-Play_Diffusion_Features_for_Text-Driven_Image-to-Image_Translation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12572-b31b1b.svg)](http://arxiv.org/abs/2211.12572) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=eemzbXXU59E) |
| Conditional Image-to-Video Generation With Latent Flow Diffusion Models | [![GitHub](https://img.shields.io/github/stars/nihaomiao/CVPR23_LFDM?style=flat)](https://github.com/nihaomiao/CVPR23_LFDM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ni_Conditional_Image-to-Video_Generation_With_Latent_Flow_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13744-b31b1b.svg)](http://arxiv.org/abs/2303.13744) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dgawtQGmMbA) |
| Local 3D Editing via 3D Distillation of CLIP Knowledge | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hyung_Local_3D_Editing_via_3D_Distillation_of_CLIP_Knowledge_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.12570-b31b1b.svg)](https://arxiv.org/abs/2306.12570) | :heavy_minus_sign: |
| Private Image Generation With Dual-Purpose Auxiliary Classifier <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Private_Image_Generation_With_Dual-Purpose_Auxiliary_Classifier_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZsjYIZ2s0fw) |
| MAGVIT: Masked Generative Video Transformer <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![Web Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://magvit.cs.cmu.edu/) <br /> [![GitHub](https://img.shields.io/github/stars/google-research/magvit?style=flat)](https://github.com/google-research/magvit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_MAGVIT_Masked_Generative_Video_Transformer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.05199-b31b1b.svg)](http://arxiv.org/abs/2212.05199) | :heavy_minus_sign: |
| Dimensionality-Varying Diffusion Process | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Dimensionality-Varying_Diffusion_Process_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.16032-b31b1b.svg)](http://arxiv.org/abs/2211.16032)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zpNhHo3s4Eo) |
| VIVE3D: Viewpoint-Independent Video Editing Using 3D-Aware GANs | [![GitHub](https://img.shields.io/github/stars/afruehstueck/VIVE3D?style=flat)](https://github.com/afruehstueck/VIVE3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Fruhstuck_VIVE3D_Viewpoint-Independent_Video_Editing_Using_3D-Aware_GANs_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15893-b31b1b.svg)](https://arxiv.org/abs/2303.15893) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qfYGQwOw8pg) |
| LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data | [![GitHub](https://img.shields.io/github/stars/KU-CVLAB/LANIT?style=flat)](https://github.com/KU-CVLAB/LANIT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Park_LANIT_Language-Driven_Image-to-Image_Translation_for_Unlabeled_Data_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.14889-b31b1b.svg)](http://arxiv.org/abs/2208.14889) | :heavy_minus_sign: |
| DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model | [![GitHub](https://img.shields.io/github/stars/gwang-kim/DATID-3D?style=flat)](https://github.com/gwang-kim/DATID-3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_DATID-3D_Diversity-Preserved_Domain_Adaptation_Using_Text-to-Image_Diffusion_for_3D_Generative_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.16374-b31b1b.svg)](https://arxiv.org/abs/2211.16374) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bjXQ4LTVE3E) |
| Delving StyleGAN Inversion for Image Editing: A Foundation Latent Space Viewpoint | [![GitHub](https://img.shields.io/github/stars/KumapowerLIU/CLCAE?style=flat)](https://github.com/KumapowerLIU/CLCAE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Delving_StyleGAN_Inversion_for_Image_Editing_A_Foundation_Latent_Space_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11448-b31b1b.svg)](http://arxiv.org/abs/2211.11448) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hsB9Wv50dm0) |
| High-Fidelity and Freely Controllable Talking Head Video Generation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_High-Fidelity_and_Freely_Controllable_Talking_Head_Video_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10168-b31b1b.svg)](http://arxiv.org/abs/2304.10168) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_syQLfiQ0_c) |
| SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sadtalker.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/OpenTalker/SadTalker?style=flat)](https://github.com/OpenTalker/SadTalker) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_SadTalker_Learning_Realistic_3D_Motion_Coefficients_for_Stylized_Audio-Driven_Single_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12194-b31b1b.svg)](http://arxiv.org/abs/2211.12194) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TjUOalcGDtE) |
| StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://kunhao-liu.github.io/StyleRF/) <br /> [![GitHub](https://img.shields.io/github/stars/Kunhao-Liu/StyleRF?style=flat)](https://github.com/Kunhao-Liu/StyleRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_StyleRF_Zero-Shot_3D_Style_Transfer_of_Neural_Radiance_Fields_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10598-b31b1b.svg)](http://arxiv.org/abs/2303.10598) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DkbRmmzTU40) |
| MOSO: Decomposing MOtion, Scene and Object for Video Prediction | [![GitHub](https://img.shields.io/github/stars/iva-mzsun/MOSO?style=flat)](https://github.com/iva-mzsun/MOSO) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_MOSO_Decomposing_MOtion_Scene_and_Object_for_Video_Prediction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03684-b31b1b.svg)](http://arxiv.org/abs/2303.03684) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5kLsKpfJFrQ) |
| Multi Domain Learning for Motion Magnification | [![GitHub](https://img.shields.io/github/stars/jasdeep-singh-007/Multi-Domain-Learning-for-Motion-Magnification?style=flat)](https://github.com/jasdeep-singh-007/Multi-Domain-Learning-for-Motion-Magnification) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Singh_Multi_Domain_Learning_for_Motion_Magnification_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://m.youtube.com/watch?v=Sg7_sXMnRLo&pp=ygUKI-ydgOuLiey4tQ%3D%3D) |
| GazeNeRF: 3D-Aware Gaze Redirection With Neural Radiance Fields | [![GitHub](https://img.shields.io/github/stars/AlessandroRuzzi/GazeNeRF?style=flat)](https://github.com/AlessandroRuzzi/GazeNeRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ruzzi_GazeNeRF_3D-Aware_Gaze_Redirection_With_Neural_Radiance_Fields_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04823-b31b1b.svg)](http://arxiv.org/abs/2212.04823) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JwqKbmUR3DE) |
| Hierarchical B-Frame Video Coding Using Two-Layer CANF Without Motion Coding | [![GitHub](https://img.shields.io/github/stars/nycu-clab/tlzmc-cvpr?style=flat)](https://github.com/nycu-clab/tlzmc-cvpr) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Alexandre_Hierarchical_B-Frame_Video_Coding_Using_Two-Layer_CANF_Without_Motion_Coding_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02690-b31b1b.svg)](https://arxiv.org/abs/2304.02690) | :heavy_minus_sign: |
| Blemish-Aware and Progressive Face Retouching With Limited Paired Data | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Blemish-Aware_and_Progressive_Face_Retouching_With_Limited_Paired_Data_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qKy6t8JbUOs) |
| Text-Guided Unsupervised Latent Transformation for Multi-Attribute Image Manipulation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Text-Guided_Unsupervised_Latent_Transformation_for_Multi-Attribute_Image_Manipulation_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| NeuralField-LDM: Scene Generation With Hierarchical Latent Diffusion Models | [![Web Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://research.nvidia.com/labs/toronto-ai/NFLDM/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_NeuralField-LDM_Scene_Generation_With_Hierarchical_Latent_Diffusion_Models_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Fix the Noise: Disentangling Source Feature for Controllable Domain Translation | [![GitHub](https://img.shields.io/github/stars/LeeDongYeun/FixNoise?style=flat)](https://github.com/LeeDongYeun/FixNoise) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Fix_the_Noise_Disentangling_Source_Feature_for_Controllable_Domain_Translation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11545-b31b1b.svg)](http://arxiv.org/abs/2303.11545) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VqN-rACydQw) |
| Class-Balancing Diffusion Models | [![GitHub](https://img.shields.io/github/stars/qym7/CBDM-pytorch?style=flat)](https://github.com/qym7/CBDM-pytorch) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Class-Balancing_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.00562-b31b1b.svg)](http://arxiv.org/abs/2305.00562) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OWY_2OZ4e_4) |
| DPE: Disentanglement of Pose and Expression for General Video Portrait Editing | [![GitHub](https://img.shields.io/github/stars/OpenTalker/DPE?style=flat)](https://github.com/OpenTalker/DPE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Pang_DPE_Disentanglement_of_Pose_and_Expression_for_General_Video_Portrait_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.06281-b31b1b.svg)](http://arxiv.org/abs/2301.06281) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vj9LELgXVJ0) |
| Inversion-Based Style Transfer With Diffusion Models | [![GitHub](https://img.shields.io/github/stars/zyxElsa/InST?style=flat)](https://github.com/zyxElsa/InST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Inversion-Based_Style_Transfer_With_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13203-b31b1b.svg)](http://arxiv.org/abs/2211.13203) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=W3urLYx9JZY) |
| Deep Curvilinear Editing: Commutative and Nonlinear Image Manipulation for Pretrained Deep Generative Model | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Aoshima_Deep_Curvilinear_Editing_Commutative_and_Nonlinear_Image_Manipulation_for_Pretrained_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14573-b31b1b.svg)](http://arxiv.org/abs/2211.14573) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=iVyvQOAhLqI) |
| FlowGrad: Controlling the Output of Generative ODEs With Gradients | [![GitHub](https://img.shields.io/github/stars/gnobitab/FlowGrad?style=flat)](https://github.com/gnobitab/FlowGrad) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_FlowGrad_Controlling_the_Output_of_Generative_ODEs_With_Gradients_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Graph Transformer GANs for Graph-Constrained House Generation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Graph_Transformer_GANs_for_Graph-Constrained_House_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08225-b31b1b.svg)](http://arxiv.org/abs/2303.08225) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rwx9OPkRc4M) |
| Master: Meta Style Transformer for Controllable Zero-Shot and Few-Shot Artistic Style Transfer | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Master_Meta_Style_Transformer_for_Controllable_Zero-Shot_and_Few-Shot_Artistic_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.11818-b31b1b.svg)](http://arxiv.org/abs/2304.11818) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PJaTztiUsTQ) |
| Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mrtornado24.github.io/Next3D/) <br /> [![GitHub](https://img.shields.io/github/stars/MrTornado24/Next3D?style=flat)](https://github.com/MrTornado24/Next3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Next3D_Generative_Neural_Texture_Rasterization_for_3D-Aware_Head_Avatars_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11208-b31b1b.svg)](http://arxiv.org/abs/2211.11208) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0F6Pmj-1sfI) |
| Ham2Pose: Animating Sign Language Notation Into Pose Sequences | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rotem-shalev.github.io/ham-to-pose/) <br /> [![GitHub](https://img.shields.io/github/stars/rotem-shalev/Ham2Pose?style=flat)](https://github.com/rotem-shalev/Ham2Pose) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Arkushin_Ham2Pose_Animating_Sign_Language_Notation_Into_Pose_Sequences_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13613-b31b1b.svg)](https://arxiv.org/abs/2211.13613) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_XOsxnwjo7s) |
| Neural Transformation Fields for Arbitrary-Styled Font Generation | [![GitHub](https://img.shields.io/github/stars/fubinfb/NTF?style=flat)](https://github.com/fubinfb/NTF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Neural_Transformation_Fields_for_Arbitrary-Styled_Font_Generation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Fxp8N7TDVkQ) |
| LayoutDM: Transformer-Based Diffusion Model for Layout Generation | [![GitHub](https://img.shields.io/github/stars/CyberAgentAILab/layout-dm?style=flat)](https://github.com/CyberAgentAILab/layout-dm) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chai_LayoutDM_Transformer-Based_Diffusion_Model_for_Layout_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.02567-b31b1b.svg)](http://arxiv.org/abs/2305.02567) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=n3akFx3mtYU) |
| Removing Objects From Neural Radiance Fields | [![GitHub](https://img.shields.io/github/stars/nianticlabs/nerf-object-removal?style=flat)](https://github.com/nianticlabs/nerf-object-removal) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Weder_Removing_Objects_From_Neural_Radiance_Fields_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.11966-b31b1b.svg)](http://arxiv.org/abs/2212.11966) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=J8r1jgELmsM) |
| Person Image Synthesis via Denoising Diffusion Model | [![GitHub](https://img.shields.io/github/stars/ankanbhunia/PIDM?style=flat)](https://github.com/ankanbhunia/PIDM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Bhunia_Person_Image_Synthesis_via_Denoising_Diffusion_Model_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12500-b31b1b.svg)](http://arxiv.org/abs/2211.12500) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=NsUb2uxj820) |
| AdaptiveMix: Improving GAN Training via Feature Space Shrinkage | [![GitHub](https://img.shields.io/github/stars/WentianZhang-ML/AdaptiveMix?style=flat)](https://github.com/WentianZhang-ML/AdaptiveMix) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_AdaptiveMix_Improving_GAN_Training_via_Feature_Space_Shrinkage_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.01559-b31b1b.svg)](https://arxiv.org/abs/2303.01559) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=BFKfw9rfYYU) |
| Learning Joint Latent Space EBM Prior Model for Multi-Layer Generator | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jcui1224.github.io/hierarchical-joint-ebm-proj/) <br /> [![GitHub](https://img.shields.io/github/stars/jcui1224/hierarchical-joint-ebm?style=flat)](https://github.com/jcui1224/hierarchical-joint-ebm) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cui_Learning_Joint_Latent_Space_EBM_Prior_Model_for_Multi-Layer_Generator_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.06323-b31b1b.svg)](https://arxiv.org/abs/2306.06323) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VGit3sZWNGM) |
| 3D Neural Field Generation Using Triplane Diffusion | [![GitHub](https://img.shields.io/github/stars/JRyanShue/NFD?style=flat)](https://github.com/JRyanShue/NFD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Shue_3D_Neural_Field_Generation_Using_Triplane_Diffusion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.16677-b31b1b.svg)](http://arxiv.org/abs/2211.16677) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nxAiyoYkTJA) |
| OmniAvatar: Geometry-Guided Controllable 3D Head Synthesis | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_OmniAvatar_Geometry-Guided_Controllable_3D_Head_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15539-b31b1b.svg)](http://arxiv.org/abs/2303.15539) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Z1aXE9q9eUI) |
| RWSC-Fusion: Region-Wise Style-Controlled Fusion Network for the Prohibited X-Ray Security Image Synthesis | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Duan_RWSC-Fusion_Region-Wise_Style-Controlled_Fusion_Network_for_the_Prohibited_X-Ray_Security_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OuOfUqWo2aQ) |
| ObjectStitch: Object Compositing With Diffusion Model | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_ObjectStitch_Object_Compositing_With_Diffusion_Model_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.00932-b31b1b.svg)](https://arxiv.org/abs/2212.00932) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tmHJVatfpn8) |
| Persistent Nature: A Generative Model of Unbounded 3D Worlds | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://chail.github.io/persistent-nature/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chai_Persistent_Nature_A_Generative_Model_of_Unbounded_3D_Worlds_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13515-b31b1b.svg)](http://arxiv.org/abs/2303.13515) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5s1WnwRHmIs) |
| Masked and Adaptive Transformer for Exemplar Based Image Translation | [![GitHub](https://img.shields.io/github/stars/AiArt-HDU/MATEBIT?style=flat)](https://github.com/AiArt-HDU/MATEBIT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Masked_and_Adaptive_Transformer_for_Exemplar_Based_Image_Translation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17123-b31b1b.svg)](http://arxiv.org/abs/2303.17123) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qCypRw-FcTM) |
| Spider GAN: Leveraging Friendly Neighbors To Accelerate GAN Training | [![GitHub](https://img.shields.io/github/stars/DarthSid95/SpiderStyleGAN?style=flat)](https://github.com/DarthSid95/SpiderStyleGAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Asokan_Spider_GAN_Leveraging_Friendly_Neighbors_To_Accelerate_GAN_Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.07613-b31b1b.svg)](http://arxiv.org/abs/2305.07613) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EDrGSguQMBU) |
| Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild | [![GitHub](https://img.shields.io/github/stars/avinabsaha/ReIQA?style=flat)](https://github.com/avinabsaha/ReIQA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Saha_Re-IQA_Unsupervised_Learning_for_Image_Quality_Assessment_in_the_Wild_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00451-b31b1b.svg)](https://arxiv.org/abs/2304.00451) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=gHIAC-L3eFg) |
| Align Your Latents: High-Resolution Video Synthesis With Latent Diffusion Models | [![GitHub](https://img.shields.io/github/stars/srpkdyy/VideoLDM?style=flat)](https://github.com/srpkdyy/VideoLDM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Blattmann_Align_Your_Latents_High-Resolution_Video_Synthesis_With_Latent_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08818-b31b1b.svg)](http://arxiv.org/abs/2304.08818) | :heavy_minus_sign: |
| All Are Worth Words: A ViT Backbone for Diffusion Models | [![GitHub](https://img.shields.io/github/stars/baofff/U-ViT?style=flat)](https://github.com/baofff/U-ViT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Bao_All_Are_Worth_Words_A_ViT_Backbone_for_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.12152-b31b1b.svg)](http://arxiv.org/abs/2209.12152) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Y2EbGfUi2SY) |
| Few-Shot Semantic Image Synthesis With Class Affinity Transfer | [![GitHub](https://img.shields.io/github/stars/endo-yuki-t/Fewshot-SMIS?style=flat)](https://github.com/endo-yuki-t/Fewshot-SMIS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Careil_Few-Shot_Semantic_Image_Synthesis_With_Class_Affinity_Transfer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02321-b31b1b.svg)](https://arxiv.org/abs/2304.02321) | :heavy_minus_sign: |
| Blowing in the Wind: CycleNet for Human Cinemagraphs From Still Images | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hbertiche.github.io/CycleNet/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Bertiche_Blowing_in_the_Wind_CycleNet_for_Human_Cinemagraphs_From_Still_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08639-b31b1b.svg)](http://arxiv.org/abs/2303.08639) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KUTJkPVwcO8) |
| StyleGene: Crossover and Mutation of Region-Level Facial Genes for Kinship Face Synthesis <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://wmpscc.github.io/stylegene/) <br /> [![GitHub](https://img.shields.io/github/stars/CVI-SZU/StyleGene?style=flat)](https://github.com/CVI-SZU/StyleGene) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_StyleGene_Crossover_and_Mutation_of_Region-Level_Facial_Genes_for_Kinship_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TDqGaL79_pg) |
| MixNeRF: Modeling a Ray With Mixture Density for Novel View Synthesis From Sparse Inputs | [![GitHub](https://img.shields.io/github/stars/shawn615/MixNeRF?style=flat)](https://github.com/shawn615/MixNeRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Seo_MixNeRF_Modeling_a_Ray_With_Mixture_Density_for_Novel_View_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.08788-b31b1b.svg)](http://arxiv.org/abs/2302.08788) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PXljJordbFk) |
| MoStGAN-V: Video Generation With Temporal Motion Styles | [![GitHub](https://img.shields.io/github/stars/xiaoqian-shen/MoStGAN-V?style=flat)](https://github.com/xiaoqian-shen/MoStGAN-V) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_MoStGAN-V_Video_Generation_With_Temporal_Motion_Styles_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02777-b31b1b.svg)](https://arxiv.org/abs/2304.02777) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hCyaAMh0Kgk) |
| Frame Interpolation Transformer and Uncertainty Guidance | [![GitHub](https://img.shields.io/github/stars/zhshi0816/Video-Frame-Interpolation-Transformer?style=flat)](https://github.com/zhshi0816/Video-Frame-Interpolation-Transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Plack_Frame_Interpolation_Transformer_and_Uncertainty_Guidance_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9KpZA-tibrU) |
| Towards End-to-End Generative Modeling of Long Videos With Memory-Efficient Bidirectional Transformers | [![GitHub](https://img.shields.io/github/stars/Ugness/MeBT?style=flat)](https://github.com/Ugness/MeBT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yoo_Towards_End-to-End_Generative_Modeling_of_Long_Videos_With_Memory-Efficient_Bidirectional_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11251-b31b1b.svg)](http://arxiv.org/abs/2303.11251) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=QTOJEO5o23k) |
| HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images | [![GitHub](https://img.shields.io/github/stars/facebookresearch/holo_diffusion?style=flat)](https://github.com/facebookresearch/holo_diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16509-b31b1b.svg)](http://arxiv.org/abs/2303.16509) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=F65fEmvUKyc) |
| Neural Texture Synthesis With Guided Correspondence | [![GitHub](https://img.shields.io/github/stars/EliotChenKJ/Guided-Correspondence-Loss?style=flat)](https://github.com/EliotChenKJ/Guided-Correspondence-Loss) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Neural_Texture_Synthesis_With_Guided_Correspondence_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=njB_O08IVCk) |
| PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360&deg; | [![GitHub](https://img.shields.io/github/stars/SizheAn/PanoHead?style=flat)](https://github.com/SizheAn/PanoHead) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/An_PanoHead_Geometry-Aware_3D_Full-Head_Synthesis_in_360deg_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13071-b31b1b.svg)](https://arxiv.org/abs/2303.13071) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Y8NXiBOEWoE) |
| InstructPix2Pix: Learning To Follow Image Editing Instructions <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![Web Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.timothybrooks.com/instruct-pix2pix/) <br /> [![GitHub](https://img.shields.io/github/stars/timothybrooks/instruct-pix2pix?style=flat)](https://github.com/timothybrooks/instruct-pix2pix) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.09800-b31b1b.svg)](http://arxiv.org/abs/2211.09800) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=GUvD5W6tBJ4) |
| Unpaired Image-to-Image Translation With Shortest Path Regularization | [![GitHub](https://img.shields.io/github/stars/Mid-Push/santa?style=flat)](https://github.com/Mid-Push/santa) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Unpaired_Image-to-Image_Translation_With_Shortest_Path_Regularization_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tdzIUbz1JTQ) |
| Freestyle Layout-to-Image Synthesis <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/essunny310/FreestyleNet?style=flat)](https://github.com/essunny310/FreestyleNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_Freestyle_Layout-to-Image_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14412-b31b1b.svg)](http://arxiv.org/abs/2303.14412) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EUeV3b3XHe8) |
| On Distillation of Guided Diffusion Models <br /> ![CVPR - Award](https://img.shields.io/badge/CVPR-Award-294A7C) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Meng_On_Distillation_of_Guided_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.03142-b31b1b.svg)](http://arxiv.org/abs/2210.03142) | :heavy_minus_sign: |
| Single Image Backdoor Inversion via Robust Smoothed Classifiers | [![GitHub](https://img.shields.io/github/stars/locuslab/smoothinv?style=flat)](https://github.com/locuslab/smoothinv) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Single_Image_Backdoor_Inversion_via_Robust_Smoothed_Classifiers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.00215-b31b1b.svg)](http://arxiv.org/abs/2303.00215) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bt1yxznTXrQ) |
| Make-a-Story: Visual Memory Conditioned Consistent Story Generation | [![GitHub](https://img.shields.io/github/stars/ubc-vision/Make-A-Story?style=flat)](https://github.com/ubc-vision/Make-A-Story) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Rahman_Make-a-Story_Visual_Memory_Conditioned_Consistent_Story_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13319-b31b1b.svg)](https://arxiv.org/abs/2211.13319) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=BXZ7LAg1sP8) |
| Towards Practical Plug-and-Play Diffusion Models | [![GitHub](https://img.shields.io/github/stars/riiid/PPAP?style=flat)](https://github.com/riiid/PPAP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Go_Towards_Practical_Plug-and-Play_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.05973-b31b1b.svg)](http://arxiv.org/abs/2212.05973) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mIKEfXJj9Zs) |
| Efficient Scale-Invariant Generator With Column-Row Entangled Pixel Synthesis | [![GitHub](https://img.shields.io/github/stars/VinAIResearch/CREPS?style=flat)](https://github.com/VinAIResearch/CREPS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Nguyen_Efficient_Scale-Invariant_Generator_With_Column-Row_Entangled_Pixel_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14157-b31b1b.svg)](http://arxiv.org/abs/2303.14157) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_gceZhQMc8U) |
| Wavelet Diffusion Models Are Fast and Scalable Image Generators | [![GitHub](https://img.shields.io/github/stars/VinAIResearch/WaveDiff?style=flat)](https://github.com/VinAIResearch/WaveDiff) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Phung_Wavelet_Diffusion_Models_Are_Fast_and_Scalable_Image_Generators_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.16152-b31b1b.svg)](http://arxiv.org/abs/2211.16152) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KaIMMamhKsU) |
| 3D GAN Inversion With Facial Symmetry Prior | [![GitHub](https://img.shields.io/github/stars/FeiiYin/SPI?style=flat)](https://github.com/FeiiYin/SPI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_3D_GAN_Inversion_With_Facial_Symmetry_Prior_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.16927-b31b1b.svg)](http://arxiv.org/abs/2211.16927) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pQF-cOVpQEE) |
| Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert | [![GitHub](https://img.shields.io/github/stars/Sxjdwang/TalkLip?style=flat)](https://github.com/Sxjdwang/TalkLip) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Seeing_What_You_Said_Talking_Face_Generation_Guided_by_a_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17480-b31b1b.svg)](http://arxiv.org/abs/2303.17480) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LUsRjL02ZsY) |
| PCT-Net: Full Resolution Image Harmonization Using Pixel-Wise Color Transformations | [![GitHub](https://img.shields.io/github/stars/rakutentech/PCT-Net-Image-Harmonization?style=flat)](https://github.com/rakutentech/PCT-Net-Image-Harmonization) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Guerreiro_PCT-Net_Full_Resolution_Image_Harmonization_Using_Pixel-Wise_Color_Transformations_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model With Knowledge-Enhanced Mixture-of-Denoising-Experts <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://wenxin.baidu.com/ernie-vilg) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_ERNIE-ViLG_2.0_Improving_Text-to-Image_Diffusion_Model_With_Knowledge-Enhanced_Mixture-of-Denoising-Experts_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.15257-b31b1b.svg)](https://arxiv.org/abs/2210.15257) | :heavy_minus_sign: |
| Video Compression With Entropy-Constrained Neural Representations | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Gomes_Video_Compression_With_Entropy-Constrained_Neural_Representations_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PyxyYlwQuCw) |
| Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models | [![GitHub](https://img.shields.io/github/stars/UCSB-NLP-Chang/DiffusionDisentanglement?style=flat)](https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Uncovering_the_Disentanglement_Capability_in_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08698-b31b1b.svg)](http://arxiv.org/abs/2212.08698) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=m3CVMRAt1Q0) |
| CoralStyleCLIP: Co-Optimized Region and Layer Selection for Image Editing | [![GitHub](https://img.shields.io/github/stars/JiauZhang/CoralStyleCLIP?style=flat)](https://github.com/JiauZhang/CoralStyleCLIP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Revanur_CoralStyleCLIP_Co-Optimized_Region_and_Layer_Selection_for_Image_Editing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05031-b31b1b.svg)](http://arxiv.org/abs/2303.05031) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5LTbVjuErkg) |
| Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding | [![GitHub](https://img.shields.io/github/stars/man805/Diffusion-Video-Autoencoders?style=flat)](https://github.com/man805/Diffusion-Video-Autoencoders) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Diffusion_Video_Autoencoders_Toward_Temporally_Consistent_Face_Video_Editing_via_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.02802-b31b1b.svg)](http://arxiv.org/abs/2212.02802) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ISawoMRNuRU) |
| Sequential Training of GANs Against GAN-Classifiers Reveals Correlated `Knowledge Gaps` Present Among Independently Trained GAN Instances | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Pathak_Sequential_Training_of_GANs_Against_GAN-Classifiers_Reveals_Correlated_Knowledge_Gaps_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15533-b31b1b.svg)](http://arxiv.org/abs/2303.15533) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=q2yOvUIaz2o) |
| Attribute-Preserving Face Dataset Anonymization via Latent Code Optimization <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/chi0tzp/FALCO?style=flat)](https://github.com/chi0tzp/FALCO) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Barattin_Attribute-Preserving_Face_Dataset_Anonymization_via_Latent_Code_Optimization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11296-b31b1b.svg)](http://arxiv.org/abs/2303.11296) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qpGecrlK7_8) |
| Shifted Diffusion for Text-to-Image Generation | [![GitHub](https://img.shields.io/github/stars/drboog/Shifted_Diffusion?style=flat)](https://github.com/drboog/Shifted_Diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Shifted_Diffusion_for_Text-to-Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.15388-b31b1b.svg)](http://arxiv.org/abs/2211.15388) | :heavy_minus_sign: |
| HandsOff: Labeled Dataset Generation With No Additional Human Annotations <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://austinxu87.github.io/handsoff/) <br /> [![GitHub](https://img.shields.io/github/stars/austinxu87/handsoff?style=flat)](https://github.com/austinxu87/handsoff/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_HandsOff_Labeled_Dataset_Generation_With_No_Additional_Human_Annotations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.12645-b31b1b.svg)](http://arxiv.org/abs/2212.12645) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Yn-1QOcrIAQ) |
| Lookahead Diffusion Probabilistic Models for Refining Mean Estimation | [![GitHub](https://img.shields.io/github/stars/guoqiang-zhang-x/LA-DPM?style=flat)](https://github.com/guoqiang-zhang-x/LA-DPM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Lookahead_Diffusion_Probabilistic_Models_for_Refining_Mean_Estimation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.11312-b31b1b.svg)](http://arxiv.org/abs/2304.11312) | :heavy_minus_sign: |
| Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![Web Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://imagen.research.google/editor/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Imagen_Editor_and_EditBench_Advancing_and_Evaluating_Text-Guided_Image_Inpainting_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.06909-b31b1b.svg)](http://arxiv.org/abs/2212.06909) | :heavy_minus_sign: |
| Re-GAN: Data-Efficient GANs Training via Architectural Reconfiguration | [![GitHub](https://img.shields.io/github/stars/IntellicentAI-Lab/Re-GAN?style=flat)](https://github.com/IntellicentAI-Lab/Re-GAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Saxena_Re-GAN_Data-Efficient_GANs_Training_via_Architectural_Reconfiguration_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| BBDM: Image-to-Image Translation With Brownian Bridge Diffusion Models | [![GitHub](https://img.shields.io/github/stars/xuekt98/BBDM?style=flat)](https://github.com/xuekt98/BBDM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_BBDM_Image-to-Image_Translation_With_Brownian_Bridge_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2205.07680-b31b1b.svg)](http://arxiv.org/abs/2205.07680) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZrW88C63Suo) |
| VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models | [![GitHub](https://img.shields.io/github/stars/ximinng/VectorFusion-pytorch?style=flat)](https://github.com/ximinng/VectorFusion-pytorch) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_VectorFusion_Text-to-SVG_by_Abstracting_Pixel-Based_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11319-b31b1b.svg)](http://arxiv.org/abs/2211.11319) | :heavy_minus_sign: |
