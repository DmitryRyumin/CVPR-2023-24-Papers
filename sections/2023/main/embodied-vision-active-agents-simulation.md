# CVPR-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
    <tr>
        <td><strong>New collections</strong></td>
        <td>
            <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README.md">
                <img src="http://img.shields.io/badge/CVPR-2024-0073AE.svg" alt="Conference">
            </a>
        </td>
    </tr>
</table>

<div align="center">
  <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/sections/2023/main/explainable-ai-for-cv.md">
    <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
  </a>
  <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README_2023.md">
    <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
  </a>
  <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/sections/2023/main/document-analysis-and-understanding.md">
    <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
  </a>
</div>

## Embodied Vision: Active Agents, Simulation

![Section Papers](https://img.shields.io/badge/Section%20Papers-14-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-11-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-12-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-10-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Open-World Multi-Task Control through Goal-Aware Representation Learning and Adaptive Horizon Prediction | [![GitHub](https://img.shields.io/github/stars/CraftJarvis/MC-Controller?style=flat)](https://github.com/CraftJarvis/MC-Controller) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_Open-World_Multi-Task_Control_Through_Goal-Aware_Representation_Learning_and_Adaptive_Horizon_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.10034-b31b1b.svg)](https://arxiv.org/abs/2301.10034) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=MRjBzRVTGAs) |
| Layout-based Causal Inference for Object Navigation | [![GitHub](https://img.shields.io/github/stars/sx-zhang/Layout-based-sTDE?style=flat)](https://github.com/sx-zhang/Layout-based-sTDE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Layout-Based_Causal_Inference_for_Object_Navigation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LrWJnxjt1go) |
| EC<sup>2</sup>: Emergent Communication for Embodied Control | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Mu_EC2_Emergent_Communication_for_Embodied_Control_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.09448-b31b1b.svg)](https://arxiv.org/abs/2304.09448) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tiUvQnQtJh8) |
| GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://pku-epic.github.io/GAPartNet/) <br /> [![GitHub](https://img.shields.io/github/stars/PKU-EPIC/GAPartNet?style=flat)](https://github.com/PKU-EPIC/GAPartNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Geng_GAPartNet_Cross-Category_Domain-Generalizable_Object_Perception_and_Manipulation_via_Generalizable_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.05272-b31b1b.svg)](https://arxiv.org/abs/2211.05272) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=cgVFAydWpdk) |
| Phone2Proc: Bringing Robust Robots into Our Chaotic World | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://allenai.org/project/phone2proc/home) <br /> [![GitHub](https://img.shields.io/github/stars/allenai/phone2proc?style=flat)](https://github.com/allenai/phone2proc) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Deitke_Phone2Proc_Bringing_Robust_Robots_Into_Our_Chaotic_World_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04819-b31b1b.svg)](https://arxiv.org/abs/2212.04819) | :heavy_minus_sign: |
| PIRLNav: Pretraining with Imitation and RL Finetuning for ObjectNav | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ram81.github.io/projects/pirlnav) <br /> [![GitHub](https://img.shields.io/github/stars/Ram81/pirlnav?style=flat)](https://github.com/Ram81/pirlnav) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ramrakhya_PIRLNav_Pretraining_With_Imitation_and_RL_Finetuning_for_ObjectNav_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.07302-b31b1b.svg)](https://arxiv.org/abs/2301.07302) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=63C9wpnFrCg) |
| CoWs on PASTURE: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://cow.cs.columbia.edu/) <br /> [![GitHub](https://img.shields.io/github/stars/real-stanford/cow?style=flat)](https://github.com/real-stanford/cow) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Gadre_CoWs_on_Pasture_Baselines_and_Benchmarks_for_Language-Driven_Zero-Shot_Object_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2203.10421-b31b1b.svg)](https://arxiv.org/abs/2203.10421) | :heavy_minus_sign: |
| 3D-Aware Object Goal Navigation via Simultaneous Exploration and Identification | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://pku-epic.github.io/3D-Aware-ObjectNav/) <br /> [![GitHub](https://img.shields.io/github/stars/jzhzhang/3DAwareNav?style=flat)](https://github.com/jzhzhang/3DAwareNav) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_3D-Aware_Object_Goal_Navigation_via_Simultaneous_Exploration_and_Identification_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.00338-b31b1b.svg)](https://arxiv.org/abs/2212.00338) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-50kIfOYTBM) |
| Modality-Invariant Visual Odometry for Embodied Vision | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://memmelma.github.io/vot/) <br /> [![GitHub](https://img.shields.io/github/stars/memmelma/VO-Transformer?style=flat)](https://github.com/memmelma/VO-Transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Memmel_Modality-Invariant_Visual_Odometry_for_Embodied_Vision_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.00348-b31b1b.svg)](https://arxiv.org/abs/2305.00348) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kZCmdHhLkP4) |
| UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://pku-epic.github.io/UniDexGrasp/) <br /> [![GitHub](https://img.shields.io/github/stars/PKU-EPIC/UniDexGrasp?style=flat)](https://github.com/PKU-EPIC/UniDexGrasp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_UniDexGrasp_Universal_Robotic_Dexterous_Grasping_via_Learning_Diverse_Proposal_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.00938-b31b1b.svg)](https://arxiv.org/abs/2303.00938) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=HR2JqApZKBs) |
| EXCALIBUR: Encouraging and Evaluating Embodied Exploration | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_EXCALIBUR_Encouraging_and_Evaluating_Embodied_Exploration_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=SboNjVuIUJA) |
| Leverage Interactive Affinity for Affordance Learning | [![GitHub](https://img.shields.io/github/stars/lhc1224/PIAL-Net?style=flat)](https://github.com/lhc1224/PIAL-Net) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Leverage_Interactive_Affinity_for_Affordance_Learning_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| LANA: A Language-Capable Navigator for Instruction Following and Generation | [![GitHub](https://img.shields.io/github/stars/wxh1996/LANA-VLN?style=flat)](https://github.com/wxh1996/LANA-VLN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_LANA_A_Language-Capable_Navigator_for_Instruction_Following_and_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08409-b31b1b.svg)](https://arxiv.org/abs/2303.08409) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=BurKOFn-78g) |
| Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at 100k Steps-per-Second | [![GitHub](https://img.shields.io/github/stars/facebookresearch/galactic?style=flat)](https://github.com/facebookresearch/galactic) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Berges_Galactic_Scaling_End-to-End_Reinforcement_Learning_for_Rearrangement_at_100k_Steps-per-Second_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.07552-b31b1b.svg)](https://arxiv.org/abs/2306.07552) | :heavy_minus_sign: |
