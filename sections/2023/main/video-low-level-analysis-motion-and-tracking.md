# CVPR-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
    <tr>
        <td><strong>New collections</strong></td>
        <td>
            <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README.md">
                <img src="http://img.shields.io/badge/CVPR-2024-0073AE.svg" alt="Conference">
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/sections/2023/main/computational-imaging.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README_2023.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/sections/2023/main/vision-applications-and-systems.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Video: Low-Level Analysis, Motion, and Tracking

![Section Papers](https://img.shields.io/badge/Section%20Papers-46-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-33-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-35-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-36-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Uncovering the Missing Pattern: Unified Framework Towards Trajectory Imputation and Prediction | [![GitHub](https://img.shields.io/github/stars/colorfulfuture/GC-VRNN?style=flat)](https://github.com/colorfulfuture/GC-VRNN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Uncovering_the_Missing_Pattern_Unified_Framework_Towards_Trajectory_Imputation_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16005-b31b1b.svg)](http://arxiv.org/abs/2303.16005) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=fneLvMJIALo) |
| Tracking Multiple Deformable Objects in Egocentric Videos | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mingzhenhuang.com/projects/detracker.html) <br /> [![GitHub](https://img.shields.io/github/stars/Mingzhen-Huang/DETracker?style=flat)](https://github.com/Mingzhen-Huang/DETracker) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Tracking_Multiple_Deformable_Objects_in_Egocentric_Videos_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Tracking through Containers and Occluders in the Wild | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://tcow.cs.columbia.edu/) <br /> [![GitHub](https://img.shields.io/github/stars/basilevh/tcow?style=flat)](https://github.com/basilevh/tcow) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Van_Hoorick_Tracking_Through_Containers_and_Occluders_in_the_Wild_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.03052-b31b1b.svg)](http://arxiv.org/abs/2305.03052) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WODiwfq9d2g) |
| TarViS: A Unified Approach for Target-based Video Segmentation <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/Ali2500/TarViS?style=flat)](https://github.com/Ali2500/TarViS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Athar_TarViS_A_Unified_Approach_for_Target-Based_Video_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.02657-b31b1b.svg)](http://arxiv.org/abs/2301.02657) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qecFRmSYq40) |
| VideoTrack: Learning to Track Objects via Video Transformer | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_VideoTrack_Learning_To_Track_Objects_via_Video_Transformer_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=v-SvFZ0FrF8) |
| ARKitTrack: A New Diverse Dataset for Tracking using Mobile RGB-D Data | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://arkittrack.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/lawrence-cj/ARKitTrack?style=flat)](https://github.com/lawrence-cj/ARKitTrack) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_ARKitTrack_A_New_Diverse_Dataset_for_Tracking_Using_Mobile_RGB-D_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13885-b31b1b.svg)](http://arxiv.org/abs/2303.13885) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=r02f6egcpdw) |
| A Dynamic Multi-Scale Voxel Flow Network for Video Prediction <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://huxiaotaostasy.github.io/DMVFN/) <br /> [![GitHub](https://img.shields.io/github/stars/megvii-research/CVPR2023-DMVFN?style=flat)](https://github.com/megvii-research/CVPR2023-DMVFN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_A_Dynamic_Multi-Scale_Voxel_Flow_Network_for_Video_Prediction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09875-b31b1b.svg)](http://arxiv.org/abs/2303.09875) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rlghCGbAqUo) |
| Representation Learning for Visual Object Tracking by Masked Appearance Transfer | [![GitHub](https://img.shields.io/github/stars/difhnp/MAT?style=flat)](https://github.com/difhnp/MAT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Representation_Learning_for_Visual_Object_Tracking_by_Masked_Appearance_Transfer_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| EqMotion: Equivariant Multi-Agent Motion Prediction with Invariant Interaction Reasoning | [![GitHub](https://img.shields.io/github/stars/MediaBrain-SJTU/EqMotion?style=flat)](https://github.com/MediaBrain-SJTU/EqMotion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_EqMotion_Equivariant_Multi-Agent_Motion_Prediction_With_Invariant_Interaction_Reasoning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10876-b31b1b.svg)](http://arxiv.org/abs/2303.10876) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ROactuGU1YA) |
| Semi-Supervised Video Inpainting with Cycle Consistency Constraints | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Semi-Supervised_Video_Inpainting_With_Cycle_Consistency_Constraints_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.06807-b31b1b.svg)](http://arxiv.org/abs/2208.06807) | :heavy_minus_sign: |
| Generalized Relation Modeling for Transformer Tracking | [![GitHub](https://img.shields.io/github/stars/Little-Podi/GRM?style=flat)](https://github.com/Little-Podi/GRM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Generalized_Relation_Modeling_for_Transformer_Tracking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16580-b31b1b.svg)](http://arxiv.org/abs/2303.16580) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bQKN3HV-8XI) |
| Breaking the ``Object`` in Video Object Segmentation | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.vostdataset.org/) <br /> [![GitHub](https://img.shields.io/github/stars/TRI-ML/VOST?style=flat)](https://github.com/TRI-ML/VOST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tokmakov_Breaking_the_Object_in_Video_Object_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.06200-b31b1b.svg)](http://arxiv.org/abs/2212.06200) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=SBdA6HCXf_M) |
| Unifying Short and Long-Term Tracking with Graph Hierarchies | [![GitHub](https://img.shields.io/github/stars/dvl-tum/SUSHI?style=flat)](https://github.com/dvl-tum/SUSHI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cetintas_Unifying_Short_and_Long-Term_Tracking_With_Graph_Hierarchies_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03038-b31b1b.svg)](http://arxiv.org/abs/2212.03038) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Q1TiZukGQYQ) |
| Simple Cues Lead to a Strong Multi-Object Tracker | [![GitHub](https://img.shields.io/github/stars/dvl-tum/GHOST?style=flat)](https://github.com/dvl-tum/GHOST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Seidenschwarz_Simple_Cues_Lead_to_a_Strong_Multi-Object_Tracker_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.04656-b31b1b.svg)](http://arxiv.org/abs/2206.04656) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3gozhzOHwE0) |
| Unified Mask Embedding and Correspondence Learning for Self-Supervised Video Segmentation | [![GitHub](https://img.shields.io/github/stars/0liliulei/Mask-VOS?style=flat)](https://github.com/0liliulei/Mask-VOS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Unified_Mask_Embedding_and_Correspondence_Learning_for_Self-Supervised_Video_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10100-b31b1b.svg)](http://arxiv.org/abs/2303.10100) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LJj4frqBgqY) |
| MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors | [![GitHub](https://img.shields.io/github/stars/megvii-research/MOTRv2?style=flat)](https://github.com/megvii-research/MOTRv2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_MOTRv2_Bootstrapping_End-to-End_Multi-Object_Tracking_by_Pretrained_Object_Detectors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.09791-b31b1b.svg)](http://arxiv.org/abs/2211.09791) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7WnQgQLQLE4) |
| SeqTrack: Sequence to Sequence Learning for Visual Object Tracking | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg?style=flat)](https://github.com/microsoft/VideoX/tree/master/SeqTrack) <br /> [![GitHub](https://img.shields.io/github/stars/microsoft/VideoX?style=flat)](https://github.com/microsoft/VideoX) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_SeqTrack_Sequence_to_Sequence_Learning_for_Visual_Object_Tracking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.14394-b31b1b.svg)](http://arxiv.org/abs/2304.14394) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jb_zZqrEcJA) |
| Joint Visual Grounding and Tracking with Natural Language Specification | [![GitHub](https://img.shields.io/github/stars/lizhou-cs/JointNLT?style=flat)](https://github.com/lizhou-cs/JointNLT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Joint_Visual_Grounding_and_Tracking_With_Natural_Language_Specification_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12027-b31b1b.svg)](http://arxiv.org/abs/2303.12027) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kUN7tchiG2Q) |
| Boosting Video Object Segmentation via Space-Time Correspondence Learning | [![GitHub](https://img.shields.io/github/stars/wenguanwang/VOS_Correspondence?style=flat)](https://github.com/wenguanwang/VOS_Correspondence) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Boosting_Video_Object_Segmentation_via_Space-Time_Correspondence_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06211-b31b1b.svg)](http://arxiv.org/abs/2304.06211) | :heavy_minus_sign: |
| Visual Prompt Multi-Modal Tracking | [![GitHub](https://img.shields.io/github/stars/jiawen-zhu/ViPT?style=flat)](https://github.com/jiawen-zhu/ViPT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Visual_Prompt_Multi-Modal_Tracking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10826-b31b1b.svg)](http://arxiv.org/abs/2303.10826) | :heavy_minus_sign: |
| OVTrack: Open-Vocabulary Multiple Object Tracking | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.vis.xyz/pub/ovtrack/) <br /> [![GitHub](https://img.shields.io/github/stars/SysCV/ovtrack?style=flat)](https://github.com/SysCV/ovtrack) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_OVTrack_Open-Vocabulary_Multiple_Object_Tracking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08408-b31b1b.svg)](http://arxiv.org/abs/2304.08408) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tMQ_sh0JbpY) |
| TransFlow: Transformer as Flow Learner <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_TransFlow_Transformer_As_Flow_Learner_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.11523-b31b1b.svg)](http://arxiv.org/abs/2304.11523) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xbnyj9wspqA) |
| Focus on Details: Online Multi-Object Tracking with Diverse Fine-grained Representation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Focus_on_Details_Online_Multi-Object_Tracking_With_Diverse_Fine-Grained_Representation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.14589-b31b1b.svg)](http://arxiv.org/abs/2302.14589) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=j4TJJEsqllM) |
| Autoregressive Visual Tracking <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/MIV-XJTU/ARTrack?style=flat)](https://github.com/MIV-XJTU/ARTrack) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Autoregressive_Visual_Tracking_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=fOkpG5SNaX0) |
| Bootstrapping Objectness from Videos by Relaxed Common Fate and Visual Grouping | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rcf-video.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/TonyLianLong/RCF-UnsupVideoSeg?style=flat)](https://github.com/TonyLianLong/RCF-UnsupVideoSeg) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lian_Bootstrapping_Objectness_From_Videos_by_Relaxed_Common_Fate_and_Visual_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08025-b31b1b.svg)](http://arxiv.org/abs/2304.08025) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dyaDEvT4YkY) |
| Tangentially Elongated Gaussian Belief Propagation for Event-based Incremental Optical Flow Estimation | [![GitHub](https://img.shields.io/github/stars/DensoITLab/tegbp?style=flat)](https://github.com/DensoITLab/tegbp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Nagata_Tangentially_Elongated_Gaussian_Belief_Propagation_for_Event-Based_Incremental_Optical_Flow_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rClkk5MY33A) |
| Bridging Search Region Interaction with Template for RGB-T Tracking | [![GitHub](https://img.shields.io/github/stars/RyanHTR/TBSI?style=flat)](https://github.com/RyanHTR/TBSI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hui_Bridging_Search_Region_Interaction_With_Template_for_RGB-T_Tracking_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Efficient RGB-T Tracking via Cross-Modality Distillation <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Efficient_RGB-T_Tracking_via_Cross-Modality_Distillation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=isTrxcb1dVs) |
| MotionTrack: Learning Robust Short-Term and Long-Term Motions for Multi-Object Tracking | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_MotionTrack_Learning_Robust_Short-Term_and_Long-Term_Motions_for_Multi-Object_Tracking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10404-b31b1b.svg)](http://arxiv.org/abs/2303.10404) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=HaS9cM75J7Y) |
| Self-Supervised AutoFlow | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Self-Supervised_AutoFlow_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.01762-b31b1b.svg)](http://arxiv.org/abs/2212.01762) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=e8EuIgzJeYc) |
| UTM: A Unified Multiple Object Tracking Model with Identity-Aware Feature Enhancement | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/You_UTM_A_Unified_Multiple_Object_Tracking_Model_With_Identity-Aware_Feature_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qLR4WJVi5O4) |
| BiFormer: Learning Bilateral Motion Estimation via Bilateral Transformer for 4K Video Frame Interpolation | [![GitHub](https://img.shields.io/github/stars/JunHeum/BiFormer?style=flat)](https://github.com/JunHeum/BiFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Park_BiFormer_Learning_Bilateral_Motion_Estimation_via_Bilateral_Transformer_for_4K_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02225-b31b1b.svg)](http://arxiv.org/abs/2304.02225) | :heavy_minus_sign: |
| Spatial-then-Temporal Self-Supervised Learning for Video Correspondence | [![GitHub](https://img.shields.io/github/stars/qianduoduolr/Spa-then-Temp?style=flat)](https://github.com/qianduoduolr/Spa-then-Temp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Spatial-Then-Temporal_Self-Supervised_Learning_for_Video_Correspondence_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.07778-b31b1b.svg)](http://arxiv.org/abs/2209.07778) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tN-PRaLS_pw) |
| BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://bundlesdf.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/NVlabs/BundleSDF?style=flat)](https://github.com/NVlabs/BundleSDF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_BundleSDF_Neural_6-DoF_Tracking_and_3D_Reconstruction_of_Unknown_Objects_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14158-b31b1b.svg)](http://arxiv.org/abs/2303.14158) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=XH5t0wEH3d0) |
| MED-VT: Multiscale Encoder-Decoder Video Transformer with Application to Object Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rkyuca.github.io/medvt/) <br /> [![GitHub](https://img.shields.io/github/stars/rkyuca/medvt?style=flat)](https://github.com/rkyuca/medvt) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Karim_MED-VT_Multiscale_Encoder-Decoder_Video_Transformer_With_Application_To_Object_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05930-b31b1b.svg)](http://arxiv.org/abs/2304.05930) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nVb9aVPyr4I) |
| Context-Aware Relative Object Queries to Unify Video Instance and Panoptic Segmentation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Choudhuri_Context-Aware_Relative_Object_Queries_To_Unify_Video_Instance_and_Panoptic_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=N6A9Q8Nji7M) |
| Unsupervised Space-Time Network for Temporally-Consistent Segmentation of Multiple Motions <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/Etienne-Meunier-Inria/ST-Space-Time-Flow-Segmentation?style=flat)](https://github.com/Etienne-Meunier-Inria/ST-Space-Time-Flow-Segmentation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Meunier_Unsupervised_Space-Time_Network_for_Temporally-Consistent_Segmentation_of_Multiple_Motions_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=fZHEOJeDQoc) |
| Resource-Efficient RGBD Aerial Tracking | [![GitHub](https://img.shields.io/github/stars/yjybuaa/RGBDAerialTracking?style=flat)](https://github.com/yjybuaa/RGBDAerialTracking) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Resource-Efficient_RGBD_Aerial_Tracking_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=yJ4Hsh8S2iA) |
| MMVC: Learned Multi-Mode Video Compression with Block-based Prediction Mode Selection and Density-Adaptive Entropy Coding | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_MMVC_Learned_Multi-Mode_Video_Compression_With_Block-Based_Prediction_Mode_Selection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02273-b31b1b.svg)](http://arxiv.org/abs/2304.02273) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mt7smZiL4CA) |
| Streaming Video Model | [![GitHub](https://img.shields.io/github/stars/yuzhms/Streaming-Video-Model?style=flat)](https://github.com/yuzhms/Streaming-Video-Model) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Streaming_Video_Model_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17228-b31b1b.svg)](http://arxiv.org/abs/2303.17228) | :heavy_minus_sign: |
| Weakly Supervised Class-Agnostic Motion Prediction for Autonomous Driving | [![GitHub](https://img.shields.io/github/stars/L1bra1/WeakMotion?style=flat)](https://github.com/L1bra1/WeakMotion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Weakly_Supervised_Class-Agnostic_Motion_Prediction_for_Autonomous_Driving_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Q6CcZ6uPqhI) |
| LSTFE-Net: Long Short-Term Feature Enhancement Network for Video Small Object Detection | [![GitHub](https://img.shields.io/github/stars/xiaojs18/LSTFE-Net?style=flat)](https://github.com/xiaojs18/LSTFE-Net) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_LSTFE-NetLong_Short-Term_Feature_Enhancement_Network_for_Video_Small_Object_Detection_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=f3vX21qP_hA) |
| DistractFlow: Improving Optical Flow Estimation via Realistic Distractions and Pseudo-Labeling | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jeong_DistractFlow_Improving_Optical_Flow_Estimation_via_Realistic_Distractions_and_Pseudo-Labeling_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14078-b31b1b.svg)](http://arxiv.org/abs/2303.14078) | :heavy_minus_sign: |
| SCOTCH and SODA: A Transformer Video Shadow Detection Framework | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lihaoliu-cambridge.github.io/scotch_and_soda/) <br /> [![GitHub](https://img.shields.io/github/stars/lihaoliu-cambridge/scotch-and-soda?style=flat)](https://github.com/lihaoliu-cambridge/scotch-and-soda) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_SCOTCH_and_SODA_A_Transformer_Video_Shadow_Detection_Framework_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.06885-b31b1b.svg)](http://arxiv.org/abs/2211.06885) | :heavy_minus_sign: |
| ZBS: Zero-Shot Background Subtraction via Instance-Level Background Modeling and Foreground Selection | [![GitHub](https://img.shields.io/github/stars/CASIA-IVA-Lab/ZBS?style=flat)](https://github.com/CASIA-IVA-Lab/ZBS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/An_ZBS_Zero-Shot_Background_Subtraction_via_Instance-Level_Background_Modeling_and_Foreground_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14679-b31b1b.svg)](http://arxiv.org/abs/2303.14679) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-WuowqTbFIw) |
| Frame-Event Alignment and Fusion Network for High Frame Rate Tracking | [![GitHub](https://img.shields.io/github/stars/Jee-King/AFNet?style=flat)](https://github.com/Jee-King/AFNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Frame-Event_Alignment_and_Fusion_Network_for_High_Frame_Rate_Tracking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.15688-b31b1b.svg)](http://arxiv.org/abs/2305.15688) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=W7EjOiGMiAQ) |
