# CVPR-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/2023/main/transfer-meta-low-shot-continual-or-long-tail-learning.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/2023/main/vision-language-and-reasoning.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Recognition: Categorization, Detection, Retrieval

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| <i>R<sup>2</sup></i>Former: Unified Retrieval and Reranking Transformer for Place Recognition | [![GitHub](https://img.shields.io/github/stars/Jeff-Zilence/R2Former?style=flat)](https://github.com/Jeff-Zilence/R2Former)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhu_R2Former_Unified_Retrieval_and_Reranking_Transformer_for_Place_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03410-b31b1b.svg)](https://arxiv.org/abs/2304.03410)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=suF-Yc2yxY4&list=PLd3hlSJsX_Ikm5jUWxEWUHAmUUq55lJDQ&index=8) |
| Mask-Free OVIS: Open-Vocabulary Instance Segmentation Without Manual Mask Annotations | [![GitHub](https://img.shields.io/github/stars/Vibashan/Maskfree-OVIS?style=flat)](https://github.com/Vibashan/Maskfree-OVIS)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/VS_Mask-Free_OVIS_Open-Vocabulary_Instance_Segmentation_Without_Manual_Mask_Annotations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16891-b31b1b.svg)](http://arxiv.org/abs/2303.16891) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7P7PX3gd14I) |
| StructVPR: Distill Structural Knowledge With Weighting Samples for Visual Place Recognition| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shen_StructVPR_Distill_Structural_Knowledge_With_Weighting_Samples_for_Visual_Place_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.00937-b31b1b.svg)](http://arxiv.org/abs/2212.00937) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LgdWnBv8Nc4) |
| MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining | [![GitHub](https://img.shields.io/github/stars/LightDXY/MaskCLIP?style=flat)](https://github.com/LightDXY/MaskCLIP)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Dong_MaskCLIP_Masked_Self-Distillation_Advances_Contrastive_Language-Image_Pretraining_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.12262-b31b1b.svg)](http://arxiv.org/abs/2208.12262) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2PPiLDTtVzQ) |
| One-to-Few Label Assignment for End-to-End Dense Detection | [![GitHub](https://img.shields.io/github/stars/strongwolf/o2f?style=flat)](https://github.com/strongwolf/o2f)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_One-to-Few_Label_Assignment_for_End-to-End_Dense_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11567-b31b1b.svg)](http://arxiv.org/abs/2303.11567) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2v7i-1TltBs) |
| Where Is My Wallet? Modeling Object Proposal Sets for Egocentric Visual Query Localization | [![GitHub](https://img.shields.io/github/stars/facebookresearch/vq2d_cvpr?style=flat)](https://github.com/facebookresearch/vq2d_cvpr)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Where_Is_My_Wallet_Modeling_Object_Proposal_Sets_for_Egocentric_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.10528-b31b1b.svg)](http://arxiv.org/abs/2211.10528) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2TU4TWD6FmM) |
| Semi-DETR: Semi-Supervised Object Detection With Detection Transformers | [![GitHub](https://img.shields.io/github/stars/JCZ404/Semi-DETR?style=flat)](https://github.com/JCZ404/Semi-DETR)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Semi-DETR_Semi-Supervised_Object_Detection_With_Detection_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08095-b31b1b.svg)](https://arxiv.org/abs/2307.08095) | :heavy_minus_sign: |
| Universal Instance Perception As Object Discovery and Retrieval | [![GitHub](https://img.shields.io/github/stars/MasterBin-IIAU/UNINEXT?style=flat)](https://github.com/MasterBin-IIAU/UNINEXT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yan_Universal_Instance_Perception_As_Object_Discovery_and_Retrieval_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06674-b31b1b.svg)](http://arxiv.org/abs/2303.06674) | :heavy_minus_sign: |
| CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ma_CAT_LoCalization_and_IdentificAtion_Cascade_Detection_Transformer_for_Open-World_Object_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.01970-b31b1b.svg)](http://arxiv.org/abs/2301.01970) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pYiWsGlg_n8) |
| Phase-Shifting Coder: Predicting Accurate Orientation in Oriented Object Detection|  [![GitHub](https://img.shields.io/github/stars/open-mmlab/mmrotate?style=flat)](https://github.com/open-mmlab/mmrotate)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_Phase-Shifting_Coder_Predicting_Accurate_Orientation_in_Oriented_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.06368-b31b1b.svg)](http://arxiv.org/abs/2211.06368) | :heavy_minus_sign: |
| FrustumFormer: Adaptive Instance-Aware Resampling for Multi-View 3D Detection | [![GitHub](https://img.shields.io/github/stars/Robertwyq/Frustum?style=flat)](https://github.com/Robertwyq/Frustum)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_FrustumFormer_Adaptive_Instance-Aware_Resampling_for_Multi-View_3D_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.04467-b31b1b.svg)](http://arxiv.org/abs/2301.04467) |  [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vQTxzc3Z5uQ) | |
| Box-Level Active Detection <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]()  | [![GitHub](https://img.shields.io/github/stars/lyumengyao/blad?style=flat)](https://github.com/lyumengyao/blad) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lyu_Box-Level_Active_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13089-b31b1b.svg)](http://arxiv.org/abs/2303.13089) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_3lfkJpltUc) |
| Learning With Noisy Labels via Self-Supervised Adversarial Noisy Masking | [![GitHub](https://img.shields.io/github/stars/yuanpengtu/SANM?style=flat)](https://github.com/yuanpengtu/SANM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tu_Learning_With_Noisy_Labels_via_Self-Supervised_Adversarial_Noisy_Masking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.06805-b31b1b.svg)](http://arxiv.org/abs/2302.06805) | :heavy_minus_sign: |
| Ambiguity-Resistant Semi-Supervised Learning for Dense Object Detection | [![GitHub](https://img.shields.io/github/stars/PaddlePaddle/PaddleDetection?style=flat)](https://github.com/PaddlePaddle/PaddleDetection) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_Ambiguity-Resistant_Semi-Supervised_Learning_for_Dense_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14960-b31b1b.svg)](http://arxiv.org/abs/2303.14960) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=aVXygEf7Z7M) |
| Aligning Bag of Regions for Open-Vocabulary Object Detection | [![GitHub](https://img.shields.io/github/stars/wusize/ovdet?style=flat)](https://github.com/wusize/ovdet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Aligning_Bag_of_Regions_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.13996-b31b1b.svg)](http://arxiv.org/abs/2302.13996) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kXZmWiHsUW0) |
| Asymmetric Feature Fusion for Image Retrieval | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Asymmetric_Feature_Fusion_for_Image_Retrieval_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.00671-b31b1b.svg)](https://arxiv.org/abs/2403.00671) | :heavy_minus_sign: |
| 3D Video Object Detection With Learnable Object-Centric Global Optimization | [![GitHub](https://img.shields.io/github/stars/jiaweihe1996/BA-Det?style=flat)](https://github.com/jiaweihe1996/BA-Det) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/He_3D_Video_Object_Detection_With_Learnable_Object-Centric_Global_Optimization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15416-b31b1b.svg)](http://arxiv.org/abs/2303.15416) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=m07Yzg9f-_g) |
| Enhanced Training of Query-Based Object Detection via Selective Query Recollection | [![GitHub](https://img.shields.io/github/stars/Fangyi-Chen/SQR?style=flat)](https://github.com/Fangyi-Chen/SQR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Enhanced_Training_of_Query-Based_Object_Detection_via_Selective_Query_Recollection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.07593-b31b1b.svg)](http://arxiv.org/abs/2212.07593) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=UIAZ75ov0ko) |
| Dense Distinct Query for End-to-End Object Detection | [![GitHub](https://img.shields.io/github/stars/jshilong/DDQ?style=flat)](https://github.com/jshilong/DDQ) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Dense_Distinct_Query_for_End-to-End_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12776-b31b1b.svg)](http://arxiv.org/abs/2303.12776) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Lwcp910FxKg) |
| On-the-Fly Category Discovery | [![GitHub](https://img.shields.io/github/stars/PRIS-CV/On-the-fly-Category-Discovery?style=flat)](https://github.com/PRIS-CV/On-the-fly-Category-Discovery) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Du_On-the-Fly_Category_Discovery_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kAH4pX0tcNg) |
| ProD: Prompting-To-Disentangle Domain Knowledge for Cross-Domain Few-Shot Image Classification | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ma_ProD_Prompting-To-Disentangle_Domain_Knowledge_for_Cross-Domain_Few-Shot_Image_Classification_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Q-DETR: An Efficient Low-Bit Quantized Detection Transformer <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/SteveTsui/Q-DETR?style=flat)](https://github.com/SteveTsui/Q-DETR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Q-DETR_An_Efficient_Low-Bit_Quantized_Detection_Transformer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00253-b31b1b.svg)](https://arxiv.org/abs/2304.00253) | :heavy_minus_sign: |
| SAP-DETR: Bridging the Gap Between Salient Points and Queries-Based Transformer Detector for Fast Model Convergency | [![GitHub](https://img.shields.io/github/stars/liuyang-ict/SAP-DETR?style=flat)](https://github.com/liuyang-ict/SAP-DETR)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_SAP-DETR_Bridging_the_Gap_Between_Salient_Points_and_Queries-Based_Transformer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.02006-b31b1b.svg)](https://arxiv.org/abs/2211.02006) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=QHpmreI7-l8) |
| An Erudite Fine-Grained Visual Classification Model | [![GitHub](https://img.shields.io/github/stars/PRIS-CV/An-Erudite-FGVC-Model?style=flat)](https://github.com/PRIS-CV/An-Erudite-FGVC-Model)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chang_An_Erudite_Fine-Grained_Visual_Classification_Model_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=glFOUKkL8NU) |
| Self-Supervised Implicit Glyph Attention for Text Recognition | [![GitHub](https://img.shields.io/github/stars/TongkunGuan/SIGA?style=flat)](https://github.com/TongkunGuan/SIGA)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Guan_Self-Supervised_Implicit_Glyph_Attention_for_Text_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2203.03382-b31b1b.svg)](http://arxiv.org/abs/2203.03382) | :heavy_minus_sign: |
| Multi-View Adversarial Discriminator: Mine the Non-Causal Factors for Object Detection in Unseen Domains <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() |  [![GitHub](https://img.shields.io/github/stars/K2OKOH/MAD?style=flat)](https://github.com/K2OKOH/MAD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Multi-View_Adversarial_Discriminator_Mine_the_Non-Causal_Factors_for_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02950-b31b1b.svg)](http://arxiv.org/abs/2304.02950) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JnJc-89on2Y) |
| HIER: Metric Learning Beyond Class Labels via Hierarchical Regularization | [![GitHub](https://img.shields.io/github/stars/tjddus9597/HIER-CVPR23?style=flat)](https://github.com/tjddus9597/HIER-CVPR23)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kim_HIER_Metric_Learning_Beyond_Class_Labels_via_Hierarchical_Regularization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.14258-b31b1b.svg)](http://arxiv.org/abs/2212.14258) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LgB3_UkJqcE) |
| DSVT: Dynamic Sparse Voxel Transformer With Rotated Sets | [![GitHub](https://img.shields.io/github/stars/Haiyang-W/DSVT?style=flat)](https://github.com/Haiyang-W/DSVT)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_DSVT_Dynamic_Sparse_Voxel_Transformer_With_Rotated_Sets_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.06051-b31b1b.svg)](http://arxiv.org/abs/2301.06051)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jg7iruprqbU) |
| Progressive Semantic-Visual Mutual Adaption for Generalized Zero-Shot Learning <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/ManLiuCoder/PSVMA?style=flat)](https://github.com/ManLiuCoder/PSVMA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_Progressive_Semantic-Visual_Mutual_Adaption_for_Generalized_Zero-Shot_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15322-b31b1b.svg)](http://arxiv.org/abs/2303.15322) | :heavy_minus_sign: |
| Fake It Till You Make It: Learning Transferable Representations From Synthetic ImageNet Clones| [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://europe.naverlabs.com/research/computer-vision/imagenet-sd/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sariyildiz_Fake_It_Till_You_Make_It_Learning_Transferable_Representations_From_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08420-b31b1b.svg)](https://arxiv.org/abs/2212.08420) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=C8cCvXBNvCI) |
| FFF: Fragment-Guided Flexible Fitting for Building Complete Protein Structures | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_FFF_Fragment-Guided_Flexible_Fitting_for_Building_Complete_Protein_Structures_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.03654-b31b1b.svg)](https://arxiv.org/abs/2308.03654) | :heavy_minus_sign: |
| Revisiting Self-Similarity: Structural Embedding for Image Retrieval | [![GitHub](https://img.shields.io/github/stars/sungonce/SENet?style=flat)](https://github.com/sungonce/SENet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lee_Revisiting_Self-Similarity_Structural_Embedding_for_Image_Retrieval_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=1_Kqu25roa4) |
| Neural Koopman Pooling: Control-Inspired Temporal Dynamics Encoding for Skeleton-Based Action Recognition | [![GitHub](https://img.shields.io/github/stars/Infinitywxh/Neural_Koopman_pooling?style=flat)](https://github.com/Infinitywxh/Neural_Koopman_pooling) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Neural_Koopman_Pooling_Control-Inspired_Temporal_Dynamics_Encoding_for_Skeleton-Based_Action_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| MixTeacher: Mining Promising Labels With Mixed Scale Teacher for Semi-Supervised Object Detection | [![GitHub](https://img.shields.io/github/stars/lliuz/MixTeacher?style=flat)](https://github.com/lliuz/MixTeacher) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_MixTeacher_Mining_Promising_Labels_With_Mixed_Scale_Teacher_for_Semi-Supervised_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09061-b31b1b.svg)](http://arxiv.org/abs/2303.09061) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mSVL2fWsgzM) |
| Learning Attention As Disentangler for Compositional Zero-Shot Learning | [![GitHub](https://img.shields.io/github/stars/haoosz/ade-czsl?style=flat)](https://github.com/haoosz/ade-czsl) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hao_Learning_Attention_As_Disentangler_for_Compositional_Zero-Shot_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15111-b31b1b.svg)](http://arxiv.org/abs/2303.15111) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=s4PzRrClyb4) |
| Towards Building Self-Aware Object Detectors via Reliable Uncertainty Quantification and Calibration | [![GitHub](https://img.shields.io/github/stars/fiveai/saod?style=flat)](https://github.com/fiveai/saod) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Oksuz_Towards_Building_Self-Aware_Object_Detectors_via_Reliable_Uncertainty_Quantification_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.00934-b31b1b.svg)](https://arxiv.org/abs/2307.00934) | :heavy_minus_sign: |
| Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection | [![GitHub](https://img.shields.io/github/stars/LutingWang/OADP?style=flat)](https://github.com/LutingWang/OADP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Object-Aware_Distillation_Pyramid_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05892-b31b1b.svg)](http://arxiv.org/abs/2303.05892) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Znu5boUUq_g) |
| SOOD: Towards Semi-Supervised Oriented Object Detection | [![GitHub](https://img.shields.io/github/stars/HamPerdredes/SOOD?style=flat)](https://github.com/HamPerdredes/SOOD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hua_SOOD_Towards_Semi-Supervised_Oriented_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04515-b31b1b.svg)](http://arxiv.org/abs/2304.04515) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6Yn_Y4ghjhU) |
| Bias-Eliminating Augmentation Learning for Debiased Federated Learning |  :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Bias-Eliminating_Augmentation_Learning_for_Debiased_Federated_Learning_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors| [![GitHub](https://img.shields.io/github/stars/ZhangGongjie/IMFA?style=flat)](https://github.com/ZhangGongjie/IMFA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Towards_Efficient_Use_of_Multi-Scale_Features_in_Transformer-Based_Object_Detectors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.11356-b31b1b.svg)](http://arxiv.org/abs/2208.11356) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9Aq0zefSM9A) |
| AsyFOD: An Asymmetric Adaptation Paradigm for Few-Shot Domain Adaptive Object Detection |  |  |  |
| CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching |  |  |  |
| Explicit Boundary Guided Semi-Push-Pull Contrastive Learning for Supervised Anomaly Detection |  |  |  |
| Disentangled Representation Learning for Unsupervised Neural Quantization |  |  |  |
| YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors |  |  |  |
| Virtual Sparse Convolution for Multimodal 3D Object Detection |  |  |  |
| TranSG: Transformer-based Skeleton Graph Prototype Contrastive Learning with Structure-Trajectory Prompted Reconstruction for Person Re-Identification |  |  |  |
| Adaptive Sparse Pairwise Loss for Object Re-Identification |  |  |  |
| Multi-Granularity Archaeological Dating of Chinese Bronze Dings based on a Knowledge-guided Relation Graph |  |  |  |
| Event-guided Person Re-Identification via Sparse-Dense Complementary Learning |  |  |  |
| Vector Quantization with Self-Attention for Quality-Independent Representation Learning |  |  |  |
| Siamese Image Modeling for Self-Supervised Vision Representation Learning |  |  |  |
| FCC: Feature Clusters Compression for Long-tailed Visual Recognition |  |  |  |
| Towards All-in-One Pre-Training via Maximizing Multi-Modal Mutual Information |  |  |  |
| Soft Augmentation for Image Classification |  |  |  |
| Correspondence Transformers with Asymmetric Feature Learning and Matching Flow Super-Resolution |  |  |  |
| Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models |  |  |  |
| Out-of-Distributed Semantic Pruning for Robust Semi-Supervised Learning |  |  |  |
| Glocal Energy-based Learning for Few-Shot Open-Set Recognition |  |  |  |
| Improving Image Recognition by Retrieving from Web-Scale Image-Text Data |  |  |  |
| Deep Factorized Metric Learning |  |  |  |
| Learning to Detect and Segment for Open Vocabulary Object Detection |  |  |  |
| ConQueR: Query Contrast Voxel-DETR for 3D Object Detection |  |  |  |
| Photo Pre-Training, But for Sketch |  |  |  |
| InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions |  |  |  |
| Detecting Everything in the Open World: Towards Universal Object Detection |  |  |  |
| Twin Contrastive Learning with Noisy Labels |  |  |  |
| Feature Aggregated Queries for Transformer-based Video Object Detectors |  |  |  |
| Learning on Gradients: Generalized Artifacts Representation for GAN-Generated Images Detection |  |  |  |
| Deep Hashing with Minimal-Distance-Separated Hash Centers |  |  |  |
| Knowledge Combination to Learn Rotated Detection without Rotated Annotation |  |  |  |
| Good is Bad: Causality Inspired Cloth-Debiasing for Cloth-Changing Person Re-Identification |  |  |  |
| Discriminating Known from Unknown Objects via Structure-Enhanced Recurrent Variational AutoEncoder |  |  |  |
| 2PCNet: Two-Phase Consistency Training for Day-to-Night Unsupervised Domain Adaptive Object Detection |  |  |  |
| LINe: Out-of-Distribution Detection by Leveraging Important Neurons |  |  |  |
| Progressive Transformation Learning for Leveraging Virtual Images in Training |  |  |  |
| Instance Relation Graph Guided Source-Free Domain Adaptive Object Detection |  |  |  |
| Decoupling MaxLogit for Out-of-Distribution Detection |  |  |  |
| Pixels, Regions, and Objects: Multiple Enhancement for Salient Object Detection |  |  |  |
| Detection Hub: Unifying Object Detection Datasets via Query Adaptation on Language Embedding |  |  |  |
| BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision |  |  |  |
| D<sup>2</sup>Former: Jointly Learning Hierarchical Detectors and Contextual Descriptors via Agent-based Transformers |  |  |  |
| CapDet: Unifying Dense Captioning and Open-World Detection Pretraining |  |  |  |
| Mapping Degeneration Meets Label Evolution: Learning Infrared Small Target Detection with Single Point Supervision |  |  |  |
| Generalized UAV Object Detection via Frequency Domain Disentanglement |  |  |  |
| Deep Frequency Filtering for Domain Generalization |  |  |  |
| Adaptive Sparse Convolutional Networks with Global Context Enhancement for Faster Object Detection on Drone Images |  |  |  |
| Improved Test-Time Adaptation for Domain Generalization |  |  |  |
| Matching Is Not Enough: A Two-Stage Framework for Category-Agnostic Pose Estimation |  |  |  |
| Recurrence without Recurrence: Stable Video Landmark Detection with Deep Equilibrium Models |  |  |  |
| VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision |  |  |  |
| DETRs with Hybrid Matching |  |  |  |
| Query-Dependent Video Representation for Moment Retrieval and Highlight Detection |  |  |  |
| Clothing-Change Feature Augmentation for Person Re-Identification |  |  |  |
| Learning Attribute and Class-Specific Representation Duet for Fine-grained Fashion Analysis |  |  |  |
| Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks |  |  |  |
| Optimal Proposal Learning for Deployable End-to-End Pedestrian Detection |  |  |  |
| DynamicDet: A Unified Dynamic Architecture for Object Detection |  |  |  |
| Switchable Representation Learning Framework with Self-Compatibility |  |  |  |
| DATE: Domain Adaptive Product Seeker for E-Commerce |  |  |  |
| PromptCAL: Contrastive Affinity Learning via Auxiliary Prompts for Generalized Novel Category Discovery |  |  |  |
| Dynamic Neural Network for Multi-Task Learning Searching across Diverse Network Topologies |  |  |  |
| OvarNet: Towards Open-Vocabulary Object Attribute Recognition |  |  |  |
| HOICLIP: Efficient Knowledge Transfer for HOI Detection with Vision-Language Models |  |  |  |
| Learning from Noisy Labels with Decoupled Meta Label Purifier |  |  |  |
| A Light Touch Approach to Teaching Transformers Multi-View Geometry |  |  |  |
| OpenMix: Exploring Outlier Samples for Misclassification Detection |  |  |  |
| Revisiting Reverse Distillation for Anomaly Detection |  |  |  |
| PROB: Probabilistic Objectness for Open World Object Detection |  |  |  |
| Equiangular Basis Vectors |  |  |  |
| Weakly Supervised Posture Mining for Fine-grained Classification |  |  |  |
| An Actor-Centric Causality Graph for Asynchronous Temporal Inference in Group Activity |  |  |  |
| Weak-Shot Object Detection through Mutual Knowledge Transfer |  |  |  |
| Zero-Shot Everything Sketch-based Image Retrieval, and in Explainable Style |  |  |  |
| Exploring Structured Semantic Prior for Multi Label Recognition with Incomplete Labels |  |  |  |
| Learning Partial Correlation based Deep Visual Representation for Image Classification |  |  |  |
| Boundary-aware Backward-Compatible Representation via Adversarial Learning in Image Retrieval |  |  |  |
| PHA: Patch-Wise High-Frequency Augmentation for Transformer-based Person Re-Identification |  |  |  |
| Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects |  |  |  |
| BoxTeacher: Exploring High-Quality Pseudo Labels for Weakly Supervised Instance Segmentation |  |  |  |
| Annealing-based Label-Transfer Learning for Open World Object Detection |  |  |  |
| Diversity-Measurable Anomaly Detection |  |  |  |
| Recurrent Vision Transformers for Object Detection with Event Cameras |  |  |  |
| AShapeFormer: Semantics-guided Object-Level Active Shape Encoding for 3D Object Detection via Transformers |  |  |  |
| Ranking Regularization for Critical Rare Classes: Minimizing False Positives at a High True Positive Rate |  |  |  |
| Contrastive Mean Teacher for Domain Adaptive Object Detectors |  |  |  |
| Bridging the Gap between Model Explanations in Partially Annotated Multi-Label Classification |  |  |  |
| PartMix: Regularization Strategy to Learn Part Discovery for Visible-Infrared Person Re-Identification |  |  |  |
| BiasAdv: Bias-Adversarial Augmentation for Model Debiasing |  |  |  |
| ViPLO: Vision Transformer based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection |  |  |  |
| Robust 3D Shape Classification via Non-Local Graph Attention Network |  |  |  |
| Two-Way Multi-Label Loss |  |  |  |
| Normalizing Flow based Feature Synthesis for Outlier-Aware Object Detection |  |  |  |
| Object Detection with Self-Supervised Scene Adaptation |  |  |  |
| Data-Efficient Large Scale Place Recognition with Graded Similarity Supervision |  |  |  |
| Generating Features with Increased Crop-related Diversity for Few-Shot Object Detection |  |  |  |
| Recognizing Rigid Patterns of Unlabeled Point Clouds by Complete and Continuous Isometry Invariants with no False Negatives and no False Positives |  |  |  |
| Deep Semi-Supervised Metric Learning with Mixed Label Propagation |  |  |  |
| Fine-grained Classification with Noisy Labels |  |  |  |
