# CVPR-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/2023/main/transfer-meta-low-shot-continual-or-long-tail-learning.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/2023/main/vision-language-and-reasoning.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Recognition: Categorization, Detection, Retrieval

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| <i>R<sup>2</sup></i>Former: Unified Retrieval and Reranking Transformer for Place Recognition | [![GitHub](https://img.shields.io/github/stars/Jeff-Zilence/R2Former?style=flat)](https://github.com/Jeff-Zilence/R2Former)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhu_R2Former_Unified_Retrieval_and_Reranking_Transformer_for_Place_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03410-b31b1b.svg)](https://arxiv.org/abs/2304.03410)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=suF-Yc2yxY4&list=PLd3hlSJsX_Ikm5jUWxEWUHAmUUq55lJDQ&index=8) |
| Mask-Free OVIS: Open-Vocabulary Instance Segmentation Without Manual Mask Annotations | [![GitHub](https://img.shields.io/github/stars/Vibashan/Maskfree-OVIS?style=flat)](https://github.com/Vibashan/Maskfree-OVIS)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/VS_Mask-Free_OVIS_Open-Vocabulary_Instance_Segmentation_Without_Manual_Mask_Annotations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16891-b31b1b.svg)](http://arxiv.org/abs/2303.16891) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7P7PX3gd14I) |
| StructVPR: Distill Structural Knowledge With Weighting Samples for Visual Place Recognition| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shen_StructVPR_Distill_Structural_Knowledge_With_Weighting_Samples_for_Visual_Place_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.00937-b31b1b.svg)](http://arxiv.org/abs/2212.00937) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LgdWnBv8Nc4) |
| MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining | [![GitHub](https://img.shields.io/github/stars/LightDXY/MaskCLIP?style=flat)](https://github.com/LightDXY/MaskCLIP)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Dong_MaskCLIP_Masked_Self-Distillation_Advances_Contrastive_Language-Image_Pretraining_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.12262-b31b1b.svg)](http://arxiv.org/abs/2208.12262) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2PPiLDTtVzQ) |
| One-to-Few Label Assignment for End-to-End Dense Detection | [![GitHub](https://img.shields.io/github/stars/strongwolf/o2f?style=flat)](https://github.com/strongwolf/o2f)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_One-to-Few_Label_Assignment_for_End-to-End_Dense_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11567-b31b1b.svg)](http://arxiv.org/abs/2303.11567) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2v7i-1TltBs) |
| Where Is My Wallet? Modeling Object Proposal Sets for Egocentric Visual Query Localization | [![GitHub](https://img.shields.io/github/stars/facebookresearch/vq2d_cvpr?style=flat)](https://github.com/facebookresearch/vq2d_cvpr)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Where_Is_My_Wallet_Modeling_Object_Proposal_Sets_for_Egocentric_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.10528-b31b1b.svg)](http://arxiv.org/abs/2211.10528) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2TU4TWD6FmM) |
| Semi-DETR: Semi-Supervised Object Detection With Detection Transformers | [![GitHub](https://img.shields.io/github/stars/JCZ404/Semi-DETR?style=flat)](https://github.com/JCZ404/Semi-DETR)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Semi-DETR_Semi-Supervised_Object_Detection_With_Detection_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08095-b31b1b.svg)](https://arxiv.org/abs/2307.08095) | :heavy_minus_sign: |
| Universal Instance Perception As Object Discovery and Retrieval | [![GitHub](https://img.shields.io/github/stars/MasterBin-IIAU/UNINEXT?style=flat)](https://github.com/MasterBin-IIAU/UNINEXT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yan_Universal_Instance_Perception_As_Object_Discovery_and_Retrieval_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06674-b31b1b.svg)](http://arxiv.org/abs/2303.06674) | :heavy_minus_sign: |
| CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ma_CAT_LoCalization_and_IdentificAtion_Cascade_Detection_Transformer_for_Open-World_Object_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.01970-b31b1b.svg)](http://arxiv.org/abs/2301.01970) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pYiWsGlg_n8) |
| Phase-Shifting Coder: Predicting Accurate Orientation in Oriented Object Detection|  [![GitHub](https://img.shields.io/github/stars/open-mmlab/mmrotate?style=flat)](https://github.com/open-mmlab/mmrotate)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_Phase-Shifting_Coder_Predicting_Accurate_Orientation_in_Oriented_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.06368-b31b1b.svg)](http://arxiv.org/abs/2211.06368) | :heavy_minus_sign: |
| FrustumFormer: Adaptive Instance-Aware Resampling for Multi-View 3D Detection | [![GitHub](https://img.shields.io/github/stars/Robertwyq/Frustum?style=flat)](https://github.com/Robertwyq/Frustum)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_FrustumFormer_Adaptive_Instance-Aware_Resampling_for_Multi-View_3D_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.04467-b31b1b.svg)](http://arxiv.org/abs/2301.04467) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vQTxzc3Z5uQ) |
| Box-Level Active Detection <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/lyumengyao/blad?style=flat)](https://github.com/lyumengyao/blad) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lyu_Box-Level_Active_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13089-b31b1b.svg)](http://arxiv.org/abs/2303.13089) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_3lfkJpltUc) |
| Learning With Noisy Labels via Self-Supervised Adversarial Noisy Masking | [![GitHub](https://img.shields.io/github/stars/yuanpengtu/SANM?style=flat)](https://github.com/yuanpengtu/SANM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tu_Learning_With_Noisy_Labels_via_Self-Supervised_Adversarial_Noisy_Masking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.06805-b31b1b.svg)](http://arxiv.org/abs/2302.06805) | :heavy_minus_sign: |
| Ambiguity-Resistant Semi-Supervised Learning for Dense Object Detection | [![GitHub](https://img.shields.io/github/stars/PaddlePaddle/PaddleDetection?style=flat)](https://github.com/PaddlePaddle/PaddleDetection) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_Ambiguity-Resistant_Semi-Supervised_Learning_for_Dense_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14960-b31b1b.svg)](http://arxiv.org/abs/2303.14960) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=aVXygEf7Z7M) |
| Aligning Bag of Regions for Open-Vocabulary Object Detection | [![GitHub](https://img.shields.io/github/stars/wusize/ovdet?style=flat)](https://github.com/wusize/ovdet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Aligning_Bag_of_Regions_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.13996-b31b1b.svg)](http://arxiv.org/abs/2302.13996) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kXZmWiHsUW0) |
| Asymmetric Feature Fusion for Image Retrieval | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Asymmetric_Feature_Fusion_for_Image_Retrieval_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2403.00671-b31b1b.svg)](https://arxiv.org/abs/2403.00671) | :heavy_minus_sign: |
| 3D Video Object Detection With Learnable Object-Centric Global Optimization | [![GitHub](https://img.shields.io/github/stars/jiaweihe1996/BA-Det?style=flat)](https://github.com/jiaweihe1996/BA-Det) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/He_3D_Video_Object_Detection_With_Learnable_Object-Centric_Global_Optimization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15416-b31b1b.svg)](http://arxiv.org/abs/2303.15416) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=m07Yzg9f-_g) |
| Enhanced Training of Query-Based Object Detection via Selective Query Recollection | [![GitHub](https://img.shields.io/github/stars/Fangyi-Chen/SQR?style=flat)](https://github.com/Fangyi-Chen/SQR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Enhanced_Training_of_Query-Based_Object_Detection_via_Selective_Query_Recollection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.07593-b31b1b.svg)](http://arxiv.org/abs/2212.07593) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=UIAZ75ov0ko) |
| Dense Distinct Query for End-to-End Object Detection | [![GitHub](https://img.shields.io/github/stars/jshilong/DDQ?style=flat)](https://github.com/jshilong/DDQ) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Dense_Distinct_Query_for_End-to-End_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12776-b31b1b.svg)](http://arxiv.org/abs/2303.12776) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Lwcp910FxKg) |
| On-the-Fly Category Discovery | [![GitHub](https://img.shields.io/github/stars/PRIS-CV/On-the-fly-Category-Discovery?style=flat)](https://github.com/PRIS-CV/On-the-fly-Category-Discovery) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Du_On-the-Fly_Category_Discovery_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kAH4pX0tcNg) |
| ProD: Prompting-To-Disentangle Domain Knowledge for Cross-Domain Few-Shot Image Classification | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ma_ProD_Prompting-To-Disentangle_Domain_Knowledge_for_Cross-Domain_Few-Shot_Image_Classification_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Q-DETR: An Efficient Low-Bit Quantized Detection Transformer <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/SteveTsui/Q-DETR?style=flat)](https://github.com/SteveTsui/Q-DETR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Q-DETR_An_Efficient_Low-Bit_Quantized_Detection_Transformer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00253-b31b1b.svg)](https://arxiv.org/abs/2304.00253) | :heavy_minus_sign: |
| SAP-DETR: Bridging the Gap Between Salient Points and Queries-Based Transformer Detector for Fast Model Convergency | [![GitHub](https://img.shields.io/github/stars/liuyang-ict/SAP-DETR?style=flat)](https://github.com/liuyang-ict/SAP-DETR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_SAP-DETR_Bridging_the_Gap_Between_Salient_Points_and_Queries-Based_Transformer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.02006-b31b1b.svg)](https://arxiv.org/abs/2211.02006) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=QHpmreI7-l8) |
| An Erudite Fine-Grained Visual Classification Model | [![GitHub](https://img.shields.io/github/stars/PRIS-CV/An-Erudite-FGVC-Model?style=flat)](https://github.com/PRIS-CV/An-Erudite-FGVC-Model)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chang_An_Erudite_Fine-Grained_Visual_Classification_Model_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=glFOUKkL8NU) |
| Self-Supervised Implicit Glyph Attention for Text Recognition | [![GitHub](https://img.shields.io/github/stars/TongkunGuan/SIGA?style=flat)](https://github.com/TongkunGuan/SIGA)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Guan_Self-Supervised_Implicit_Glyph_Attention_for_Text_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2203.03382-b31b1b.svg)](http://arxiv.org/abs/2203.03382) | :heavy_minus_sign: |
| Multi-View Adversarial Discriminator: Mine the Non-Causal Factors for Object Detection in Unseen Domains <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/K2OKOH/MAD?style=flat)](https://github.com/K2OKOH/MAD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Multi-View_Adversarial_Discriminator_Mine_the_Non-Causal_Factors_for_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02950-b31b1b.svg)](http://arxiv.org/abs/2304.02950) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JnJc-89on2Y) |
| HIER: Metric Learning Beyond Class Labels via Hierarchical Regularization | [![GitHub](https://img.shields.io/github/stars/tjddus9597/HIER-CVPR23?style=flat)](https://github.com/tjddus9597/HIER-CVPR23)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kim_HIER_Metric_Learning_Beyond_Class_Labels_via_Hierarchical_Regularization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.14258-b31b1b.svg)](http://arxiv.org/abs/2212.14258) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LgB3_UkJqcE) |
| DSVT: Dynamic Sparse Voxel Transformer With Rotated Sets | [![GitHub](https://img.shields.io/github/stars/Haiyang-W/DSVT?style=flat)](https://github.com/Haiyang-W/DSVT)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_DSVT_Dynamic_Sparse_Voxel_Transformer_With_Rotated_Sets_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.06051-b31b1b.svg)](http://arxiv.org/abs/2301.06051)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jg7iruprqbU) |
| Progressive Semantic-Visual Mutual Adaption for Generalized Zero-Shot Learning <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/ManLiuCoder/PSVMA?style=flat)](https://github.com/ManLiuCoder/PSVMA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_Progressive_Semantic-Visual_Mutual_Adaption_for_Generalized_Zero-Shot_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15322-b31b1b.svg)](http://arxiv.org/abs/2303.15322) | :heavy_minus_sign: |
| Fake It Till You Make It: Learning Transferable Representations From Synthetic ImageNet Clones| [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://europe.naverlabs.com/research/computer-vision/imagenet-sd/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sariyildiz_Fake_It_Till_You_Make_It_Learning_Transferable_Representations_From_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08420-b31b1b.svg)](https://arxiv.org/abs/2212.08420) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=C8cCvXBNvCI) |
| FFF: Fragment-Guided Flexible Fitting for Building Complete Protein Structures | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_FFF_Fragment-Guided_Flexible_Fitting_for_Building_Complete_Protein_Structures_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.03654-b31b1b.svg)](https://arxiv.org/abs/2308.03654) | :heavy_minus_sign: |
| Revisiting Self-Similarity: Structural Embedding for Image Retrieval | [![GitHub](https://img.shields.io/github/stars/sungonce/SENet?style=flat)](https://github.com/sungonce/SENet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lee_Revisiting_Self-Similarity_Structural_Embedding_for_Image_Retrieval_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=1_Kqu25roa4) |
| Neural Koopman Pooling: Control-Inspired Temporal Dynamics Encoding for Skeleton-Based Action Recognition | [![GitHub](https://img.shields.io/github/stars/Infinitywxh/Neural_Koopman_pooling?style=flat)](https://github.com/Infinitywxh/Neural_Koopman_pooling) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Neural_Koopman_Pooling_Control-Inspired_Temporal_Dynamics_Encoding_for_Skeleton-Based_Action_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| MixTeacher: Mining Promising Labels With Mixed Scale Teacher for Semi-Supervised Object Detection | [![GitHub](https://img.shields.io/github/stars/lliuz/MixTeacher?style=flat)](https://github.com/lliuz/MixTeacher) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_MixTeacher_Mining_Promising_Labels_With_Mixed_Scale_Teacher_for_Semi-Supervised_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09061-b31b1b.svg)](http://arxiv.org/abs/2303.09061) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mSVL2fWsgzM) |
| Learning Attention As Disentangler for Compositional Zero-Shot Learning | [![GitHub](https://img.shields.io/github/stars/haoosz/ade-czsl?style=flat)](https://github.com/haoosz/ade-czsl) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hao_Learning_Attention_As_Disentangler_for_Compositional_Zero-Shot_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15111-b31b1b.svg)](http://arxiv.org/abs/2303.15111) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=s4PzRrClyb4) |
| Towards Building Self-Aware Object Detectors via Reliable Uncertainty Quantification and Calibration | [![GitHub](https://img.shields.io/github/stars/fiveai/saod?style=flat)](https://github.com/fiveai/saod) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Oksuz_Towards_Building_Self-Aware_Object_Detectors_via_Reliable_Uncertainty_Quantification_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.00934-b31b1b.svg)](https://arxiv.org/abs/2307.00934) | :heavy_minus_sign: |
| Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection | [![GitHub](https://img.shields.io/github/stars/LutingWang/OADP?style=flat)](https://github.com/LutingWang/OADP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Object-Aware_Distillation_Pyramid_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05892-b31b1b.svg)](http://arxiv.org/abs/2303.05892) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Znu5boUUq_g) |
| SOOD: Towards Semi-Supervised Oriented Object Detection | [![GitHub](https://img.shields.io/github/stars/HamPerdredes/SOOD?style=flat)](https://github.com/HamPerdredes/SOOD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hua_SOOD_Towards_Semi-Supervised_Oriented_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04515-b31b1b.svg)](http://arxiv.org/abs/2304.04515) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6Yn_Y4ghjhU) |
| Bias-Eliminating Augmentation Learning for Debiased Federated Learning |  :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Bias-Eliminating_Augmentation_Learning_for_Debiased_Federated_Learning_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors| [![GitHub](https://img.shields.io/github/stars/ZhangGongjie/IMFA?style=flat)](https://github.com/ZhangGongjie/IMFA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Towards_Efficient_Use_of_Multi-Scale_Features_in_Transformer-Based_Object_Detectors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.11356-b31b1b.svg)](http://arxiv.org/abs/2208.11356) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9Aq0zefSM9A) |
| AsyFOD: An Asymmetric Adaptation Paradigm for Few-Shot Domain Adaptive Object Detection | [![GitHub](https://img.shields.io/github/stars/Hlings/AsyFOD?style=flat)](https://github.com/Hlings/AsyFOD)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_AsyFOD_An_Asymmetric_Adaptation_Paradigm_for_Few-Shot_Domain_Adaptive_Object_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=1K4WE14sNLk) |
| CORA: Adapting CLIP for Open-Vocabulary Detection With Region Prompting and Anchor Pre-Matching | [![GitHub](https://img.shields.io/github/stars/tgxs002/CORA?style=flat)](https://github.com/tgxs002/CORA)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_CORA_Adapting_CLIP_for_Open-Vocabulary_Detection_With_Region_Prompting_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13076-b31b1b.svg)](http://arxiv.org/abs/2303.13076) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9-J7aArp4do) |
| Explicit Boundary Guided Semi-Push-Pull Contrastive Learning for Supervised Anomaly Detection | [![GitHub](https://img.shields.io/github/stars/xcyao00/BGAD?style=flat)](https://github.com/xcyao00/BGAD)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yao_Explicit_Boundary_Guided_Semi-Push-Pull_Contrastive_Learning_for_Supervised_Anomaly_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2207.01463-b31b1b.svg)](http://arxiv.org/abs/2207.01463) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=SlxVA5ehlvc) |
| Disentangled Representation Learning for Unsupervised Neural Quantization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Noh_Disentangled_Representation_Learning_for_Unsupervised_Neural_Quantization_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors | [![GitHub](https://img.shields.io/github/stars/WongKinYiu/yolov7?style=flat)](https://github.com/WongKinYiu/yolov7)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_YOLOv7_Trainable_Bag-of-Freebies_Sets_New_State-of-the-Art_for_Real-Time_Object_Detectors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2207.02696-b31b1b.svg)](http://arxiv.org/abs/2207.02696) | :heavy_minus_sign: |
| Virtual Sparse Convolution for Multimodal 3D Object Detection | [![GitHub](https://img.shields.io/github/stars/hailanyi/VirConv?style=flat)](https://github.com/hailanyi/VirConv)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Virtual_Sparse_Convolution_for_Multimodal_3D_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02314-b31b1b.svg)](http://arxiv.org/abs/2303.02314)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=T8Hg7ELW2HE) |
| TranSG: Transformer-Based Skeleton Graph Prototype Contrastive Learning With Structure-Trajectory Prompted Reconstruction for Person Re-Identification | [![GitHub](https://img.shields.io/github/stars/Kali-Hac/TranSG?style=flat)](https://github.com/Kali-Hac/TranSG)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Rao_TranSG_Transformer-Based_Skeleton_Graph_Prototype_Contrastive_Learning_With_Structure-Trajectory_Prompted_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06819-b31b1b.svg)](http://arxiv.org/abs/2303.06819) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jKLa36jPg-M) |
| Adaptive Sparse Pairwise Loss for Object Re-Identification | [![GitHub](https://img.shields.io/github/stars/Astaxanthin/AdaSP?style=flat)](https://github.com/Astaxanthin/AdaSP)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhou_Adaptive_Sparse_Pairwise_Loss_for_Object_Re-Identification_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.18247-b31b1b.svg)](http://arxiv.org/abs/2303.18247) | :heavy_minus_sign: |
| Multi-Granularity Archaeological Dating of Chinese Bronze Dings Based on a Knowledge-Guided Relation Graph | [![GitHub](https://img.shields.io/github/stars/zhourixin/bronze-Ding?style=flat)](https://github.com/zhourixin/bronze-Ding) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhou_Multi-Granularity_Archaeological_Dating_of_Chinese_Bronze_Dings_Based_on_a_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15266-b31b1b.svg)](http://arxiv.org/abs/2303.15266) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nsfQUozrMCE) |
| Event-Guided Person Re-Identification via Sparse-Dense Complementary Learning | [![GitHub](https://img.shields.io/github/stars/Chengzhi-Cao/SDCL?style=flat)](https://github.com/Chengzhi-Cao/SDCL)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cao_Event-Guided_Person_Re-Identification_via_Sparse-Dense_Complementary_Learning_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Vector Quantization With Self-Attention for Quality-Independent Representation Learning | [![GitHub](https://img.shields.io/github/stars/yangzhou321/VQSA?style=flat)](https://github.com/yangzhou321/VQSA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_Vector_Quantization_With_Self-Attention_for_Quality-Independent_Representation_Learning_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=IiBvFBnsYCE) |
| Siamese Image Modeling for Self-Supervised Vision Representation Learning | [![GitHub](https://img.shields.io/github/stars/fundamentalvision/Siamese-Image-Modeling?style=flat)](https://github.com/fundamentalvision/Siamese-Image-Modeling) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tao_Siamese_Image_Modeling_for_Self-Supervised_Vision_Representation_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.01204-b31b1b.svg)](http://arxiv.org/abs/2206.01204) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ULMWPspu2ek) |
| FCC: Feature Clusters Compression for Long-Tailed Visual Recognition | [![GitHub](https://img.shields.io/github/stars/lijian16/FCC?style=flat)](https://github.com/lijian16/FCC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_FCC_Feature_Clusters_Compression_for_Long-Tailed_Visual_Recognition_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=fE3rmOSreHI) |
| Towards All-in-One Pre-Training via Maximizing Multi-Modal Mutual Information | [![GitHub](https://img.shields.io/github/stars/OpenGVLab/M3I-Pretraining?style=flat)](https://github.com/OpenGVLab/M3I-Pretraining) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Su_Towards_All-in-One_Pre-Training_via_Maximizing_Multi-Modal_Mutual_Information_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.09807-b31b1b.svg)](http://arxiv.org/abs/2211.09807) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3O_SPwAJ86Y) |
| Soft Augmentation for Image Classification | [![GitHub](https://img.shields.io/github/stars/youngleox/soft_augmentation?style=flat)](https://github.com/youngleox/soft_augmentation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_Soft_Augmentation_for_Image_Classification_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.04625-b31b1b.svg)](https://arxiv.org/abs/2211.04625) | :heavy_minus_sign: |
| Correspondence Transformers With Asymmetric Feature Learning and Matching Flow Super-Resolution| [![GitHub](https://img.shields.io/github/stars/YXSUNMADMAX/ACTR?style=flat)](https://github.com/YXSUNMADMAX/ACTR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sun_Correspondence_Transformers_With_Asymmetric_Feature_Learning_and_Matching_Flow_Super-Resolution_CVPR_2023_paper.pdf) |  :heavy_minus_sign: |
| Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning With Multimodal Models | [![GitHub](https://img.shields.io/github/stars/linzhiqiu/cross_modal_adaptation?style=flat)](https://github.com/linzhiqiu/cross_modal_adaptation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_Multimodality_Helps_Unimodality_Cross-Modal_Few-Shot_Learning_With_Multimodal_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.06267-b31b1b.svg)](http://arxiv.org/abs/2301.06267) | :heavy_minus_sign: |
| Out-of-Distributed Semantic Pruning for Robust Semi-Supervised Learning | [![GitHub](https://img.shields.io/github/stars/RabbitBoss/Awesome-Realistic-Semi-Supervised-Learning?style=flat)](https://github.com/RabbitBoss/Awesome-Realistic-Semi-Supervised-Learning) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Out-of-Distributed_Semantic_Pruning_for_Robust_Semi-Supervised_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.18158-b31b1b.svg)](https://arxiv.org/abs/2305.18158) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=uGV24BwIRqU) |
| Glocal Energy-Based Learning for Few-Shot Open-Set Recognition | [![GitHub](https://img.shields.io/github/stars/00why00/Glocal?style=flat)](https://github.com/00why00/Glocal) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Glocal_Energy-Based_Learning_for_Few-Shot_Open-Set_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.11855-b31b1b.svg)](http://arxiv.org/abs/2304.11855) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=UD19I7zdKKs) |
| Improving Image Recognition by Retrieving From Web-Scale Image-Text Data| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Iscen_Improving_Image_Recognition_by_Retrieving_From_Web-Scale_Image-Text_Data_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05173-b31b1b.svg)](http://arxiv.org/abs/2304.05173) | :heavy_minus_sign: |
| Deep Factorized Metric Learning | [![GitHub](https://img.shields.io/github/stars/wangck20/DFML?style=flat)](https://github.com/wangck20/DFML)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Deep_Factorized_Metric_Learning_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=u1V_92eiyK0) |
| Learning To Detect and Segment for Open Vocabulary Object Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Learning_To_Detect_and_Segment_for_Open_Vocabulary_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.12130-b31b1b.svg)](http://arxiv.org/abs/2212.12130) | :heavy_minus_sign: |
| ConQueR: Query Contrast Voxel-DETR for 3D Object Detection <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://benjin.me/publication/cvpr2023_conquer/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhu_ConQueR_Query_Contrast_Voxel-DETR_for_3D_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.07289-b31b1b.svg)](http://arxiv.org/abs/2212.07289) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kwHVjA4gIOA) |
| Photo Pre-Training, but for Sketch | [![GitHub](https://img.shields.io/github/stars/KeLi-SketchX/Photo-Pre-Training-But-for-Sketch?style=flat)](https://github.com/KeLi-SketchX/Photo-Pre-Training-But-for-Sketch)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Photo_Pre-Training_but_for_Sketch_CVPR_2023_paper.pdf)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tmCmPZC756E) |
| InternImage: Exploring Large-Scale Vision Foundation Models With Deformable Convolutions | [![GitHub](https://img.shields.io/github/stars/OpenGVLab/InternImage?style=flat)](https://github.com/OpenGVLab/InternImage)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_InternImage_Exploring_Large-Scale_Vision_Foundation_Models_With_Deformable_Convolutions_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.05778-b31b1b.svg)](http://arxiv.org/abs/2211.05778)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_LEitBd5Tfs) |
| Detecting Everything in the Open World: Towards Universal Object Detection | [![GitHub](https://img.shields.io/github/stars/zhenyuw16/UniDetector?style=flat)](https://github.com/zhenyuw16/UniDetector)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Detecting_Everything_in_the_Open_World_Towards_Universal_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11749-b31b1b.svg)](http://arxiv.org/abs/2303.11749)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Wz8p4Edcj6U) |
| Twin Contrastive Learning With Noisy Labels | [![GitHub](https://img.shields.io/github/stars/Hzzone/TCL?style=flat)](https://github.com/Hzzone/TCL)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Twin_Contrastive_Learning_With_Noisy_Labels_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06930-b31b1b.svg)](http://arxiv.org/abs/2303.06930) | :heavy_minus_sign: |
| Feature Aggregated Queries for Transformer-Based Video Object Detectors | [![GitHub](https://img.shields.io/github/stars/YimingCuiCuiCui/FAQ?style=flat)](https://github.com/YimingCuiCuiCui/FAQ)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cui_Feature_Aggregated_Queries_for_Transformer-Based_Video_Object_Detectors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08319-b31b1b.svg)](http://arxiv.org/abs/2303.08319) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=e1iTV5riSdo) |
| Learning on Gradients: Generalized Artifacts Representation for GAN-Generated Images Detection | [![GitHub](https://img.shields.io/github/stars/chuangchuangtan/LGrad?style=flat)](https://github.com/chuangchuangtan/LGrad)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tan_Learning_on_Gradients_Generalized_Artifacts_Representation_for_GAN-Generated_Images_Detection_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-Wa-Si9LZyk) |
| Deep Hashing With Minimal-Distance-Separated Hash Centers| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Deep_Hashing_With_Minimal-Distance-Separated_Hash_Centers_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hy5LkF3yJpI) |
| Knowledge Combination To Learn Rotated Detection Without Rotated Annotation | [![GitHub](https://img.shields.io/github/stars/alanzty/KCR-Official?style=flat)](https://github.com/alanzty/KCR-Official) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhu_Knowledge_Combination_To_Learn_Rotated_Detection_Without_Rotated_Annotation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02199-b31b1b.svg)](http://arxiv.org/abs/2304.02199) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mo7zoII_LwU) |
| Good Is Bad: Causality Inspired Cloth-Debiasing for Cloth-Changing Person Re-Identification | [![GitHub](https://img.shields.io/github/stars/BoomShakaY/AIM-CCReID?style=flat)](https://github.com/BoomShakaY/AIM-CCReID) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_Good_Is_Bad_Causality_Inspired_Cloth-Debiasing_for_Cloth-Changing_Person_Re-Identification_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PbJqw7J7z40) |
| Discriminating Known From Unknown Objects via Structure-Enhanced Recurrent Variational AutoEncoder| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Discriminating_Known_From_Unknown_Objects_via_Structure-Enhanced_Recurrent_Variational_AutoEncoder_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=gcFcDKL9imw) |
| 2PCNet: Two-Phase Consistency Training for Day-to-Night Unsupervised Domain Adaptive Object Detection | [![GitHub](https://img.shields.io/github/stars/mecarill/2pcnet?style=flat)](https://github.com/mecarill/2pcnet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kennerley_2PCNet_Two-Phase_Consistency_Training_for_Day-to-Night_Unsupervised_Domain_Adaptive_Object_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13853-b31b1b.svg)](http://arxiv.org/abs/2303.13853) | :heavy_minus_sign: |
| LINe: Out-of-Distribution Detection by Leveraging Important Neurons | [![GitHub](https://img.shields.io/github/stars/YongHyun-Ahn/LINe-Out-of-Distribution-Detection-by-Leveraging-Important-Neurons?style=flat)](https://github.com/YongHyun-Ahn/LINe-Out-of-Distribution-Detection-by-Leveraging-Important-Neurons) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ahn_LINe_Out-of-Distribution_Detection_by_Leveraging_Important_Neurons_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13995-b31b1b.svg)](http://arxiv.org/abs/2303.13995) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CRqrRGK69Mk) |
| Progressive Transformation Learning for Leveraging Virtual Images in Training <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://gitlab.umiacs.umd.edu/dspcad/ptl-release) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shen_Progressive_Transformation_Learning_for_Leveraging_Virtual_Images_in_Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.01778-b31b1b.svg)](http://arxiv.org/abs/2211.01778) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-P1pyGn-1zw) |
| Instance Relation Graph Guided Source-Free Domain Adaptive Object Detection | [![GitHub](https://img.shields.io/github/stars/Vibashan/irg-sfda?style=flat)](https://github.com/Vibashan/irg-sfda) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/VS_Instance_Relation_Graph_Guided_Source-Free_Domain_Adaptive_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2203.15793-b31b1b.svg)](http://arxiv.org/abs/2203.15793) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=D8-BsMLGrDQ) |
| Decoupling MaxLogit for Out-of-Distribution Detection | [![GitHub](https://img.shields.io/github/stars/Z-ZHHH/CVPR23-DML?style=flat)](https://github.com/Z-ZHHH/CVPR23-DML) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Decoupling_MaxLogit_for_Out-of-Distribution_Detection_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Y2GzLcvMu7M) |
| Pixels, Regions, and Objects: Multiple Enhancement for Salient Object Detection | [![GitHub](https://img.shields.io/github/stars/yiwangtz/MENet?style=flat)](https://github.com/yiwangtz/MENet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Pixels_Regions_and_Objects_Multiple_Enhancement_for_Salient_Object_Detection_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Detection Hub: Unifying Object Detection Datasets via Query Adaptation on Language Embedding| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Meng_Detection_Hub_Unifying_Object_Detection_Datasets_via_Query_Adaptation_on_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.03484-b31b1b.svg)](http://arxiv.org/abs/2206.03484) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=D5yIlLxA5Gk) |
| BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]()  | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_BEVFormer_v2_Adapting_Modern_Image_Backbones_to_Birds-Eye-View_Recognition_via_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.10439-b31b1b.svg)](https://arxiv.org/abs/2211.10439) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=sP_yzc67ykg) |
| D<sup>2</sup>Former: Jointly Learning Hierarchical Detectors and Contextual Descriptors via Agent-Based Transformers | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/He_D2Former_Jointly_Learning_Hierarchical_Detectors_and_Contextual_Descriptors_via_Agent-Based_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| CapDet: Unifying Dense Captioning and Open-World Detection Pretraining | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Long_CapDet_Unifying_Dense_Captioning_and_Open-World_Detection_Pretraining_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02489-b31b1b.svg)](http://arxiv.org/abs/2303.02489) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=luzweBuUS2k) |
| Mapping Degeneration Meets Label Evolution: Learning Infrared Small Target Detection With Single Point Supervision | [![GitHub](https://img.shields.io/github/stars/XinyiYing/LESPS?style=flat)](https://github.com/XinyiYing/LESPS)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ying_Mapping_Degeneration_Meets_Label_Evolution_Learning_Infrared_Small_Target_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01484-b31b1b.svg)](http://arxiv.org/abs/2304.01484) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=sWmBtxv-8k8) |
| Generalized UAV Object Detection via Frequency Domain Disentanglement | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Generalized_UAV_Object_Detection_via_Frequency_Domain_Disentanglement_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=gNlKrXWf8qE) |
| Deep Frequency Filtering for Domain Generalization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_Deep_Frequency_Filtering_for_Domain_Generalization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2203.12198-b31b1b.svg)](http://arxiv.org/abs/2203.12198) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2QawQCl6X7g) |
| Adaptive Sparse Convolutional Networks With Global Context Enhancement for Faster Object Detection on Drone Images | [![GitHub](https://img.shields.io/github/stars/Cuogeihong/CEASC?style=flat)](https://github.com/Cuogeihong/CEASC)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Du_Adaptive_Sparse_Convolutional_Networks_With_Global_Context_Enhancement_for_Faster_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14488-b31b1b.svg)](http://arxiv.org/abs/2303.14488) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FQccbuMghCM) |
| Improved Test-Time Adaptation for Domain Generalization | [![GitHub](https://img.shields.io/github/stars/liangchen527/ITTA?style=flat)](https://github.com/liangchen527/ITTA)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Improved_Test-Time_Adaptation_for_Domain_Generalization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04494-b31b1b.svg)](http://arxiv.org/abs/2304.04494)  | :heavy_minus_sign: |
| Matching Is Not Enough: A Two-Stage Framework for Category-Agnostic Pose Estimation <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/flyinglynx/CapeFormer?style=flat)](https://github.com/flyinglynx/CapeFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shi_Matching_Is_Not_Enough_A_Two-Stage_Framework_for_Category-Agnostic_Pose_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=K87x9-pdSqE) |
| Recurrence Without Recurrence: Stable Video Landmark Detection With Deep Equilibrium Models | [![GitHub](https://img.shields.io/github/stars/polo5/LDEQ_RwR?style=flat)](https://github.com/polo5/LDEQ_RwR)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Micaelli_Recurrence_Without_Recurrence_Stable_Video_Landmark_Detection_With_Deep_Equilibrium_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00600-b31b1b.svg)](http://arxiv.org/abs/2304.00600) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6n95iEFZS_k) |
| VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision | [![GitHub](https://img.shields.io/github/stars/lmy98129/VLPD?style=flat)](https://github.com/lmy98129/VLPD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_VLPD_Context-Aware_Pedestrian_Detection_via_Vision-Language_Semantic_Self-Supervision_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03135-b31b1b.svg)](http://arxiv.org/abs/2304.03135) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZC68kTJ-FSE) |
| DETRs With Hybrid Matching | [![GitHub](https://img.shields.io/github/stars/HDETR/H-Deformable-DETR?style=flat)](https://github.com/HDETR/H-Deformable-DETR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jia_DETRs_With_Hybrid_Matching_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2207.13080-b31b1b.svg)](http://arxiv.org/abs/2207.13080) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3bP8p9D2D1Y) |
| Query-Dependent Video Representation for Moment Retrieval and Highlight Detection | [![GitHub](https://img.shields.io/github/stars/wjun0830/QD-DETR?style=flat)](https://github.com/wjun0830/QD-DETR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Moon_Query-Dependent_Video_Representation_for_Moment_Retrieval_and_Highlight_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13874-b31b1b.svg)](http://arxiv.org/abs/2303.13874) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=df-gtJcZEw8) |
| Clothing-Change Feature Augmentation for Person Re-Identification | [![GitHub](https://img.shields.io/github/stars/wangxiao5791509/Cloth_Change_Person_reID_Paper_List?style=flat)](https://github.com/wangxiao5791509/Cloth_Change_Person_reID_Paper_List) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Han_Clothing-Change_Feature_Augmentation_for_Person_Re-Identification_CVPR_2023_paper.pdf) |  [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=y8scT7MHrXs) |
| Learning Attribute and Class-Specific Representation Duet for Fine-Grained Fashion Analysis | [![GitHub](https://img.shields.io/github/stars/wendashi/Cool-GenAI-Fashion-Papers?style=flat)](https://github.com/wendashi/Cool-GenAI-Fashion-Papers) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jiao_Learning_Attribute_and_Class-Specific_Representation_Duet_for_Fine-Grained_Fashion_Analysis_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=feE07KpJCfo) |
| Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/fundamentalvision/Uni-Perceiver?style=flat)](https://github.com/fundamentalvision/Uni-Perceiver) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Uni-Perceiver_v2_A_Generalist_Model_for_Large-Scale_Vision_and_Vision-Language_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.09808-b31b1b.svg)](http://arxiv.org/abs/2211.09808) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xqjhX1BqPt8) |
| Optimal Proposal Learning for Deployable End-to-End Pedestrian Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Song_Optimal_Proposal_Learning_for_Deployable_End-to-End_Pedestrian_Detection_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| DynamicDet: A Unified Dynamic Architecture for Object Detection | [![GitHub](https://img.shields.io/github/stars/VDIGPKU/DynamicDet?style=flat)](https://github.com/VDIGPKU/DynamicDet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_DynamicDet_A_Unified_Dynamic_Architecture_for_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05552-b31b1b.svg)](http://arxiv.org/abs/2304.05552) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=54ICccjEhiM) |
| Switchable Representation Learning Framework With Self-Compatibility | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Switchable_Representation_Learning_Framework_With_Self-Compatibility_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.08289-b31b1b.svg)](http://arxiv.org/abs/2206.08289) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kn_l9B1gEI0) |
| DATE: Domain Adaptive Product Seeker for E-Commerce| [![GitHub](https://img.shields.io/github/stars/Taobao-live/Product-Seeking?style=flat)](https://github.com/Taobao-live/Product-Seeking) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_DATE_Domain_Adaptive_Product_Seeker_for_E-Commerce_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03669-b31b1b.svg)](https://arxiv.org/abs/2304.03669) | :heavy_minus_sign: |
| PromptCAL: Contrastive Affinity Learning via Auxiliary Prompts for Generalized Novel Category Discovery | [![GitHub](https://img.shields.io/github/stars/sheng-eatamath/PromptCAL?style=flat)](https://github.com/sheng-eatamath/PromptCAL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_PromptCAL_Contrastive_Affinity_Learning_via_Auxiliary_Prompts_for_Generalized_Novel_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.05590-b31b1b.svg)](http://arxiv.org/abs/2212.05590) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=NsltTFVabDs) |
| Dynamic Neural Network for Multi-Task Learning Searching Across Diverse Network Topologies| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Choi_Dynamic_Neural_Network_for_Multi-Task_Learning_Searching_Across_Diverse_Network_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06856-b31b1b.svg)](http://arxiv.org/abs/2303.06856) | :heavy_minus_sign: |
| OvarNet: Towards Open-Vocabulary Object Attribute Recognition | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://kyanchen.github.io/OvarNet/) <br /> [![GitHub](https://img.shields.io/github/stars/KyanChen/OvarNet?style=flat)](https://github.com/KyanChen/OvarNet)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_OvarNet_Towards_Open-Vocabulary_Object_Attribute_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.09506-b31b1b.svg)](http://arxiv.org/abs/2301.09506) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Ak3102VDj84) |
| HOICLIP: Efficient Knowledge Transfer for HOI Detection With Vision-Language Models | [![GitHub](https://img.shields.io/github/stars/Artanic30/HOICLIP?style=flat)](https://github.com/Artanic30/HOICLIP)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ning_HOICLIP_Efficient_Knowledge_Transfer_for_HOI_Detection_With_Vision-Language_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15786-b31b1b.svg)](http://arxiv.org/abs/2303.15786)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=n7LiapvX77Q) |
| Learning From Noisy Labels With Decoupled Meta Label Purifier | [![GitHub](https://img.shields.io/github/stars/yuanpengtu/DMLP?style=flat)](https://github.com/yuanpengtu/DMLP)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tu_Learning_From_Noisy_Labels_With_Decoupled_Meta_Label_Purifier_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.06810-b31b1b.svg)](http://arxiv.org/abs/2302.06810) | :heavy_minus_sign: |
| A Light Touch Approach to Teaching Transformers Multi-View Geometry | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Bhalgat_A_Light_Touch_Approach_to_Teaching_Transformers_Multi-View_Geometry_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.15107-b31b1b.svg)](http://arxiv.org/abs/2211.15107) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=aFP_vfqGIPk) |
| OpenMix: Exploring Outlier Samples for Misclassification Detection <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/Impression2805/OpenMix?style=flat)](https://github.com/Impression2805/OpenMix) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhu_OpenMix_Exploring_Outlier_Samples_for_Misclassification_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17093-b31b1b.svg)](http://arxiv.org/abs/2303.17093) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=q9pa2J0jxQE) |
| Revisiting Reverse Distillation for Anomaly Detection | [![GitHub](https://img.shields.io/github/stars/tientrandinh/Revisiting-Reverse-Distillation?style=flat)](https://github.com/tientrandinh/Revisiting-Reverse-Distillation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tien_Revisiting_Reverse_Distillation_for_Anomaly_Detection_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=cGRgy2Z0XQo) |
| PROB: Probabilistic Objectness for Open World Object Detection | [![GitHub](https://img.shields.io/github/stars/orrzohar/PROB?style=flat)](https://github.com/orrzohar/PROB) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zohar_PROB_Probabilistic_Objectness_for_Open_World_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.01424-b31b1b.svg)](http://arxiv.org/abs/2212.01424) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=prSeAoO82M4) |
| Equiangular Basis Vectors | [![GitHub](https://img.shields.io/github/stars/NJUST-VIPGroup/Equiangular-Basis-Vectors?style=flat)](https://github.com/NJUST-VIPGroup/Equiangular-Basis-Vectors) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shen_Equiangular_Basis_Vectors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11637-b31b1b.svg)](http://arxiv.org/abs/2303.11637) | :heavy_minus_sign: |
| Weakly Supervised Posture Mining for Fine-Grained Classification | [![GitHub](https://img.shields.io/github/stars/ZhenchaoTang/Fine-grainedImageClassification?style=flat)](https://github.com/ZhenchaoTang/Fine-grainedImageClassification) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tang_Weakly_Supervised_Posture_Mining_for_Fine-Grained_Classification_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| An Actor-Centric Causality Graph for Asynchronous Temporal Inference in Group Activity | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xie_An_Actor-Centric_Causality_Graph_for_Asynchronous_Temporal_Inference_in_Group_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Weak-Shot Object Detection Through Mutual Knowledge Transfer | :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Du_Weak-Shot_Object_Detection_Through_Mutual_Knowledge_Transfer_CVPR_2023_paper.pdf) | :heavy_minus_sign: | 
| Zero-Shot Everything Sketch-based Image Retrieval, and in Explainable Style |  |  |  |
| Exploring Structured Semantic Prior for Multi Label Recognition with Incomplete Labels |  |  |  |
| Learning Partial Correlation based Deep Visual Representation for Image Classification |  |  |  |
| Boundary-aware Backward-Compatible Representation via Adversarial Learning in Image Retrieval |  |  |  |
| PHA: Patch-Wise High-Frequency Augmentation for Transformer-based Person Re-Identification |  |  |  |
| Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects |  |  |  |
| BoxTeacher: Exploring High-Quality Pseudo Labels for Weakly Supervised Instance Segmentation |  |  |  |
| Annealing-based Label-Transfer Learning for Open World Object Detection |  |  |  |
| Diversity-Measurable Anomaly Detection |  |  |  |
| Recurrent Vision Transformers for Object Detection with Event Cameras |  |  |  |
| AShapeFormer: Semantics-guided Object-Level Active Shape Encoding for 3D Object Detection via Transformers |  |  |  |
| Ranking Regularization for Critical Rare Classes: Minimizing False Positives at a High True Positive Rate |  |  |  |
| Contrastive Mean Teacher for Domain Adaptive Object Detectors |  |  |  |
| Bridging the Gap between Model Explanations in Partially Annotated Multi-Label Classification |  |  |  |
| PartMix: Regularization Strategy to Learn Part Discovery for Visible-Infrared Person Re-Identification |  |  |  |
| BiasAdv: Bias-Adversarial Augmentation for Model Debiasing |  |  |  |
| ViPLO: Vision Transformer based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection |  |  |  |
| Robust 3D Shape Classification via Non-Local Graph Attention Network |  |  |  |
| Two-Way Multi-Label Loss |  |  |  |
| Normalizing Flow based Feature Synthesis for Outlier-Aware Object Detection |  |  |  |
| Object Detection with Self-Supervised Scene Adaptation |  |  |  |
| Data-Efficient Large Scale Place Recognition with Graded Similarity Supervision |  |  |  |
| Generating Features with Increased Crop-related Diversity for Few-Shot Object Detection |  |  |  |
| Recognizing Rigid Patterns of Unlabeled Point Clouds by Complete and Continuous Isometry Invariants with no False Negatives and no False Positives |  |  |  |
| Deep Semi-Supervised Metric Learning with Mixed Label Propagation |  |  |  |
| Fine-grained Classification with Noisy Labels |  |  |  |
