# CVPR-2023-24-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/sections/2023/main/segmentation-grouping-and-shape-analysis.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/sections/2023/main/multimodal-learning.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Deep Learning Architectures and Techniques

![Section Papers](https://img.shields.io/badge/Section%20Papers-91-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-70-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-69-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-69-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| PA&DA: Jointly Sampling PAth and DAta for Consistent NAS | [![GitHub](https://img.shields.io/github/stars/ShunLu91/PA-DA?style=flat)](https://github.com/ShunLu91/PA-DA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_PADA_Jointly_Sampling_Path_and_Data_for_Consistent_NAS_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.14772-b31b1b.svg)](https://arxiv.org/abs/2302.14772) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Aphq-rzvato) |
| Top-Down Visual Attention from Analysis by Synthesis | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sites.google.com/view/absvit) <br /> [![GitHub](https://img.shields.io/github/stars/bfshi/AbSViT?style=flat)](https://github.com/bfshi/AbSViT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_Top-Down_Visual_Attention_From_Analysis_by_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13043-b31b1b.svg)](http://arxiv.org/abs/2303.13043) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9MG--mbXfYE) |
| CUF: Continuous Upsampling Filters | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cuf-paper.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Vasconcelos_CUF_Continuous_Upsampling_Filters_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.06965-b31b1b.svg)](http://arxiv.org/abs/2210.06965) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pusooSMrbHs) |
| Curvature-Balanced Feature Manifold Learning for Long-tailed Classification | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_Curvature-Balanced_Feature_Manifold_Learning_for_Long-Tailed_Classification_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12307-b31b1b.svg)](http://arxiv.org/abs/2303.12307) | :heavy_minus_sign: |
| Neighborhood Attention Transformer | [![GitHub](https://img.shields.io/github/stars/SHI-Labs/Neighborhood-Attention-Transformer?style=flat)](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hassani_Neighborhood_Attention_Transformer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2204.07143-b31b1b.svg)](http://arxiv.org/abs/2204.07143) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Ya4BfioxIHA) |
| Progressive Random Convolutions for Single Domain Generalization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Progressive_Random_Convolutions_for_Single_Domain_Generalization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00424-b31b1b.svg)](http://arxiv.org/abs/2304.00424) | :heavy_minus_sign: |
| Domain Expansion of Image Generators | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yotamnitzan.github.io/domain-expansion/) <br /> [![GitHub](https://img.shields.io/github/stars/adobe-research/domain-expansion?style=flat)](https://github.com/adobe-research/domain-expansion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Nitzan_Domain_Expansion_of_Image_Generators_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.05225-b31b1b.svg)](http://arxiv.org/abs/2301.05225) | :heavy_minus_sign: |
| Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/xxgege/GAM?style=flat)](https://github.com/xxgege/GAM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Gradient_Norm_Aware_Minimization_Seeks_First-Order_Flatness_and_Improves_Generalization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03108-b31b1b.svg)](http://arxiv.org/abs/2303.03108) | :heavy_minus_sign: |
| Boosting Verified Training for Robust Image Classifications via Abstraction | [![GitHub](https://img.shields.io/github/stars/zhangzhaodi233/ABSCERT?style=flat)](https://github.com/zhangzhaodi233/ABSCERT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Boosting_Verified_Training_for_Robust_Image_Classifications_via_Abstraction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11552-b31b1b.svg)](http://arxiv.org/abs/2303.11552) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=abLiWBDfxCU) |
| Joint Token Pruning and Squeezing Towards more Aggressive Compression of Vision Transformers | [![GitHub](https://img.shields.io/github/stars/megvii-research/TPS-CVPR2023?style=flat)](https://github.com/megvii-research/TPS-CVPR2023) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Joint_Token_Pruning_and_Squeezing_Towards_More_Aggressive_Compression_of_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10716-b31b1b.svg)](http://arxiv.org/abs/2304.10716) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=BxM3R_9KKRs) |
| PointListNet: Deep Learning on 3D Point Lists | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_PointListNet_Deep_Learning_on_3D_Point_Lists_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Rate Gradient Approximation Attack Threats Deep Spiking Neural Networks | [![GitHub](https://img.shields.io/github/stars/putshua/SNN_attack_RGA?style=flat)](https://github.com/putshua/SNN_attack_RGA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Bu_Rate_Gradient_Approximation_Attack_Threats_Deep_Spiking_Neural_Networks_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dt2c0vP1cGk) |
| Masked Jigsaw Puzzle: A Versatile Position Embedding for Vision Transformers | [![GitHub](https://img.shields.io/github/stars/yhlleo/MJP?style=flat)](https://github.com/yhlleo/MJP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Masked_Jigsaw_Puzzle_A_Versatile_Position_Embedding_for_Vision_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2205.12551-b31b1b.svg)](http://arxiv.org/abs/2205.12551) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2ebjSE9pHRc) |
| Deep Graph Reprogramming <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jing_Deep_Graph_Reprogramming_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.14593-b31b1b.svg)](http://arxiv.org/abs/2304.14593) | :heavy_minus_sign: |
| ConvNeXt V2: Co-Designing and Scaling ConvNets with Masked Autoencoders | [![GitHub](https://img.shields.io/github/stars/facebookresearch/ConvNeXt-V2?style=flat)](https://github.com/facebookresearch/ConvNeXt-V2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Woo_ConvNeXt_V2_Co-Designing_and_Scaling_ConvNets_With_Masked_Autoencoders_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.00808-b31b1b.svg)](http://arxiv.org/abs/2301.00808) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wXuC7iDZI2M) |
| Frustratingly Easy Regularization on Representation can Boost Deep Reinforcement Learning | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sites.google.com/view/peer-cvpr2023/) <br /> [![GitHub](https://img.shields.io/github/stars/sweetice/PEER-CVPR23?style=flat)](https://github.com/sweetice/PEER-CVPR23) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/He_Frustratingly_Easy_Regularization_on_Representation_Can_Boost_Deep_Reinforcement_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2205.14557-b31b1b.svg)](http://arxiv.org/abs/2205.14557) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9YQoxTVn0nI) |
| Unified Pose Sequence Modeling | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Foo_Unified_Pose_Sequence_Modeling_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=M6hthIqiLQM) |
| RIFormer: Keep Your Vision Backbone Effective but Removing Token Mixer | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://techmonsterwang.github.io/RIFormer/) <br /> [![GitHub](https://img.shields.io/github/stars/techmonsterwang/RIFormer?style=flat)](https://github.com/techmonsterwang/RIFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_RIFormer_Keep_Your_Vision_Backbone_Effective_but_Removing_Token_Mixer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05659-b31b1b.svg)](https://arxiv.org/abs/2304.05659) | :heavy_minus_sign: |
| Real-Time Neural Light Field on Mobile Devices | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://snap-research.github.io/MobileR2L/) <br /> [![GitHub](https://img.shields.io/github/stars/snap-research/MobileR2L?style=flat)](https://github.com/snap-research/MobileR2L) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Real-Time_Neural_Light_Field_on_Mobile_Devices_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08057-b31b1b.svg)](http://arxiv.org/abs/2212.08057) | :heavy_minus_sign: |
| Towards Scalable Neural Representation for Diverse Videos | [![GitHub](https://img.shields.io/github/stars/boheumd/D-NeRV?style=flat)](https://github.com/boheumd/D-NeRV) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/He_Towards_Scalable_Neural_Representation_for_Diverse_Videos_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14124-b31b1b.svg)](http://arxiv.org/abs/2303.14124) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ByqNGvCCHPA) |
| AutoFocusFormer: Image Segmentation Off the Grid | [![GitHub](https://img.shields.io/github/stars/apple/ml-autofocusformer?style=flat)](https://github.com/apple/ml-autofocusformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ziwen_AutoFocusFormer_Image_Segmentation_off_the_Grid_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.12406-b31b1b.svg)](http://arxiv.org/abs/2304.12406) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=i1mZtk70yGY) |
| Minimizing the Accumulated Trajectory Error to Improve Dataset Distillation | [![GitHub](https://img.shields.io/github/stars/AngusDujw/FTD-distillation?style=flat)](https://github.com/AngusDujw/FTD-distillation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11004-b31b1b.svg)](https://arxiv.org/abs/2211.11004) | :heavy_minus_sign: |
| Deep Learning of Partial Graph Matching via Differentiable Top-K | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/Thinklab-SJTU/ThinkMatch/tree/master/models/AFAT) <br /> [![GitHub](https://img.shields.io/github/stars/Thinklab-SJTU/ThinkMatch?style=flat)](https://github.com/Thinklab-SJTU/ThinkMatch) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Deep_Learning_of_Partial_Graph_Matching_via_Differentiable_Top-K_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zFZ2WQHlVGo) |
| WIRE: Wavelet Implicit Neural Representations | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://vishwa91.github.io/wire) <br /> [![GitHub](https://img.shields.io/github/stars/vishwa91/wire?style=flat)](https://github.com/vishwa91/wire) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Saragadam_WIRE_Wavelet_Implicit_Neural_Representations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.05187-b31b1b.svg)](http://arxiv.org/abs/2301.05187) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=4jI8DZPEfEY) |
| Decompose, Adjust, Compose: Effective Normalization by Playing with Frequency for Domain Generalization | [![GitHub](https://img.shields.io/github/stars/sangrokleeeeee/DAC?style=flat)](https://github.com/sangrokleeeeee/DAC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Decompose_Adjust_Compose_Effective_Normalization_by_Playing_With_Frequency_for_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02328-b31b1b.svg)](http://arxiv.org/abs/2303.02328) | :heavy_minus_sign: |
| Towards a Smaller Student: Capacity Dynamic Distillation for Efficient Image Retrieval | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Towards_a_Smaller_Student_Capacity_Dynamic_Distillation_for_Efficient_Image_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09230-b31b1b.svg)](http://arxiv.org/abs/2303.09230) | :heavy_minus_sign: |
| UniHCP: A Unified Model for Human-Centric Perceptions | [![GitHub](https://img.shields.io/github/stars/OpenGVLab/UniHCP?style=flat)](https://github.com/OpenGVLab/UniHCP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ci_UniHCP_A_Unified_Model_for_Human-Centric_Perceptions_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02936-b31b1b.svg)](http://arxiv.org/abs/2303.02936) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=K0_ffbSmiiM) |
| Trainable Projected Gradient Method for Robust Fine-Tuning | [![GitHub](https://img.shields.io/github/stars/PotatoTian/TPGM?style=flat)](https://github.com/PotatoTian/TPGM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_Trainable_Projected_Gradient_Method_for_Robust_Fine-Tuning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10720-b31b1b.svg)](http://arxiv.org/abs/2303.10720) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=RHEzpEtH4wg) |
| Boosting Semi-Supervised Learning by Exploiting All Unlabeled Data | [![GitHub](https://img.shields.io/github/stars/megvii-research/FullMatch?style=flat)](https://github.com/megvii-research/FullMatch) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Boosting_Semi-Supervised_Learning_by_Exploiting_All_Unlabeled_Data_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11066-b31b1b.svg)](http://arxiv.org/abs/2303.11066) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=AhizJGbenCY) |
| B-Spline Texture Coefficients Estimator for Screen Content Image Super-Resolution <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/ByeongHyunPak/btc?style=flat)](https://github.com/ByeongHyunPak/btc) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Pak_B-Spline_Texture_Coefficients_Estimator_for_Screen_Content_Image_Super-Resolution_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=SmVsO3POFZk) |
| Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks | [![GitHub](https://img.shields.io/github/stars/JierunChen/FasterNet?style=flat)](https://github.com/JierunChen/FasterNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Run_Dont_Walk_Chasing_Higher_FLOPS_for_Faster_Neural_Networks_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03667-b31b1b.svg)](https://arxiv.org/abs/2303.03667) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Z-SX5r2gVeU) |
| HyperMatch: Noise-Tolerant Semi-Supervised Learning via Relaxed Contrastive Constraint | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_HyperMatch_Noise-Tolerant_Semi-Supervised_Learning_via_Relaxed_Contrastive_Constraint_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=RHF3ADsRwUI) |
| From Node Interaction to Hop Interaction: New Effective and Scalable Graph Learning Paradigm | [![GitHub](https://img.shields.io/github/stars/JC-202/HopGNN?style=flat)](https://github.com/JC-202/HopGNN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_From_Node_Interaction_To_Hop_Interaction_New_Effective_and_Scalable_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11761-b31b1b.svg)](http://arxiv.org/abs/2211.11761) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=z14shuNm6Fk) |
| Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention | [![GitHub](https://img.shields.io/github/stars/LeapLabTHU/Slide-Transformer?style=flat)](https://github.com/LeapLabTHU/Slide-Transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Slide-Transformer_Hierarchical_Vision_Transformer_With_Local_Self-Attention_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04237-b31b1b.svg)](https://arxiv.org/abs/2304.04237) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-lceSYJ4adE) |
| On the Pitfall of Mixup for Uncertainty Calibration | [![GitHub](https://img.shields.io/github/stars/dengbaowang/Mixup-Inference-in-Training?style=flat)](https://github.com/dengbaowang/Mixup-Inference-in-Training) |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_On_the_Pitfall_of_Mixup_for_Uncertainty_Calibration_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xyJBTPsL-dQ) |
| Edges to Shapes to Concepts: Adversarial Augmentation for Robust Vision | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tripathi_Edges_to_Shapes_to_Concepts_Adversarial_Augmentation_for_Robust_Vision_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=grwNf3mEzlo) |
| Mod-Squad: Designing Mixtures of Experts as Modular Multi-Task Learners | [![GitHub](https://img.shields.io/github/stars/UMass-Foundation-Model/Mod-Squad?style=flat)](https://github.com/UMass-Foundation-Model/Mod-Squad) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Mod-Squad_Designing_Mixtures_of_Experts_As_Modular_Multi-Task_Learners_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08066-b31b1b.svg)](https://arxiv.org/abs/2212.08066) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PK44nE1M8ug) |
| DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network | [![GitHub](https://img.shields.io/github/stars/alibaba/lightweight-neural-architecture-search?style=flat)](https://github.com/alibaba/lightweight-neural-architecture-search) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_DeepMAD_Mathematical_Architecture_Design_for_Deep_Convolutional_Neural_Network_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02165-b31b1b.svg)](http://arxiv.org/abs/2303.02165) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=IfmdIs3QGaA) |
| PaCa-ViT: Learning Patch-to-Cluster Attention in Vision Transformers | [![GitHub](https://img.shields.io/github/stars/iVMCL/PaCaViT?style=flat)](https://github.com/iVMCL/PaCaViT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Grainger_PaCa-ViT_Learning_Patch-to-Cluster_Attention_in_Vision_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2203.11987-b31b1b.svg)](http://arxiv.org/abs/2203.11987) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KajsMJAhisI) |
| BiFormer: Vision Transformer with Bi-Level Routing Attention | [![GitHub](https://img.shields.io/github/stars/rayleizhu/BiFormer?style=flat)](https://github.com/rayleizhu/BiFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_BiFormer_Vision_Transformer_With_Bi-Level_Routing_Attention_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08810-b31b1b.svg)](http://arxiv.org/abs/2303.08810) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VHaJl2FNxP8) |
| DeSTSeg: Segmentation Guided Denoising Student-Teacher for Anomaly Detection | [![GitHub](https://img.shields.io/github/stars/apple/ml-destseg?style=flat)](https://github.com/apple/ml-destseg) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_DeSTSeg_Segmentation_Guided_Denoising_Student-Teacher_for_Anomaly_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11317-b31b1b.svg)](http://arxiv.org/abs/2211.11317) | :heavy_minus_sign: |
| Hierarchical Neural Memory Network for Low Latency Event Processing | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hamarh.github.io/hmnet/) <br /> [![GitHub](https://img.shields.io/github/stars/hamarh/HMNet_pth?style=flat)](https://github.com/hamarh/HMNet_pth) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hamaguchi_Hierarchical_Neural_Memory_Network_for_Low_Latency_Event_Processing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.17852-b31b1b.svg)](http://arxiv.org/abs/2305.17852) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Er-sKbcov8I) |
| Block Selection Method for using Feature Norm in Out-of-Distribution Detection | [![GitHub](https://img.shields.io/github/stars/gist-ailab/block-selection-for-OOD-detection?style=flat)](https://github.com/gist-ailab/block-selection-for-OOD-detection) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Block_Selection_Method_for_Using_Feature_Norm_in_Out-of-Distribution_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.02295-b31b1b.svg)](http://arxiv.org/abs/2212.02295) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=prgocfj5hnc) |
| NAR-Former: Neural Architecture Representation Learning Towards Holistic Attributes Prediction | [![GitHub](https://img.shields.io/github/stars/yuny220/NAR-Former?style=flat)](https://github.com/yuny220/NAR-Former) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_NAR-Former_Neural_Architecture_Representation_Learning_Towards_Holistic_Attributes_Prediction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.08024-b31b1b.svg)](https://arxiv.org/abs/2211.08024) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VuCVQy7Ea9I) |
| MDL-NAS: A Joint Multi-Domain Learning Framework for Vision Transformer | :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MDL-NAS_A_Joint_Multi-Domain_Learning_Framework_for_Vision_Transformer_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=SjL4JXxiUw8) |
| VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution | [![GitHub](https://img.shields.io/github/stars/jaeill/CVPR23-VNE?style=flat)](https://github.com/jaeill/CVPR23-VNE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_VNE_An_Effective_Method_for_Improving_Deep_Representation_by_Manipulating_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01434-b31b1b.svg)](http://arxiv.org/abs/2304.01434) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=YW4DRxU0LrI) |
| Multi-Agent Automated Machine Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Multi-Agent_Automated_Machine_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.09084-b31b1b.svg)](http://arxiv.org/abs/2210.09084) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lVkpmbUbvM4) |
| Making Vision Transformers Efficient from a Token Sparsification View | [![GitHub](https://img.shields.io/github/stars/changsn/STViT-R?style=flat)](https://github.com/changsn/STViT-R) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_Making_Vision_Transformers_Efficient_From_a_Token_Sparsification_View_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08685-b31b1b.svg)](http://arxiv.org/abs/2303.08685) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=obLwEmelttw) |
| Integral Neural Networks <br /> ![CVPR - Award](https://img.shields.io/badge/CVPR-Award-294A7C) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://inn.thestage.ai/) <br /> [![GitHub](https://img.shields.io/github/stars/TheStageAI/TorchIntegral?style=flat)](https://github.com/TheStageAI/TorchIntegral) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Solodskikh_Integral_Neural_Networks_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=MdSVyi00r3E) |
| RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving | [![GitHub](https://img.shields.io/github/stars/valeoai/rangevit?style=flat)](https://github.com/valeoai/rangevit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ando_RangeViT_Towards_Vision_Transformers_for_3D_Semantic_Segmentation_in_Autonomous_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.10222-b31b1b.svg)](http://arxiv.org/abs/2301.10222) | :heavy_minus_sign: |
| MIANet: Aggregating Unbiased Instance and General Information for Few-Shot Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/Aldrich2y/MIANet?style=flat)](https://github.com/Aldrich2y/MIANet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_MIANet_Aggregating_Unbiased_Instance_and_General_Information_for_Few-Shot_Semantic_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.13864-b31b1b.svg)](http://arxiv.org/abs/2305.13864) | :heavy_minus_sign: |
| One-Shot Model for Mixed-Precision Quantization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Koryakovskiy_One-Shot_Model_for_Mixed-Precision_Quantization_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vHSGGL9XR6M) |
| Learning Dynamic Style Kernels for Artistic Style Transfer | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Learning_Dynamic_Style_Kernels_for_Artistic_Style_Transfer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00414-b31b1b.svg)](http://arxiv.org/abs/2304.00414) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=r5xUjntANCA) |
| SVGformer: Representation Learning for Continuous Vector Graphics Using Transformers | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_SVGformer_Representation_Learning_for_Continuous_Vector_Graphics_Using_Transformers_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=s6OqutGKGa4) |
| How to Prevent the Continuous Damage of Noises to Model Training? | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_How_To_Prevent_the_Continuous_Damage_of_Noises_To_Model_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=B3g-PE5R3Ac) |
| GKEAL: Gaussian Kernel Embedded Analytic Learning for Few-Shot Class Incremental Task | [![GitHub](https://img.shields.io/github/stars/ZHUANGHP/Analytic-continual-learning?style=flat)](https://github.com/ZHUANGHP/Analytic-continual-learning) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhuang_GKEAL_Gaussian_Kernel_Embedded_Analytic_Learning_for_Few-Shot_Class_Incremental_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5hSrTcvkQ00) |
| Differentiable Architecture Search with Random Features | :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Differentiable_Architecture_Search_With_Random_Features_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.08835-b31b1b.svg)](http://arxiv.org/abs/2208.08835) | :heavy_minus_sign: |
| ERM-KTP: Knowledge-Level Machine Unlearning via Knowledge Transfer | [![GitHub](https://img.shields.io/github/stars/RUIYUN-ML/ERM-KTP?style=flat)](https://github.com/RUIYUN-ML/ERM-KTP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_ERM-KTP_Knowledge-Level_Machine_Unlearning_via_Knowledge_Transfer_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VUG0eFlyYn4) |
| FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Karpikova_FIANCEE_Faster_Inference_of_Adversarial_Networks_via_Conditional_Early_Exits_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10306-b31b1b.svg)](http://arxiv.org/abs/2304.10306) | :heavy_minus_sign: |
| Beyond Attentive Tokens: Incorporating Token Importance and Diversity for Efficient Vision Transformers | [![GitHub](https://img.shields.io/github/stars/BWLONG/BeyondAttentiveTokens?style=flat)](https://github.com/BWLONG/BeyondAttentiveTokens) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Long_Beyond_Attentive_Tokens_Incorporating_Token_Importance_and_Diversity_for_Efficient_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11315-b31b1b.svg)](http://arxiv.org/abs/2211.11315) | :heavy_minus_sign: |
| Token Turing Machines | [![GitHub](https://img.shields.io/github/stars/google-research/scenic?style=flat)](https://github.com/google-research/scenic/tree/main/scenic/projects/token_turing) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ryoo_Token_Turing_Machines_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.09119-b31b1b.svg)](http://arxiv.org/abs/2211.09119) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=A21OGHJ4FmE) |
| Co-Training 2<sup>L</sup> Submodels for Visual Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Touvron_Co-Training_2L_Submodels_for_Visual_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04884-b31b1b.svg)](https://arxiv.org/abs/2212.04884) | :heavy_minus_sign: |
| HOTNAS: Hierarchical Optimal Transport for Neural Architecture Search | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_HOTNAS_Hierarchical_Optimal_Transport_for_Neural_Architecture_Search_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=NV-HVjmtr_8) |
| SLACK: Stable Learning of Augmentations with Cold-Start and KL Regularization | [![GitHub](https://img.shields.io/github/stars/naver/slack?style=flat)](https://github.com/naver/slack) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Marrie_SLACK_Stable_Learning_of_Augmentations_With_Cold-Start_and_KL_Regularization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.09998-b31b1b.svg)](https://arxiv.org/abs/2306.09998v1) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nTH5SME2_fk) |
| MarginMatch: Improving Semi-Supervised Learning with Pseudo-Margins | [![GitHub](https://img.shields.io/github/stars/tsosea2/marginmatch?style=flat)](https://github.com/tsosea2/marginmatch) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Sosea_MarginMatch_Improving_Semi-Supervised_Learning_with_Pseudo-Margins_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09037-b31b1b.svg)](https://arxiv.org/abs/2308.09037) | :heavy_minus_sign: |
| Alias-Free Convnets: Fractional Shift Invariance via Polynomial Activations | [![GitHub](https://img.shields.io/github/stars/hmichaeli/alias_free_convnets?style=flat)](https://github.com/hmichaeli/alias_free_convnets) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Michaeli_Alias-Free_Convnets_Fractional_Shift_Invariance_via_Polynomial_Activations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08085-b31b1b.svg)](http://arxiv.org/abs/2303.08085) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-8SeI0KDuAI) |
| Detection of Out-of-Distribution Samples using Binary Neuron Activation Patterns | [![GitHub](https://img.shields.io/github/stars/safednn-group/nap-ood?style=flat)](https://github.com/safednn-group/nap-ood) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Olber_Detection_of_Out-of-Distribution_Samples_Using_Binary_Neuron_Activation_Patterns_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.14268-b31b1b.svg)](http://arxiv.org/abs/2212.14268) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=UoptSWWXV9U) |
| Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives | [![GitHub](https://img.shields.io/github/stars/zhichao-lu/etr-nlp-mtl?style=flat)](https://github.com/zhichao-lu/etr-nlp-mtl) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_Mitigating_Task_Interference_in_Multi-Task_Learning_via_Explicit_Task_Routing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.02066-b31b1b.svg)](https://arxiv.org/abs/2308.02066) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Oue6vtprTPk) |
| Superclass Learning with Representation Enhancement | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Superclass_Learning_With_Representation_Enhancement_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FH1WWEvKJI0) |
| Perception and Semantic Aware Regularization for Sequential Confidence Calibration | [![GitHub](https://img.shields.io/github/stars/husterpzh/PSSR?style=flat)](https://github.com/husterpzh/PSSR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Peng_Perception_and_Semantic_Aware_Regularization_for_Sequential_Confidence_Calibration_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.19498-b31b1b.svg)](http://arxiv.org/abs/2305.19498)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=cic1NxV9G5Y) |
| DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks | [![GitHub](https://img.shields.io/github/stars/val-iisc/DART?style=flat)](https://github.com/val-iisc/DART) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_DART_Diversify-Aggregate-Repeat_Training_Improves_Generalization_of_Neural_Networks_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.14685-b31b1b.svg)](http://arxiv.org/abs/2302.14685) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=4QAZrLp7AWM) |
| Improving Robustness of Vision Transformers by Reducing Sensitivity to Patch Corruptions | [![GitHub](https://img.shields.io/github/stars/guoyongcs/RSPC?style=flat)](https://github.com/guoyongcs/RSPC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Improving_Robustness_of_Vision_Transformers_by_Reducing_Sensitivity_To_Patch_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| E2PN: Efficient SE(3)-Equivariant Point Network | [![GitHub](https://img.shields.io/github/stars/minghanz/E2PN?style=flat)](https://github.com/minghanz/E2PN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_E2PN_Efficient_SE3-Equivariant_Point_Network_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=B4XDxd0h08I) |
| Generalization Matters: Loss Minima Flattening via Parameter Hybridization for Efficient Online Knowledge Distillation | [![GitHub](https://img.shields.io/github/stars/tianlizhang/OKDPH?style=flat)](https://github.com/tianlizhang/OKDPH) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Generalization_Matters_Loss_Minima_Flattening_via_Parameter_Hybridization_for_Efficient_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14666-b31b1b.svg)](http://arxiv.org/abs/2303.14666) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LpUXn1ytYnA) |
| Regularization of Polynomial Networks for Image Recognition | [![GitHub](https://img.shields.io/github/stars/grigorisg9gr/regularized_polynomials?style=flat)](https://github.com/grigorisg9gr/regularized_polynomials) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chrysos_Regularization_of_Polynomial_Networks_for_Image_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13896-b31b1b.svg)](http://arxiv.org/abs/2303.13896) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=h7iBpEHGVNc) |
| Hyperspherical Embedding for Point Cloud Completion | [![GitHub](https://img.shields.io/github/stars/haomengz/HyperPC?style=flat)](https://github.com/haomengz/HyperPC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Hyperspherical_Embedding_for_Point_Cloud_Completion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.05634-b31b1b.svg)](https://arxiv.org/abs/2307.05634) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lyoOYk_gPlQ) |
| On the Effectiveness of Partial Variance Reduction in Federated Learning with Heterogeneous Data <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/lyn1874/fedpvr?style=flat)](https://github.com/lyn1874/fedpvr) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_On_the_Effectiveness_of_Partial_Variance_Reduction_in_Federated_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.02191-b31b1b.svg)](https://arxiv.org/abs/2212.02191) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=J6ZtL46Gr0w) |
| Independent Component Alignment for Multi-Task Learning | [![GitHub](https://img.shields.io/github/stars/SamsungLabs/MTL?style=flat)](https://github.com/SamsungLabs/MTL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Senushkin_Independent_Component_Alignment_for_Multi-Task_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.19000-b31b1b.svg)](https://arxiv.org/abs/2305.19000) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mDSPJsrLu1g) |
| MP-Former: Mask-Piloted Transformer for Image Segmentation | [![GitHub](https://img.shields.io/github/stars/IDEA-Research/MP-Former?style=flat)](https://github.com/IDEA-Research/MP-Former) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_MP-Former_Mask-Piloted_Transformer_for_Image_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07336-b31b1b.svg)](https://arxiv.org/abs/2303.07336) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WXaGsgv0ngk) |
| SMPConv: Self-Moving Point Representations for Continuous Convolution | [![GitHub](https://img.shields.io/github/stars/sangnekim/SMPConv?style=flat)](https://github.com/sangnekim/SMPConv) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_SMPConv_Self-Moving_Point_Representations_for_Continuous_Convolution_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02330-b31b1b.svg)](http://arxiv.org/abs/2304.02330) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tMiwzeD13uk) |
| MaskCon: Masked Contrastive Learning for Coarse-Labelled Dataset | [![GitHub](https://img.shields.io/github/stars/MrChenFeng/MaskCon_CVPR2023?style=flat)](https://github.com/MrChenFeng/MaskCon_CVPR2023) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_MaskCon_Masked_Contrastive_Learning_for_Coarse-Labelled_Dataset_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12756-b31b1b.svg)](http://arxiv.org/abs/2303.12756) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=V4is0vAYNOw) |
| FlexiViT: One Model for All Patch Sizes | [![GitHub](https://img.shields.io/github/stars/bwconrad/flexivit?style=flat)](https://github.com/bwconrad/flexivit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Beyer_FlexiViT_One_Model_for_All_Patch_Sizes_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08013-b31b1b.svg)](http://arxiv.org/abs/2212.08013) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jijeGJWh4u4) |
| GEN: Pushing the Limits of Softmax-based Out-of-Distribution Detection | [![GitHub](https://img.shields.io/github/stars/XixiLiu95/GEN?style=flat)](https://github.com/XixiLiu95/GEN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_GEN_Pushing_the_Limits_of_Softmax-Based_Out-of-Distribution_Detection_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2zEhSGNd_I4) |
| Zero-Shot Noise2Noise: Efficient Image Denoising without Any Data | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1i82nyizTdszyHkaHBuKPbWnTzao8HF9b) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Mansour_Zero-Shot_Noise2Noise_Efficient_Image_Denoising_Without_Any_Data_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11253-b31b1b.svg)](http://arxiv.org/abs/2303.11253) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=l6s68JAfknE) |
| Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference | [![GitHub](https://img.shields.io/github/stars/GATECH-EIC/Castling-ViT?style=flat)](https://github.com/GATECH-EIC/Castling-ViT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/You_Castling-ViT_Compressing_Self-Attention_via_Switching_Towards_Linear-Angular_Attention_at_Vision_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.10526-b31b1b.svg)](https://arxiv.org/abs/2211.10526) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2x31-_xbi5U) |
| HNeRV: A Hybrid Neural Representation for Videos | [![GitHub](https://img.shields.io/github/stars/haochen-rye/HNeRV?style=flat)](https://github.com/haochen-rye/HNeRV) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_HNeRV_A_Hybrid_Neural_Representation_for_Videos_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02633-b31b1b.svg)](http://arxiv.org/abs/2304.02633) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=k2WNoWFCbHA) |
| Re-Basin via Implicit Sinkhorn Differentiation | [![GitHub](https://img.shields.io/github/stars/fagp/sinkhorn-rebasin?style=flat)](https://github.com/fagp/sinkhorn-rebasin) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Pena_Re-Basin_via_Implicit_Sinkhorn_Differentiation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.12042-b31b1b.svg)](https://arxiv.org/abs/2212.12042) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=RPSqoLx-ggk) |
| Bayesian Posterior Approximation with Stochastic Ensembles | [![GitHub](https://img.shields.io/github/stars/oleksandr-balabanov/stochastic-ensembles?style=flat)](https://github.com/oleksandr-balabanov/stochastic-ensembles) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Balabanov_Bayesian_Posterior_Approximation_With_Stochastic_Ensembles_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08123-b31b1b.svg)](http://arxiv.org/abs/2212.08123) | :heavy_minus_sign: |
| FedDM: Iterative Distribution Matching for Communication-Efficient Federated Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_FedDM_Iterative_Distribution_Matching_for_Communication-Efficient_Federated_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2207.09653-b31b1b.svg)](http://arxiv.org/abs/2207.09653) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WDNOxM6UILs) |
| Enhancing Multiple Reliability Measures via Nuisance-Extended Information Bottleneck | [![GitHub](https://img.shields.io/github/stars/jh-jeong/nuisance_ib?style=flat)](https://github.com/jh-jeong/nuisance_ib) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jeong_Enhancing_Multiple_Reliability_Measures_via_Nuisance-Extended_Information_Bottleneck_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14096-b31b1b.svg)](http://arxiv.org/abs/2303.14096) | :heavy_minus_sign: |
| Federated Learning with Data-Agnostic Distribution Fusion | [![GitHub](https://img.shields.io/github/stars/LiruichenSpace/FedFusion?style=flat)](https://github.com/LiruichenSpace/FedFusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Duan_Federated_Learning_With_Data-Agnostic_Distribution_Fusion_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=IW2BhW8POyg) |
<!-- | Vision Transformer with Super Token Sampling | [![GitHub](https://img.shields.io/github/stars/hhb072/STViT?style=flat)](https://github.com/hhb072/STViT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Vision_Transformer_With_Super_Token_Sampling_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11167-b31b1b.svg)](http://arxiv.org/abs/2211.11167) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-n-DuHRgmpY) | Not found -->