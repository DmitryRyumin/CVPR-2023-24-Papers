# CVPR-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
    <tr>
        <td><strong>New collections</strong></td>
        <td>
            <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README.md">
                <img src="http://img.shields.io/badge/CVPR-2024-0073AE.svg" alt="Conference">
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/sections/2023/main/recognition-categorization-detection-retrieval.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README_2023.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/sections/2023/main/low-level-vision.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Vision, Language, and Reasoning

![Section Papers](https://img.shields.io/badge/Section%20Papers-118-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-94-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-85-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-90-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://antoyang.github.io/vid2seq.html) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Vid2Seq_Large-Scale_Pretraining_of_a_Visual_Language_Model_for_Dense_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.14115-b31b1b.svg)](http://arxiv.org/abs/2302.14115) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hXP-2fYzq4g) |
| Open-Vocabulary Panoptic Segmentation With Text-to-Image Diffusion Models <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/NVlabs/ODISE?style=flat)](https://github.com/NVlabs/ODISE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Open-Vocabulary_Panoptic_Segmentation_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.04803-b31b1b.svg)](http://arxiv.org/abs/2303.04803)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=eW2vF8o_7p0) |
| Iterative Proposal Refinement for Weakly-Supervised Video Grounding | [![GitHub](https://img.shields.io/github/stars/ttengwang/Awesome_Long_Term_Video_Understanding?style=flat)](https://github.com/ttengwang/Awesome_Long_Term_Video_Understanding) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Iterative_Proposal_Refinement_for_Weakly-Supervised_Video_Grounding_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=BbGvHI_pVXk) |
| MetaCLUE: Towards Comprehensive Visual Metaphors Research | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://metaclue.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Akula_MetaCLUE_Towards_Comprehensive_Visual_Metaphors_Research_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09898-b31b1b.svg)](http://arxiv.org/abs/2212.09898) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=V3TmeNETL-o) |
| PolyFormer: Referring Image Segmentation As Sequential Polygon Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://polyformer.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/amazon-science/polygon-transformer?style=flat)](https://github.com/amazon-science/polygon-transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_PolyFormer_Referring_Image_Segmentation_As_Sequential_Polygon_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.07387-b31b1b.svg)](http://arxiv.org/abs/2302.07387) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6LNrqoxQR1M) |
| GeneCIS: A Benchmark for General Conditional Image Similarity <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) |[![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sgvaze.github.io/genecis/) <br /> [![GitHub](https://img.shields.io/github/stars/facebookresearch/genecis?style=flat)](https://github.com/facebookresearch/genecis) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Vaze_GeneCIS_A_Benchmark_for_General_Conditional_Image_Similarity_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.07969-b31b1b.svg)](https://arxiv.org/abs/2306.07969)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wu3U2iNGIUw) |
| FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) |[![GitHub](https://img.shields.io/github/stars/BrandonHanx/FAME-ViL?style=flat)](https://github.com/BrandonHanx/FAME-ViL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Han_FAME-ViL_Multi-Tasking_Vision-Language_Model_for_Heterogeneous_Fashion_Tasks_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02483-b31b1b.svg)](https://arxiv.org/abs/2303.02483) | :heavy_minus_sign: |
| Generative Bias for Robust Visual Question Answering | [![GitHub](https://img.shields.io/github/stars/chojw/genb?style=flat)](https://github.com/chojw/genb) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Generative_Bias_for_Robust_Visual_Question_Answering_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.00690-b31b1b.svg)](http://arxiv.org/abs/2208.00690)| :heavy_minus_sign: |
| Advancing Visual Grounding With Scene Knowledge: Benchmark and Method | [![GitHub](https://img.shields.io/github/stars/zhjohnchan/SK-VG?style=flat)](https://github.com/zhjohnchan/SK-VG) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Advancing_Visual_Grounding_With_Scene_Knowledge_Benchmark_and_Method_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11558-b31b1b.svg)](https://arxiv.org/abs/2307.11558) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DmmPiseO59o) |
| Gloss Attention for Gloss-Free Sign Language Translation | [![GitHub](https://img.shields.io/github/stars/YinAoXiong/GASLT?style=flat)](https://github.com/YinAoXiong/GASLT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_Gloss_Attention_for_Gloss-Free_Sign_Language_Translation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.07361-b31b1b.svg)](https://arxiv.org/abs/2307.07361) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=NEoWvxkJXfU) |
| You Can Ground Earlier Than See: An Effective and Efficient Pipeline for Temporal Sentence Grounding in Compressed Videos | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_You_Can_Ground_Earlier_Than_See_An_Effective_and_Efficient_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07863-b31b1b.svg)](http://arxiv.org/abs/2303.07863) | :heavy_minus_sign: |
| Generalized Decoding for Pixel, Image, and Language | [![GitHub](https://img.shields.io/github/stars/microsoft/X-Decoder?style=flat)](https://github.com/microsoft/X-Decoder) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zou_Generalized_Decoding_for_Pixel_Image_and_Language_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.11270-b31b1b.svg)](http://arxiv.org/abs/2212.11270) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wYp6vmyolqE) |
| Accelerating Vision-Language Pretraining With Free Language Modeling | [![GitHub](https://img.shields.io/github/stars/TencentARC/FLM?style=flat)](https://github.com/TencentARC/FLM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Accelerating_Vision-Language_Pretraining_With_Free_Language_Modeling_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14038-b31b1b.svg)](http://arxiv.org/abs/2303.14038) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WbH_5DH_jfY) |
| GRES: Generalized Referring Expression Segmentation <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://henghuiding.github.io/GRES/) <br /> [![GitHub](https://img.shields.io/github/stars/henghuiding/ReLA?style=flat)](https://github.com/henghuiding/ReLA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_GRES_Generalized_Referring_Expression_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.00968-b31b1b.svg)](http://arxiv.org/abs/2306.00968) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=eWjAgYUU6Do) |
| BUFFER: Balancing Accuracy, Efficiency, and Generalizability in Point Cloud Registration | [![GitHub](https://img.shields.io/github/stars/The-Learning-And-Vision-Atelier-LAVA/BUFFER?style=flat)](https://github.com/The-Learning-And-Vision-Atelier-LAVA/BUFFER) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ao_BUFFER_Balancing_Accuracy_Efficiency_and_Generalizability_in_Point_Cloud_Registration_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=STmAkRWuSiY) |
| RGB No More: Minimally-Decoded JPEG Vision Transformers | [![GitHub](https://img.shields.io/github/stars/JeongsooP/RGB-no-more?style=flat)](https://github.com/JeongsooP/RGB-no-more) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Park_RGB_No_More_Minimally-Decoded_JPEG_Vision_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.16421-b31b1b.svg)](http://arxiv.org/abs/2211.16421) | :heavy_minus_sign: |
| Scaling Language-Image Pre-Training via Masking | [![GitHub](https://img.shields.io/github/stars/facebookresearch/flip?style=flat)](https://github.com/facebookresearch/flip) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Scaling_Language-Image_Pre-Training_via_Masking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.00794-b31b1b.svg)](http://arxiv.org/abs/2212.00794) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=coOHTSMWhb8) |
| EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding | [![GitHub](https://img.shields.io/github/stars/yanmin-wu/EDA?style=flat)](https://github.com/yanmin-wu/EDA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_EDA_Explicit_Text-Decoupling_and_Dense_Alignment_for_3D_Visual_Grounding_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.14941-b31b1b.svg)](http://arxiv.org/abs/2209.14941) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=YBpPqYU07Es) |
| RefTeacher: A Strong Baseline for Semi-Supervised Referring Expression Comprehension | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://refteacher.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/Disguiser15/RefTeacher?style=flat)](https://github.com/Disguiser15/RefTeacher) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_RefTeacher_A_Strong_Baseline_for_Semi-Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Mobile User Interface Element Detection via Adaptively Prompt Tuning | [![GitHub](https://img.shields.io/github/stars/antmachineintelligence/MUI-zh?style=flat)](https://github.com/antmachineintelligence/MUI-zh) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Gu_Mobile_User_Interface_Element_Detection_via_Adaptively_Prompt_Tuning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.09699-b31b1b.svg)](http://arxiv.org/abs/2305.09699) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dMC26H1DQWw) |
| Context-Aware Alignment and Mutual Masking for 3D-Language Pre-Training <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/leolyj/3D-VLP?style=flat)](https://github.com/leolyj/3D-VLP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Context-Aware_Alignment_and_Mutual_Masking_for_3D-Language_Pre-Training_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Tell Me What Happened: Unifying Text-Guided Video Completion via Multimodal Masked Video Generation | [![GitHub](https://img.shields.io/github/stars/tsujuifu/pytorch_tvc?style=flat)](https://github.com/tsujuifu/pytorch_tvc) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Tell_Me_What_Happened_Unifying_Text-Guided_Video_Completion_via_Multimodal_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12824-b31b1b.svg)](http://arxiv.org/abs/2211.12824) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dnBzUfsf9Cc) |
| Meta Compositional Referring Expression Segmentation |  :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Meta_Compositional_Referring_Expression_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04415-b31b1b.svg)](http://arxiv.org/abs/2304.04415) | :heavy_minus_sign: |
| VindLU: A Recipe for Effective Video-and-Language Pretraining | [![GitHub](https://img.shields.io/github/stars/klauscc/VindLU?style=flat)](https://github.com/klauscc/VindLU) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_VindLU_A_Recipe_for_Effective_Video-and-Language_Pretraining_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.05051-b31b1b.svg)](http://arxiv.org/abs/2212.05051) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9koWpSPcYBQ) |
| Super-CLEVR: A Virtual Benchmark To Diagnose Domain Robustness in Visual Reasoning <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/Lizw14/Super-CLEVR?style=flat)](https://github.com/Lizw14/Super-CLEVR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Super-CLEVR_A_Virtual_Benchmark_To_Diagnose_Domain_Robustness_in_Visual_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.00259-b31b1b.svg)](https://arxiv.org/abs/2212.00259) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DWRp_70ypiA) |
| GIVL: Improving Geographical Inclusivity of Vision-Language Models With Pre-Training Methods| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_GIVL_Improving_Geographical_Inclusivity_of_Vision-Language_Models_With_Pre-Training_Methods_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.01893-b31b1b.svg)](http://arxiv.org/abs/2301.01893) | :heavy_minus_sign: |
| Learning Customized Visual Models With Retrieval-Augmented Knowledge <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://react-vl.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/microsoft/react?style=flat)](https://github.com/microsoft/react) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Learning_Customized_Visual_Models_With_Retrieval-Augmented_Knowledge_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.07094-b31b1b.svg)](http://arxiv.org/abs/2301.07094) | :heavy_minus_sign: |
| LAVENDER: Unifying Video-Language Understanding As Masked Language Modeling | [![GitHub](https://img.shields.io/github/stars/microsoft/LAVENDER?style=flat)](https://github.com/microsoft/LAVENDER) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_LAVENDER_Unifying_Video-Language_Understanding_As_Masked_Language_Modeling_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.07160-b31b1b.svg)](http://arxiv.org/abs/2206.07160) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=f8scI82_caE) |
| An Empirical Study of End-to-End Video-Language Transformers With Masked Visual Modeling | [![GitHub](https://img.shields.io/github/stars/tsujuifu/pytorch_empirical-mvm?style=flat)](https://github.com/tsujuifu/pytorch_empirical-mvm) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_An_Empirical_Study_of_End-to-End_Video-Language_Transformers_With_Masked_Visual_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.01540-b31b1b.svg)](http://arxiv.org/abs/2209.01540) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=T1qTkcMCq1k) |
| NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations | [![GitHub](https://img.shields.io/github/stars/joyhsu0504/NS3D?style=flat)](https://github.com/joyhsu0504/NS3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hsu_NS3D_Neuro-Symbolic_Grounding_of_3D_Objects_and_Relations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13483-b31b1b.svg)](http://arxiv.org/abs/2303.13483) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FmWgRR3NKIg) |
| Clover: Towards a Unified Video-Language Alignment and Fusion Model | [![GitHub](https://img.shields.io/github/stars/LeeYN-43/Clover?style=flat)](https://github.com/LeeYN-43/Clover) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Clover_Towards_a_Unified_Video-Language_Alignment_and_Fusion_Model_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2207.07885-b31b1b.svg)](http://arxiv.org/abs/2207.07885) | :heavy_minus_sign: |
| Dual Alignment Unsupervised Domain Adaptation for Video-Text Retrieval| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hao_Dual_Alignment_Unsupervised_Domain_Adaptation_for_Video-Text_Retrieval_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FuwfdNEpfsM) |
| Task Residual for Tuning Vision-Language Models | [![GitHub](https://img.shields.io/github/stars/geekyutao/TaskRes?style=flat)](https://github.com/geekyutao/TaskRes) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Task_Residual_for_Tuning_Vision-Language_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.10277-b31b1b.svg)](http://arxiv.org/abs/2211.10277) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VbkT-AS3ZRA) |
| Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models| [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://bluestyle97.github.io/dream3d/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Dream3D_Zero-Shot_Text-to-3D_Synthesis_Using_3D_Shape_Prior_and_Text-to-Image_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.14704-b31b1b.svg)](http://arxiv.org/abs/2212.14704) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rjomF5oe4_M) |
| End-to-End 3D Dense Captioning With Vote2Cap-DETR | [![GitHub](https://img.shields.io/github/stars/ch3cook-fdu/Vote2Cap-DETR?style=flat)](https://github.com/ch3cook-fdu/Vote2Cap-DETR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_End-to-End_3D_Dense_Captioning_With_Vote2Cap-DETR_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.02508-b31b1b.svg)](http://arxiv.org/abs/2301.02508) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=azR_OvPWYfo) |
| Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Towards_Generalisable_Video_Moment_Retrieval_Visual-Dynamic_Injection_to_Image-Text_Pre-Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.00040-b31b1b.svg)](http://arxiv.org/abs/2303.00040) | :heavy_minus_sign: |
| Adaptive Zone-Aware Hierarchical Planner for Vision-Language Navigation | [![GitHub](https://img.shields.io/github/stars/chengaopro/azhp?style=flat)](https://github.com/chengaopro/azhp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Visual Programming: Compositional Visual Reasoning Without Training <br /> ![CVPR - Award](https://img.shields.io/badge/CVPR-Award-294A7C) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://prior.allenai.org/projects/visprog) <br /> [![GitHub](https://img.shields.io/github/stars/allenai/visprog?style=flat)](https://github.com/allenai/visprog) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11559-b31b1b.svg)](http://arxiv.org/abs/2211.11559) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_Gv7VQHPKOo) |
| Exploring the Effect of Primitives for Compositional Generalization in Vision-and-Language | [![GitHub](https://img.shields.io/github/stars/NeverMoreLCH/SSL2CG?style=flat)](https://github.com/NeverMoreLCH/SSL2CG) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Exploring_the_Effect_of_Primitives_for_Compositional_Generalization_in_Vision-and-Language_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Referring Multi-Object Tracking | [![GitHub](https://img.shields.io/github/stars/wudongming97/RMOT?style=flat)](https://github.com/wudongming97/RMOT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Referring_Multi-Object_Tracking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03366-b31b1b.svg)](http://arxiv.org/abs/2303.03366) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=oXY0s8P5C1g) |
| Mofusion: A Framework for Denoising-Diffusion-Based Motion Synthesis | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://vcai.mpi-inf.mpg.de/projects/MoFusion/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Dabral_Mofusion_A_Framework_for_Denoising-Diffusion-Based_Motion_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04495-b31b1b.svg)](http://arxiv.org/abs/2212.04495) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Woo73Gy2jUg) |
| MIST: Multi-Modal Iterative Spatial-Temporal Transformer for Long-Form Video Question Answering | [![GitHub](https://img.shields.io/github/stars/showlab/mist?style=flat)](https://github.com/showlab/mist) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_MIST_Multi-Modal_Iterative_Spatial-Temporal_Transformer_for_Long-Form_Video_Question_Answering_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09522-b31b1b.svg)](http://arxiv.org/abs/2212.09522) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Tov7aR_Dnr0) |
| Learning To Segment Every Referring Object Point by Point | [![GitHub](https://img.shields.io/github/stars/qumengxue/Partial-RES?style=flat)](https://github.com/qumengxue/Partial-RES) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_Learning_To_Segment_Every_Referring_Object_Point_by_Point_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=XRKem03hl_k) |
| Contrastive Grouping With Transformer for Referring Image Segmentation | [![GitHub](https://img.shields.io/github/stars/SooLab/CGFormer?style=flat)](https://github.com/SooLab/CGFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Contrastive_Grouping_With_Transformer_for_Referring_Image_Segmentation_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Prototype-Based Embedding Network for Scene Graph Generation | [![GitHub](https://img.shields.io/github/stars/VL-Group/PENET?style=flat)](https://github.com/VL-Group/PENET) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Prototype-Based_Embedding_Network_for_Scene_Graph_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07096-b31b1b.svg)](http://arxiv.org/abs/2303.07096) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pMBYBYlmkNQ) |
| Collaborative Static and Dynamic Vision-Language Streams for Spatio-Temporal Video Grounding | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Collaborative_Static_and_Dynamic_Vision-Language_Streams_for_Spatio-Temporal_Video_Grounding_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=NmfykPpl1vE) |
| S<sup>3</sup>C: Semi-Supervised VQA Natural Language Explanation via Self-Critical Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Suo_S3C_Semi-Supervised_VQA_Natural_Language_Explanation_via_Self-Critical_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.02155-b31b1b.svg)](https://arxiv.org/abs/2309.02155) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7p_rkzsaXnk) |
| REVEAL: Retrieval-Augmented Visual-Language Pre-Training With Multi-Source Multimodal Knowledge Memory <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://reveal-cvpr.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_REVEAL_Retrieval-Augmented_Visual-Language_Pre-Training_With_Multi-Source_Multimodal_Knowledge_Memory_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.05221-b31b1b.svg)](http://arxiv.org/abs/2212.05221) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CqPxhsggb_I) |
| Prompting Large Language Models With Answer Heuristics for Knowledge-Based Visual Question Answering | [![GitHub](https://img.shields.io/github/stars/MILVLG/prophet?style=flat)](https://github.com/MILVLG/prophet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Shao_Prompting_Large_Language_Models_With_Answer_Heuristics_for_Knowledge-Based_Visual_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.01903-b31b1b.svg)](http://arxiv.org/abs/2303.01903) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=IjRpn5rDLyo) |
| Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval? <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/whwu95/Cap4Video?style=flat)](https://github.com/whwu95/Cap4Video) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Cap4Video_What_Can_Auxiliary_Captions_Do_for_Text-Video_Retrieval_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.00184-b31b1b.svg)](http://arxiv.org/abs/2301.00184) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0-Uq-j259nI) |
| Video-Text As Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jpthu17.github.io/HBI/) <br /> [![GitHub](https://img.shields.io/github/stars/jpthu17/HBI?style=flat)](https://github.com/jpthu17/HBI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Video-Text_As_Game_Players_Hierarchical_Banzhaf_Interaction_for_Cross-Modal_Representation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14369-b31b1b.svg)](http://arxiv.org/abs/2303.14369) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=UeRRVHEP79c) |
| HAAV: Hierarchical Aggregation of Augmented Views for Image Captioning | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sites.google.com/view/chiawen-kuo/home/haav) <br /> [![GitHub](https://img.shields.io/github/stars/GT-RIPL/HAAV?style=flat)](https://github.com/GT-RIPL/HAAV) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kuo_HAAV_Hierarchical_Aggregation_of_Augmented_Views_for_Image_Captioning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.16295-b31b1b.svg)](https://arxiv.org/abs/2305.16295) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=s9nq8T51Gs0) |
| Zero-Shot Referring Image Segmentation With Global-Local Context Features | [![GitHub](https://img.shields.io/github/stars/Seonghoon-Yu/Zero-shot-RIS?style=flat)](https://github.com/Seonghoon-Yu/Zero-shot-RIS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Zero-Shot_Referring_Image_Segmentation_With_Global-Local_Context_Features_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17811-b31b1b.svg)](http://arxiv.org/abs/2303.17811) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=X_37jodjz2Y) |
| Hierarchical Semantic Correspondence Networks for Video Paragraph Grounding | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Hierarchical_Semantic_Correspondence_Networks_for_Video_Paragraph_Grounding_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=fkGopOsJAF8) |
| Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training | [![GitHub](https://img.shields.io/github/stars/facebookresearch/diht?style=flat)](https://github.com/facebookresearch/diht) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Radenovic_Filtering_Distillation_and_Hard_Negatives_for_Vision-Language_Pre-Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.02280-b31b1b.svg)](http://arxiv.org/abs/2301.02280) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qvLxglovwDU) |
| Probabilistic Prompt Learning for Dense Prediction| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kwon_Probabilistic_Prompt_Learning_for_Dense_Prediction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00779-b31b1b.svg)](http://arxiv.org/abs/2304.00779) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6PMEDyNa2xQ) |
| DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-Training via Word-Region Alignment | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_DetCLIPv2_Scalable_Open-Vocabulary_Object_Detection_Pre-Training_via_Word-Region_Alignment_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04514-b31b1b.svg)](http://arxiv.org/abs/2304.04514) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LIUcWY84Egg) |
| All in One: Exploring Unified Video-Language Pre-Training | [![GitHub](https://img.shields.io/github/stars/showlab/all-in-one?style=flat)](https://github.com/showlab/all-in-one) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_All_in_One_Exploring_Unified_Video-Language_Pre-Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2203.07303-b31b1b.svg)](http://arxiv.org/abs/2203.07303) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=sH6tdF8OTPM) |
| Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding | [![GitHub](https://img.shields.io/github/stars/TAU-VAILab/isbertblind?style=flat)](https://github.com/TAU-VAILab/isbertblind) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Alper_Is_BERT_Blind_Exploring_the_Effect_of_Vision-and-Language_Pretraining_on_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12513-b31b1b.svg)](http://arxiv.org/abs/2303.12513) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=iZcA0Q07naY) |
| Divide and Conquer: Answering Questions With Object Factorization and Compositional Reasoning | [![GitHub](https://img.shields.io/github/stars/szzexpoi/POEM?style=flat)](https://github.com/szzexpoi/POEM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Divide_and_Conquer_Answering_Questions_With_Object_Factorization_and_Compositional_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10482-b31b1b.svg)](http://arxiv.org/abs/2303.10482) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WtBq2BKnRd0) |
| ConZIC: Controllable Zero-Shot Image Captioning by Sampling-Based Polishing | [![GitHub](https://img.shields.io/github/stars/joeyz0z/ConZIC?style=flat)](https://github.com/joeyz0z/ConZIC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_ConZIC_Controllable_Zero-Shot_Image_Captioning_by_Sampling-Based_Polishing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02437-b31b1b.svg)](http://arxiv.org/abs/2303.02437) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=sHbE88HsvNg) |
| RefCLIP: A Universal Teacher for Weakly Supervised Referring Expression Comprehension | [![GitHub](https://img.shields.io/github/stars/kingthreestones/RefCLIP?style=flat)](https://github.com/kingthreestones/RefCLIP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_RefCLIP_A_Universal_Teacher_for_Weakly_Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation | [![GitHub](https://img.shields.io/github/stars/XiangyangLi20/KERM?style=flat)](https://github.com/XiangyangLi20/KERM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_KERM_Knowledge_Enhanced_Reasoning_for_Vision-and-Language_Navigation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15796-b31b1b.svg)](http://arxiv.org/abs/2303.15796) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Afm4rymYtMM) |
| ANetQA: A Large-Scale Benchmark for Fine-Grained Compositional Reasoning Over Untrimmed Videos | [![GitHub](https://img.shields.io/github/stars/MILVLG/anetqa-code?style=flat)](https://github.com/MILVLG/anetqa-code) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_ANetQA_A_Large-Scale_Benchmark_for_Fine-Grained_Compositional_Reasoning_Over_Untrimmed_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.02519-b31b1b.svg)](http://arxiv.org/abs/2305.02519) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hVvfP_xCUGM) |
| ViLEM: Visual-Language Error Modeling for Image-Text Retrieval | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_ViLEM_Visual-Language_Error_Modeling_for_Image-Text_Retrieval_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Multi-Modal Representation Learning With Text-Driven Soft Masks | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Park_Multi-Modal_Representation_Learning_With_Text-Driven_Soft_Masks_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00719-b31b1b.svg)](http://arxiv.org/abs/2304.00719) | :heavy_minus_sign: |
| Meta-Personalizing Vision-Language Models To Find Named Instances in Video | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://danielchyeh.github.io/metaper/) <br /> [![GitHub](https://img.shields.io/github/stars/danielchyeh/this-is-my?style=flat)](https://github.com/danielchyeh/this-is-my) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yeh_Meta-Personalizing_Vision-Language_Models_To_Find_Named_Instances_in_Video_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.10169-b31b1b.svg)](https://arxiv.org/abs/2306.10169) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DnOOThEGZmU) |
| ReCo: Region-Controlled Text-to-Image Generation | [![GitHub](https://img.shields.io/github/stars/microsoft/ReCo?style=flat)](https://github.com/microsoft/ReCo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_ReCo_Region-Controlled_Text-to-Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.15518-b31b1b.svg)](http://arxiv.org/abs/2211.15518) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8X0zvl05mtU) |
| Are Deep Neural Networks SMARTer Than Second Graders? | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://smartdataset.github.io/smart101/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cherian_Are_Deep_Neural_Networks_SMARTer_Than_Second_Graders_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09993-b31b1b.svg)](http://arxiv.org/abs/2212.09993) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nLtlC1fs5vY) |
| Graph Representation for Order-Aware Visual Transformation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Qiu_Graph_Representation_for_Order-Aware_Visual_Transformation_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| 3D Concept Learning and Reasoning From Multi-View Images| [![GitHub](https://img.shields.io/github/stars/evelinehong/3D-CLR-Official?style=flat)](https://github.com/evelinehong/3D-CLR-Official) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hong_3D_Concept_Learning_and_Reasoning_From_Multi-View_Images_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11327-b31b1b.svg)](http://arxiv.org/abs/2303.11327) | :heavy_minus_sign: |
| Text With Knowledge Graph Augmented Transformer for Video Captioning | [![GitHub](https://img.shields.io/github/stars/nightonion/textkg?style=flat)](https://github.com/nightonion/textkg) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Gu_Text_With_Knowledge_Graph_Augmented_Transformer_for_Video_Captioning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12423-b31b1b.svg)](http://arxiv.org/abs/2303.12423) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=GCxQ8pKalcM) |
| Crossing the Gap: Domain Generalization for Image Captioning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Crossing_the_Gap_Domain_Generalization_for_Image_Captioning_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KdZTnBMaQ5c) |
| MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering | [![GitHub](https://img.shields.io/github/stars/jingjing12110/MixPHM?style=flat)](https://github.com/jingjing12110/MixPHM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_MixPHM_Redundancy-Aware_Parameter-Efficient_Tuning_for_Low-Resource_Visual_Question_Answering_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.01239-b31b1b.svg)](http://arxiv.org/abs/2303.01239) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=NMnrcXqYaZo) |
| VQACL: A Novel Visual Question Answering Continual Learning Setting | [![GitHub](https://img.shields.io/github/stars/zhangxi1997/VQACL?style=flat)](https://github.com/zhangxi1997/VQACL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_VQACL_A_Novel_Visual_Question_Answering_Continual_Learning_Setting_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zlkJEQEjTKc) |
| Improving Selective Visual Question Answering by Learning From Your Peers | [![GitHub](https://img.shields.io/github/stars/facebookresearch/selective-vqa_ood?style=flat)](https://github.com/facebookresearch/selective-vqa_ood) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Dancette_Improving_Selective_Visual_Question_Answering_by_Learning_From_Your_Peers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.08751-b31b1b.svg)](https://arxiv.org/abs/2306.08751) | :heavy_minus_sign: |
| High-Fidelity 3D Face Generation From Natural Language Descriptions | [![GitHub](https://img.shields.io/github/stars/zhuhao-nju/describe3d?style=flat)](https://github.com/zhuhao-nju/describe3d) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_High-Fidelity_3D_Face_Generation_From_Natural_Language_Descriptions_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.03302-b31b1b.svg)](http://arxiv.org/abs/2305.03302) | :heavy_minus_sign: |
| Language-Guided Audio-Visual Source Separation via Trimodal Consistency | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cs-people.bu.edu/rxtan/projects/VAST/) <br /> [![GitHub](https://img.shields.io/github/stars/rxtan2/AVSeT?style=flat)](https://github.com/rxtan2/AVSeT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Language-Guided_Audio-Visual_Source_Separation_via_Trimodal_Consistency_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16342-b31b1b.svg)](http://arxiv.org/abs/2303.16342) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tjrmTEOJ7Wk) |
| Test of Time: Instilling Video-Language Models With a Sense of Time | [![GitHub](https://img.shields.io/github/stars/bpiyush/TestOfTime?style=flat)](https://github.com/bpiyush/TestOfTime) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Bagad_Test_of_Time_Instilling_Video-Language_Models_With_a_Sense_of_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.02074-b31b1b.svg)](http://arxiv.org/abs/2301.02074) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=RTRRdrA5H88) |
| Learning Situation Hyper-Graphs for Video Question Answering | [![GitHub](https://img.shields.io/github/stars/aurooj/SHG-VQA?style=flat)](https://github.com/aurooj/SHG-VQA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Urooj_Learning_Situation_Hyper-Graphs_for_Video_Question_Answering_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08682-b31b1b.svg)](http://arxiv.org/abs/2304.08682) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lxkFfadg_78) |
| Pic2Word: Mapping Pictures to Words for Zero-Shot Composed Image Retrieval | [![GitHub](https://img.shields.io/github/stars/google-research/composed_image_retrieval?style=flat)](https://github.com/google-research/composed_image_retrieval) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Saito_Pic2Word_Mapping_Pictures_to_Words_for_Zero-Shot_Composed_Image_Retrieval_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.03084-b31b1b.svg)](http://arxiv.org/abs/2302.03084) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=m-Z72lBPeYc) |
| Fine-Grained Audible Video Description | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](http://www.avlbench.opennlplab.cn/papers/favd) <br /> [![GitHub](https://img.shields.io/github/stars/OpenNLPLab/FAVDBench?style=flat)](https://github.com/OpenNLPLab/FAVDBench) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_Fine-Grained_Audible_Video_Description_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15616-b31b1b.svg)](http://arxiv.org/abs/2303.15616) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=sIuYDu6t9hk) |
| Discovering the Real Association: Multimodal Causal Reasoning in Video Question Answering | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zang_Discovering_the_Real_Association_Multimodal_Causal_Reasoning_in_Video_Question_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| A-Cap: Anticipation Captioning With Commonsense Knowledge | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Vo_A-Cap_Anticipation_Captioning_With_Commonsense_Knowledge_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06602-b31b1b.svg)](https://arxiv.org/abs/2304.06602) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kJtIXqXsDHQ) |
| Cross-Domain Image Captioning With Discriminative Finetuning| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Dessi_Cross-Domain_Image_Captioning_With_Discriminative_Finetuning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01662-b31b1b.svg)](https://arxiv.org/abs/2304.01662) | :heavy_minus_sign: |
| Improving Visual Grounding by Encouraging Consistent Gradient-Based Explanations | [![GitHub](https://img.shields.io/github/stars/uvavision/AMC-grounding?style=flat)](https://github.com/uvavision/AMC-grounding) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Improving_Visual_Grounding_by_Encouraging_Consistent_Gradient-Based_Explanations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.15462-b31b1b.svg)](http://arxiv.org/abs/2206.15462) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=meZu2mKfhUM) |
| The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training | [![GitHub](https://img.shields.io/github/stars/gicheonkang/gst-visdial?style=flat)](https://github.com/gicheonkang/gst-visdial) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_The_Dialog_Must_Go_On_Improving_Visual_Dialog_via_Generative_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2205.12502-b31b1b.svg)](http://arxiv.org/abs/2205.12502) |  [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=SrOzZRyqezs) |
| Q: How To Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images! | [![GitHub](https://img.shields.io/github/stars/codezakh/SelTDA?style=flat)](https://github.com/codezakh/SelTDA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Khan_Q_How_To_Specialize_Large_Vision-Language_Models_to_Data-Scarce_VQA_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.03932-b31b1b.svg)](https://arxiv.org/abs/2306.03932) | :heavy_minus_sign: |
| Open-Vocabulary Semantic Segmentation With Mask-Adapted CLIP | [![GitHub](https://img.shields.io/github/stars/facebookresearch/ov-seg?style=flat)](https://github.com/facebookresearch/ov-seg) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_Open-Vocabulary_Semantic_Segmentation_With_Mask-Adapted_CLIP_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.04150-b31b1b.svg)](http://arxiv.org/abs/2210.04150) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xIUSG0pLNyo) |
| Similarity Maps for Self-Training Weakly-Supervised Phrase Grounding | [![GitHub](https://img.shields.io/github/stars/talshaharabany/Similarity-Maps-for-Self-Training-Weakly-Supervised-Phrase-Grounding?style=flat)](https://github.com/talshaharabany/Similarity-Maps-for-Self-Training-Weakly-Supervised-Phrase-Grounding) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Shaharabany_Similarity_Maps_for_Self-Training_Weakly-Supervised_Phrase_Grounding_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0wg84aYN9U4) |
| Language Adaptive Weight Generation for Multi-Task Visual Grounding | [![GitHub](https://img.shields.io/github/stars/Charles-Xie/awesome-described-object-detection?style=flat)](https://github.com/Charles-Xie/awesome-described-object-detection) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Su_Language_Adaptive_Weight_Generation_for_Multi-Task_Visual_Grounding_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.04652-b31b1b.svg)](https://arxiv.org/abs/2306.04652) | :heavy_minus_sign: |
| CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition With Variational Alignment <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/binbinjiang/CVT-SLR?style=flat)](https://github.com/binbinjiang/CVT-SLR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_CVT-SLR_Contrastive_Visual-Textual_Transformation_for_Sign_Language_Recognition_With_Variational_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05725-b31b1b.svg)](https://arxiv.org/abs/2303.05725) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=IVLqr2SNZiE) |
| Region-Aware Pretraining for Open-Vocabulary Object Detection With Vision Transformers <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/mcahny/rovit?style=flat)](https://github.com/mcahny/rovit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Region-Aware_Pretraining_for_Open-Vocabulary_Object_Detection_With_Vision_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.07011-b31b1b.svg)](http://arxiv.org/abs/2305.07011) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=u2gb8Q20B2k) |
| A Simple Framework for Text-Supervised Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/muyangyi/SimSeg?style=flat)](https://github.com/muyangyi/SimSeg) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_A_Simple_Framework_for_Text-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=smL7mboV3l0) |
| Learning To Name Classes for Vision and Language Models | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Parisot_Learning_To_Name_Classes_for_Vision_and_Language_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01830-b31b1b.svg)](http://arxiv.org/abs/2304.01830) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=O1nW_7Xjmys) |
| Iterative Vision-and-Language Navigation | [![GitHub](https://img.shields.io/github/stars/jacobkrantz/IVLN-CE?style=flat)](https://github.com/jacobkrantz/IVLN-CE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Krantz_Iterative_Vision-and-Language_Navigation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.03087-b31b1b.svg)](http://arxiv.org/abs/2210.03087) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nxWUedX5VpQ) |
| Behavioral Analysis of Vision-and-Language Navigation Agents | [![GitHub](https://img.shields.io/github/stars/Yoark/vln-behave?style=flat)](https://github.com/Yoark/vln-behave) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Behavioral_Analysis_of_Vision-and-Language_Navigation_Agents_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.10790-b31b1b.svg)](https://arxiv.org/abs/2307.10790) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0bdanHI6I-U) |
| Towards Fast Adaptation of Pretrained Contrastive Models for Multi-Channel Video-Language Retrieval| [![GitHub](https://img.shields.io/github/stars/XudongLinthu/upgradable-multimodal-intelligence?style=flat)](https://github.com/XudongLinthu/upgradable-multimodal-intelligence) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Towards_Fast_Adaptation_of_Pretrained_Contrastive_Models_for_Multi-Channel_Video-Language_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.02082-b31b1b.svg)](http://arxiv.org/abs/2206.02082) |[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=L6ePHpy9TYM) |
| SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://liuxubo717.github.io/SynthVSR/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_SynthVSR_Scaling_Up_Visual_Speech_Recognition_With_Synthetic_Supervision_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17200-b31b1b.svg)](http://arxiv.org/abs/2303.17200) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PFA4ACgq3O4) |
| METransformer: Radiology Report Generation by Transformer With Multiple Learnable Expert Tokens| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_METransformer_Radiology_Report_Generation_by_Transformer_With_Multiple_Learnable_Expert_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02211-b31b1b.svg)](http://arxiv.org/abs/2304.02211) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=GtQhE51Jsxs) |
| Fusing Pre-Trained Language Models With Multimodal Prompts Through Reinforcement Learning | [![GitHub](https://img.shields.io/github/stars/JiwanChung/esper?style=flat)](https://github.com/JiwanChung/esper) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Fusing_Pre-Trained_Language_Models_With_Multimodal_Prompts_Through_Reinforcement_Learning_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nY3izP28d-g) |
| Abstract Visual Reasoning: An Algebraic Approach for Solving Raven's Progressive Matrices | [![GitHub](https://img.shields.io/github/stars/Xu-Jingyi/AlgebraicMR?style=flat)](https://github.com/Xu-Jingyi/AlgebraicMR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Abstract_Visual_Reasoning_An_Algebraic_Approach_for_Solving_Ravens_Progressive_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11730-b31b1b.svg)](https://arxiv.org/abs/2303.11730) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=yRWJdxzN-3g) |
| Hierarchical Prompt Learning for Multi-Task Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Hierarchical_Prompt_Learning_for_Multi-Task_Learning_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=gICLBA-MYrs) |
| Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image Person Retrieval | [![GitHub](https://img.shields.io/github/stars/anosorae/IRRA?style=flat)](https://github.com/anosorae/IRRA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Cross-Modal_Implicit_Relation_Reasoning_and_Aligning_for_Text-to-Image_Person_Retrieval_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12501-b31b1b.svg)](http://arxiv.org/abs/2303.12501) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=BzX3bI8k69M) |
| SViTT: Temporal Learning of Sparse Video-Text Transformers | [![GitHub](https://img.shields.io/github/stars/jerryyli/svitt?style=flat)](https://github.com/jerryyli/svitt/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SViTT_Temporal_Learning_of_Sparse_Video-Text_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08809-b31b1b.svg)](http://arxiv.org/abs/2304.08809) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=11MZj4xkZyY) |
| How You Feelin'? Learning Emotions and Mental States in Movie Scenes | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://katha-ai.github.io/projects/emotx/) <br /> [![GitHub](https://img.shields.io/github/stars/katha-ai/EmoTx-CVPR2023?style=flat)](https://github.com/katha-ai/EmoTx-CVPR2023) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Srivastava_How_You_Feelin_Learning_Emotions_and_Mental_States_in_Movie_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05634-b31b1b.svg)](http://arxiv.org/abs/2304.05634) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7OATfNOdAzI) |
| Logical Implications for Visual Question Answering Consistency | [![GitHub](https://img.shields.io/github/stars/sergiotasconmorales/imp_vqa?style=flat)](https://github.com/sergiotasconmorales/imp_vqa) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tascon-Morales_Logical_Implications_for_Visual_Question_Answering_Consistency_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09427-b31b1b.svg)](https://arxiv.org/abs/2303.09427) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LBNNw-Erm74) |
| Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/microsoft/unilm/blob/master/beit3/README.md) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Image_as_a_Foreign_Language_BEiT_Pretraining_for_Vision_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.10442-b31b1b.svg)](https://arxiv.org/abs/2208.10442) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ULgQ1lwvTrk) |
| DeCo: Decomposition and Reconstruction for Compositional Temporal Grounding via Coarse-To-Fine Contrastive Ranking | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_DeCo_Decomposition_and_Reconstruction_for_Compositional_Temporal_Grounding_via_Coarse-To-Fine_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=P6kpaHV2uYU) |
| iCLIP: Bridging Image Classification and Contrastive Language-Image Pre-Training for Visual Recognition| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_iCLIP_Bridging_Image_Classification_and_Contrastive_Language-Image_Pre-Training_for_Visual_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VHZk2pp-H48) |
| Semantic-Conditional Diffusion Networks for Image Captioning | [![GitHub](https://img.shields.io/github/stars/jianjieluo/SCD-Net?style=flat)](https://github.com/jianjieluo/SCD-Net) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Semantic-Conditional_Diffusion_Networks_for_Image_Captioning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03099-b31b1b.svg)](http://arxiv.org/abs/2212.03099) | :heavy_minus_sign: |
| CREPE: Can Vision-Language Foundation Models Reason Compositionally? <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/RAIVNLab/CREPE?style=flat)](https://github.com/RAIVNLab/CREPE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_CREPE_Can_Vision-Language_Foundation_Models_Reason_Compositionally_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.07796-b31b1b.svg)](http://arxiv.org/abs/2212.07796) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8rN_1HWs8Bc) |
| RMLVQA: A Margin Loss Approach for Visual Question Answering With Language Biases| [![GitHub](https://img.shields.io/github/stars/val-iisc/RMLVQA?style=flat)](https://github.com/val-iisc/RMLVQA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Basu_RMLVQA_A_Margin_Loss_Approach_for_Visual_Question_Answering_With_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Improving Vision-and-Language Navigation by Generating Future-View Image Semantics | [![GitHub](https://img.shields.io/github/stars/jialuli-luka/VLN-SIG?style=flat)](https://github.com/jialuli-luka/VLN-SIG) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Improving_Vision-and-Language_Navigation_by_Generating_Future-View_Image_Semantics_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04907-b31b1b.svg)](http://arxiv.org/abs/2304.04907) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=l_9YIMCk_P0) |
| Prefix Conditioning Unifies Language and Label Supervision | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Saito_Prefix_Conditioning_Unifies_Language_and_Label_Supervision_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.01125-b31b1b.svg)](http://arxiv.org/abs/2206.01125) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-8KU1S7VqWw) |
| A New Path: Scaling Vision-and-Language Navigation With Synthetic Instructions and Imitation Learning| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kamath_A_New_Path_Scaling_Vision-and-Language_Navigation_With_Synthetic_Instructions_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.03112-b31b1b.svg)](http://arxiv.org/abs/2210.03112) | :heavy_minus_sign: |
| From Images to Textual Prompts: Zero-Shot Visual Question Answering With Frozen Large Language Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/salesforce/LAVIS/tree/main/projects/img2llm-vqa) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_From_Images_to_Textual_Prompts_Zero-Shot_Visual_Question_Answering_With_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.10846-b31b1b.svg)](https://arxiv.org/abs/2212.10846) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Vvej9bCQ33o) |
| Hierarchical Video-Moment Retrieval and Step-Captioning | [![GitHub](https://img.shields.io/github/stars/j-min/HiREST?style=flat)](https://github.com/j-min/HiREST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zala_Hierarchical_Video-Moment_Retrieval_and_Step-Captioning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16406-b31b1b.svg)](http://arxiv.org/abs/2303.16406) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=U1o9g_8cNJ4) |
