# CVPR-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/2023/main/recognition-categorization-detection-retrieval.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/2023/main/low-level-vision.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Vision, Language, and Reasoning

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://antoyang.github.io/vid2seq.html) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Vid2Seq_Large-Scale_Pretraining_of_a_Visual_Language_Model_for_Dense_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.14115-b31b1b.svg)](http://arxiv.org/abs/2302.14115) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hXP-2fYzq4g) |
| Open-Vocabulary Panoptic Segmentation With Text-to-Image Diffusion Models <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/NVlabs/ODISE?style=flat)](https://github.com/NVlabs/ODISE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Open-Vocabulary_Panoptic_Segmentation_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.04803-b31b1b.svg)](http://arxiv.org/abs/2303.04803)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=eW2vF8o_7p0) |
| Iterative Proposal Refinement for Weakly-Supervised Video Grounding | [![GitHub](https://img.shields.io/github/stars/ttengwang/Awesome_Long_Term_Video_Understanding?style=flat)](https://github.com/ttengwang/Awesome_Long_Term_Video_Understanding) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Iterative_Proposal_Refinement_for_Weakly-Supervised_Video_Grounding_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=BbGvHI_pVXk) |
| MetaCLUE: Towards Comprehensive Visual Metaphors Research | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://metaclue.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Akula_MetaCLUE_Towards_Comprehensive_Visual_Metaphors_Research_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09898-b31b1b.svg)](http://arxiv.org/abs/2212.09898) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=V3TmeNETL-o) |
| PolyFormer: Referring Image Segmentation As Sequential Polygon Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://polyformer.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/amazon-science/polygon-transformer?style=flat)](https://github.com/amazon-science/polygon-transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_PolyFormer_Referring_Image_Segmentation_As_Sequential_Polygon_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.07387-b31b1b.svg)](http://arxiv.org/abs/2302.07387) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6LNrqoxQR1M) |
| GeneCIS: A Benchmark for General Conditional Image Similarity <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) |[![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sgvaze.github.io/genecis/) <br /> [![GitHub](https://img.shields.io/github/stars/facebookresearch/genecis?style=flat)](https://github.com/facebookresearch/genecis) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Vaze_GeneCIS_A_Benchmark_for_General_Conditional_Image_Similarity_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.07969-b31b1b.svg)](https://arxiv.org/abs/2306.07969)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wu3U2iNGIUw) |
| FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) |[![GitHub](https://img.shields.io/github/stars/BrandonHanx/FAME-ViL?style=flat)](https://github.com/BrandonHanx/FAME-ViL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Han_FAME-ViL_Multi-Tasking_Vision-Language_Model_for_Heterogeneous_Fashion_Tasks_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02483-b31b1b.svg)](https://arxiv.org/abs/2303.02483) | :heavy_minus_sign: |
| Generative Bias for Robust Visual Question Answering | [![GitHub](https://img.shields.io/github/stars/chojw/genb?style=flat)](https://github.com/chojw/genb) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Generative_Bias_for_Robust_Visual_Question_Answering_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.00690-b31b1b.svg)](http://arxiv.org/abs/2208.00690)| :heavy_minus_sign: |
| Advancing Visual Grounding With Scene Knowledge: Benchmark and Method | [![GitHub](https://img.shields.io/github/stars/zhjohnchan/SK-VG?style=flat)](https://github.com/zhjohnchan/SK-VG) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Advancing_Visual_Grounding_With_Scene_Knowledge_Benchmark_and_Method_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11558-b31b1b.svg)](https://arxiv.org/abs/2307.11558) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DmmPiseO59o) |
| Gloss Attention for Gloss-Free Sign Language Translation | [![GitHub](https://img.shields.io/github/stars/YinAoXiong/GASLT?style=flat)](https://github.com/YinAoXiong/GASLT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_Gloss_Attention_for_Gloss-Free_Sign_Language_Translation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.07361-b31b1b.svg)](https://arxiv.org/abs/2307.07361) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=NEoWvxkJXfU) |
| You Can Ground Earlier Than See: An Effective and Efficient Pipeline for Temporal Sentence Grounding in Compressed Videos | :heavy_minus_sign:|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_You_Can_Ground_Earlier_Than_See_An_Effective_and_Efficient_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07863-b31b1b.svg)](http://arxiv.org/abs/2303.07863) | :heavy_minus_sign: |
| Generalized Decoding for Pixel, Image, and Language | [![GitHub](https://img.shields.io/github/stars/microsoft/X-Decoder?style=flat)](https://github.com/microsoft/X-Decoder) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zou_Generalized_Decoding_for_Pixel_Image_and_Language_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.11270-b31b1b.svg)](http://arxiv.org/abs/2212.11270) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wYp6vmyolqE) |
| Accelerating Vision-Language Pretraining With Free Language Modeling | [![GitHub](https://img.shields.io/github/stars/TencentARC/FLM?style=flat)](https://github.com/TencentARC/FLM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Accelerating_Vision-Language_Pretraining_With_Free_Language_Modeling_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14038-b31b1b.svg)](http://arxiv.org/abs/2303.14038) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WbH_5DH_jfY) |
| GRES: Generalized Referring Expression Segmentation <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://henghuiding.github.io/GRES/) <br /> [![GitHub](https://img.shields.io/github/stars/henghuiding/ReLA?style=flat)](https://github.com/henghuiding/ReLA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_GRES_Generalized_Referring_Expression_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.00968-b31b1b.svg)](http://arxiv.org/abs/2306.00968) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=eWjAgYUU6Do) |
| BUFFER: Balancing Accuracy, Efficiency, and Generalizability in Point Cloud Registration | [![GitHub](https://img.shields.io/github/stars/The-Learning-And-Vision-Atelier-LAVA/BUFFER?style=flat)](https://github.com/The-Learning-And-Vision-Atelier-LAVA/BUFFER) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ao_BUFFER_Balancing_Accuracy_Efficiency_and_Generalizability_in_Point_Cloud_Registration_CVPR_2023_paper.pdf)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=STmAkRWuSiY) |
| RGB No More: Minimally-Decoded JPEG Vision Transformers | [![GitHub](https://img.shields.io/github/stars/JeongsooP/RGB-no-more?style=flat)](https://github.com/JeongsooP/RGB-no-more) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Park_RGB_No_More_Minimally-Decoded_JPEG_Vision_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.16421-b31b1b.svg)](http://arxiv.org/abs/2211.16421) | :heavy_minus_sign: |
| Scaling Language-Image Pre-Training via Masking | [![GitHub](https://img.shields.io/github/stars/facebookresearch/flip?style=flat)](https://github.com/facebookresearch/flip)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Scaling_Language-Image_Pre-Training_via_Masking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.00794-b31b1b.svg)](http://arxiv.org/abs/2212.00794) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=coOHTSMWhb8) |
| EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding | [![GitHub](https://img.shields.io/github/stars/yanmin-wu/EDA?style=flat)](https://github.com/yanmin-wu/EDA)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_EDA_Explicit_Text-Decoupling_and_Dense_Alignment_for_3D_Visual_Grounding_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.14941-b31b1b.svg)](http://arxiv.org/abs/2209.14941) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=YBpPqYU07Es) |
| RefTeacher: A Strong Baseline for Semi-Supervised Referring Expression Comprehension | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://refteacher.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/Disguiser15/RefTeacher?style=flat)](https://github.com/Disguiser15/RefTeacher) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sun_RefTeacher_A_Strong_Baseline_for_Semi-Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Mobile User Interface Element Detection via Adaptively Prompt Tuning | [![GitHub](https://img.shields.io/github/stars/antmachineintelligence/MUI-zh?style=flat)](https://github.com/antmachineintelligence/MUI-zh)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gu_Mobile_User_Interface_Element_Detection_via_Adaptively_Prompt_Tuning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.09699-b31b1b.svg)](http://arxiv.org/abs/2305.09699) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dMC26H1DQWw) |
| Context-Aware Alignment and Mutual Masking for 3D-Language Pre-Training <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]()  | [![GitHub](https://img.shields.io/github/stars/leolyj/3D-VLP?style=flat)](https://github.com/leolyj/3D-VLP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jin_Context-Aware_Alignment_and_Mutual_Masking_for_3D-Language_Pre-Training_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Tell Me What Happened: Unifying Text-Guided Video Completion via Multimodal Masked Video Generation | [![GitHub](https://img.shields.io/github/stars/tsujuifu/pytorch_tvc?style=flat)](https://github.com/tsujuifu/pytorch_tvc) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Fu_Tell_Me_What_Happened_Unifying_Text-Guided_Video_Completion_via_Multimodal_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12824-b31b1b.svg)](http://arxiv.org/abs/2211.12824) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dnBzUfsf9Cc) |
| Meta Compositional Referring Expression Segmentation |  :heavy_minus_sign:  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Meta_Compositional_Referring_Expression_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04415-b31b1b.svg)](http://arxiv.org/abs/2304.04415) | :heavy_minus_sign: |
| VindLU: A Recipe for Effective Video-and-Language Pretraining | [![GitHub](https://img.shields.io/github/stars/klauscc/VindLU?style=flat)](https://github.com/klauscc/VindLU) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cheng_VindLU_A_Recipe_for_Effective_Video-and-Language_Pretraining_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.05051-b31b1b.svg)](http://arxiv.org/abs/2212.05051) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9koWpSPcYBQ) |
| Super-CLEVR: A Virtual Benchmark To Diagnose Domain Robustness in Visual Reasoning <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/Lizw14/Super-CLEVR?style=flat)](https://github.com/Lizw14/Super-CLEVR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Super-CLEVR_A_Virtual_Benchmark_To_Diagnose_Domain_Robustness_in_Visual_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.00259-b31b1b.svg)](https://arxiv.org/abs/2212.00259) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DWRp_70ypiA) |
| GIVL: Improving Geographical Inclusivity of Vision-Language Models With Pre-Training Methods| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yin_GIVL_Improving_Geographical_Inclusivity_of_Vision-Language_Models_With_Pre-Training_Methods_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.01893-b31b1b.svg)](http://arxiv.org/abs/2301.01893) | :heavy_minus_sign: |
| Learning Customized Visual Models With Retrieval-Augmented Knowledge <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://react-vl.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/microsoft/react?style=flat)](https://github.com/microsoft/react) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_Learning_Customized_Visual_Models_With_Retrieval-Augmented_Knowledge_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.07094-b31b1b.svg)](http://arxiv.org/abs/2301.07094) | :heavy_minus_sign: |
| LAVENDER: Unifying Video-Language Understanding As Masked Language Modeling | [![GitHub](https://img.shields.io/github/stars/microsoft/LAVENDER?style=flat)](https://github.com/microsoft/LAVENDER) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_LAVENDER_Unifying_Video-Language_Understanding_As_Masked_Language_Modeling_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.07160-b31b1b.svg)](http://arxiv.org/abs/2206.07160) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=f8scI82_caE) |
| An Empirical Study of End-to-End Video-Language Transformers With Masked Visual Modeling | [![GitHub](https://img.shields.io/github/stars/tsujuifu/pytorch_empirical-mvm?style=flat)](https://github.com/tsujuifu/pytorch_empirical-mvm)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Fu_An_Empirical_Study_of_End-to-End_Video-Language_Transformers_With_Masked_Visual_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.01540-b31b1b.svg)](http://arxiv.org/abs/2209.01540) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=T1qTkcMCq1k) |
| NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations | [![GitHub](https://img.shields.io/github/stars/joyhsu0504/NS3D?style=flat)](https://github.com/joyhsu0504/NS3D)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hsu_NS3D_Neuro-Symbolic_Grounding_of_3D_Objects_and_Relations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13483-b31b1b.svg)](http://arxiv.org/abs/2303.13483) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FmWgRR3NKIg) |
| Clover: Towards a Unified Video-Language Alignment and Fusion Model | [![GitHub](https://img.shields.io/github/stars/LeeYN-43/Clover?style=flat)](https://github.com/LeeYN-43/Clover)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Clover_Towards_a_Unified_Video-Language_Alignment_and_Fusion_Model_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2207.07885-b31b1b.svg)](http://arxiv.org/abs/2207.07885) | :heavy_minus_sign: |
| Dual Alignment Unsupervised Domain Adaptation for Video-Text Retrieval| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hao_Dual_Alignment_Unsupervised_Domain_Adaptation_for_Video-Text_Retrieval_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FuwfdNEpfsM) |
| Task Residual for Tuning Vision-Language Models | [![GitHub](https://img.shields.io/github/stars/geekyutao/TaskRes?style=flat)](https://github.com/geekyutao/TaskRes)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_Task_Residual_for_Tuning_Vision-Language_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.10277-b31b1b.svg)](http://arxiv.org/abs/2211.10277) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VbkT-AS3ZRA) |
| Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models| [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://bluestyle97.github.io/dream3d/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Dream3D_Zero-Shot_Text-to-3D_Synthesis_Using_3D_Shape_Prior_and_Text-to-Image_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.14704-b31b1b.svg)](http://arxiv.org/abs/2212.14704)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rjomF5oe4_M) |
| End-to-End 3D Dense Captioning With Vote2Cap-DETR | [![GitHub](https://img.shields.io/github/stars/ch3cook-fdu/Vote2Cap-DETR?style=flat)](https://github.com/ch3cook-fdu/Vote2Cap-DETR)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_End-to-End_3D_Dense_Captioning_With_Vote2Cap-DETR_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.02508-b31b1b.svg)](http://arxiv.org/abs/2301.02508) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=azR_OvPWYfo) |
| Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Luo_Towards_Generalisable_Video_Moment_Retrieval_Visual-Dynamic_Injection_to_Image-Text_Pre-Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.00040-b31b1b.svg)](http://arxiv.org/abs/2303.00040) | :heavy_minus_sign: |
| Adaptive Zone-Aware Hierarchical Planner for Vision-Language Navigation | [![GitHub](https://img.shields.io/github/stars/chengaopro/azhp?style=flat)](https://github.com/chengaopro/azhp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Visual Programming: Compositional Visual Reasoning Without Training <br/> [![CVPR - Award](https://img.shields.io/badge/CVPR-Award-294A7C)]() | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://prior.allenai.org/projects/visprog) <br /> [![GitHub](https://img.shields.io/github/stars/allenai/visprog?style=flat)](https://github.com/allenai/visprog) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11559-b31b1b.svg)](http://arxiv.org/abs/2211.11559) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_Gv7VQHPKOo) |
| Exploring the Effect of Primitives for Compositional Generalization in Vision-and-Language | [![GitHub](https://img.shields.io/github/stars/NeverMoreLCH/SSL2CG?style=flat)](https://github.com/NeverMoreLCH/SSL2CG) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Exploring_the_Effect_of_Primitives_for_Compositional_Generalization_in_Vision-and-Language_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Referring Multi-Object Tracking | [![GitHub](https://img.shields.io/github/stars/wudongming97/RMOT?style=flat)](https://github.com/wudongming97/RMOT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Referring_Multi-Object_Tracking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03366-b31b1b.svg)](http://arxiv.org/abs/2303.03366) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=oXY0s8P5C1g) |
| Mofusion: A Framework for Denoising-Diffusion-Based Motion Synthesis | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://vcai.mpi-inf.mpg.de/projects/MoFusion/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Dabral_Mofusion_A_Framework_for_Denoising-Diffusion-Based_Motion_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04495-b31b1b.svg)](http://arxiv.org/abs/2212.04495) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Woo73Gy2jUg) |
| MIST: Multi-Modal Iterative Spatial-Temporal Transformer for Long-Form Video Question Answering | [![GitHub](https://img.shields.io/github/stars/showlab/mist?style=flat)](https://github.com/showlab/mist) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_MIST_Multi-Modal_Iterative_Spatial-Temporal_Transformer_for_Long-Form_Video_Question_Answering_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09522-b31b1b.svg)](http://arxiv.org/abs/2212.09522) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Tov7aR_Dnr0) |
| Learning To Segment Every Referring Object Point by Point | [![GitHub](https://img.shields.io/github/stars/qumengxue/Partial-RES?style=flat)](https://github.com/qumengxue/Partial-RES) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Qu_Learning_To_Segment_Every_Referring_Object_Point_by_Point_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=XRKem03hl_k) |
| Contrastive Grouping With Transformer for Referring Image Segmentation | [![GitHub](https://img.shields.io/github/stars/SooLab/CGFormer?style=flat)](https://github.com/SooLab/CGFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tang_Contrastive_Grouping_With_Transformer_for_Referring_Image_Segmentation_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Prototype-Based Embedding Network for Scene Graph Generation | [![GitHub](https://img.shields.io/github/stars/VL-Group/PENET?style=flat)](https://github.com/VL-Group/PENET) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zheng_Prototype-Based_Embedding_Network_for_Scene_Graph_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07096-b31b1b.svg)](http://arxiv.org/abs/2303.07096) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pMBYBYlmkNQ) |
| Collaborative Static and Dynamic Vision-Language Streams for Spatio-Temporal Video Grounding | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_Collaborative_Static_and_Dynamic_Vision-Language_Streams_for_Spatio-Temporal_Video_Grounding_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=NmfykPpl1vE) |
| S<sup>3</sup>C: Semi-Supervised VQA Natural Language Explanation via Self-Critical Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Suo_S3C_Semi-Supervised_VQA_Natural_Language_Explanation_via_Self-Critical_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.02155-b31b1b.svg)](https://arxiv.org/abs/2309.02155) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7p_rkzsaXnk) |
| REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory |  |  |  |
| Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering |  |  |  |
| Cap4Video: What can Auxiliary Captions do for Text-Video Retrieval? |  |  |  |
| Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning |  |  |  |
| HAAV: Hierarchical Aggregation of Augmented Views for Image Captioning |  |  |  |
| Zero-Shot Referring Image Segmentation with Global-Local Context Features |  |  |  |
| Hierarchical Semantic Correspondence Networks for Video Paragraph Grounding |  |  |  |
| Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training |  |  |  |
| Probabilistic Prompt Learning for Dense Prediction |  |  |  |
| DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-Training via Word-Region Alignment |  |  |  |
| All in One: Exploring Unified Video-Language Pre-Training |  |  |  |
| Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding |  |  |  |
| Divide and Conquer: Answering Questions with Object Factorization and Compositional Reasoning |  |  |  |
| ConZIC: Controllable Zero-Shot Image Captioning by Sampling-based Polishing |  |  |  |
| RefCLIP: A Universal Teacher for Weakly Supervised Referring Expression Comprehension |  |  |  |
| KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation |  |  |  |
| ANetQA: A Large-Scale Benchmark for Fine-grained Compositional Reasoning over Untrimmed Videos |  |  |  |
| ViLEM: Visual-Language Error Modeling for Image-Text Retrieval |  |  |  |
| Multi-Modal Representation Learning with Text-Driven Soft Masks |  |  |  |
| Meta-Personalizing Vision-Language Models to Find Named Instances in Video |  |  |  |
| ReCo: Region-Controlled Text-to-Image Generation |  |  |  |
| Are Deep Neural Networks SMARTer than Second Graders? |  |  |  |
| Graph Representation for Order-Aware Visual Transformation |  |  |  |
| 3D Concept Learning and Reasoning from Multi-View Images |  |  |  |
| Text with Knowledge Graph Augmented Transformer for Video Captioning |  |  |  |
| Crossing the Gap: Domain Generalization for Image Captioning |  |  |  |
| MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering |  |  |  |
| VQACL: A Novel Visual Question Answering Continual Learning Setting |  |  |  |
| Improving Selective Visual Question Answering by Learning from Your Peers |  |  |  |
| High-Fidelity 3D Face Generation from Natural Language Descriptions |  |  |  |
| Language-guided Audio-Visual Source Separation via Trimodal Consistency |  |  |  |
| Test of Time: Instilling Video-Language Models with a Sense of Time |  |  |  |
| Learning Situation Hyper-Graphs for Video Question Answering |  |  |  |
| Pic2Word: Mapping Pictures to Words for Zero-Shot Composed Image Retrieval |  |  |  |
| Fine-grained Audible Video Description |  |  |  |
| Discovering the Real Association: Multimodal Causal Reasoning in Video Question Answering |  |  |  |
| A-Cap: Anticipation Captioning with Commonsense Knowledge |  |  |  |
| Cross-Domain Image Captioning with Discriminative Finetuning |  |  |  |
| Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations |  |  |  |
| The Dialog Must Go on: Improving Visual Dialog via Generative Self-Training |  |  |  |
| Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images! |  |  |  |
| Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP |  |  |  |
| Similarity Maps for Self-Training Weakly-Supervised Phrase Grounding |  |  |  |
| Language Adaptive Weight Generation for Multi-Task Visual Grounding |  |  |  |
| CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment |  |  |  |
| Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers |  |  |  |
| A Simple Framework for Text-Supervised Semantic Segmentation |  |  |  |
| Learning to Name Classes for Vision and Language Models |  |  |  |
| Iterative Vision-and-Language Navigation |  |  |  |
| Behavioral Analysis of Vision-and-Language Navigation Agents |  |  |  |
| Towards Fast Adaptation of Pretrained Contrastive Models for Multi-Channel Video-Language Retrieval |  |  |  |
| SynthVSR: Scaling Up Visual Speech Recognition with Synthetic Supervision |  |  |  |
| METransformer: Radiology Report Generation by Transformer with Multiple Learnable Expert Tokens |  |  |  |
| Fusing Pre-trained Language Models with Multimodal Prompts through Reinforcement Learning |  |  |  |
| Abstract Visual Reasoning: An Algebraic Approach for Solving Raven's Progressive Matrices |  |  |  |
| Hierarchical Prompt Learning for Multi-Task Learning |  |  |  |
| Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image Person Retrieval |  |  |  |
| SViTT: Temporal Learning of Sparse Video-Text Transformers |  |  |  |
| How You Feelin'? Learning Emotions and Mental States in Movie Scenes |  |  |  |
| Logical Implications for Visual Question Answering Consistency |  |  |  |
| Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks |  |  |  |
| DeCo: Decomposition and Reconstruction for Compositional Temporal Grounding via Coarse-to-Fine Contrastive Ranking |  |  |  |
| iCLIP: Bridging Image Classification and Contrastive Language-Image Pre-Training for Visual Recognition |  |  |  |
| Semantic-Conditional Diffusion Networks for Image Captioning |  |  |  |
| CREPE: Can Vision-Language Foundation Models Reason Compositionally? |  |  |  |
| RMLVQA: A Margin Loss Approach for Visual Question Answering with Language Biases |  |  |  |
| Improving Vision-and-Language Navigation by Generating Future-View Image Semantics |  |  |  |
| Prefix Conditioning Unifies Language and Label Supervision |  |  |  |
| A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning |  |  |  |
| From Images to Textual Prompts: Zero-Shot Visual Question Answering with Frozen Large Language Models |  |  |  |
| Hierarchical Video-Moment Retrieval and Step-Captioning |  |  |  |
