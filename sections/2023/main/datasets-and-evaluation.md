# CVPR-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
    <tr>
        <td><strong>New collections</strong></td>
        <td>
            <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README.md">
                <img src="http://img.shields.io/badge/CVPR-2024-0073AE.svg" alt="Conference">
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/sections/2023/main/self-supervised-or-unsupervised-representation-learning.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README_2023.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/sections/2023/main/scene-analysis-and-understanding.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Datasets and Evaluation

![Section Papers](https://img.shields.io/badge/Section%20Papers-54-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-39-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-43-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-36-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Large-Scale Training Data Search for Object Re-Identification | [![GitHub](https://img.shields.io/github/stars/yorkeyao/SnP?style=flat)](https://github.com/yorkeyao/SnP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Large-Scale_Training_Data_Search_for_Object_Re-Identification_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16186-b31b1b.svg)](http://arxiv.org/abs/2303.16186) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OAZ0Pka2mKE) |
| Class Prototypes based Contrastive Learning for Classifying Multi-Label and Fine-grained Educational Videos | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Class_Prototypes_Based_Contrastive_Learning_for_Classifying_Multi-Label_and_Fine-Grained_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EhIeZtBB8bk) |
| V2X-Seq: A Large-Scale Sequential Dataset for Vehicle-Infrastructure Cooperative Perception and Forecasting | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://thudair.baai.ac.cn/index) <br /> [![GitHub](https://img.shields.io/github/stars/AIR-THU/DAIR-V2X-Seq?style=flat)](https://github.com/AIR-THU/DAIR-V2X-Seq) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_V2X-Seq_A_Large-Scale_Sequential_Dataset_for_Vehicle-Infrastructure_Cooperative_Perception_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.05938-b31b1b.svg)](http://arxiv.org/abs/2305.05938) | :heavy_minus_sign: |
| NewsNet: A Novel Dataset for Hierarchical Temporal Segmentation | [![GitHub](https://img.shields.io/github/stars/NewsNet-Benchmark/NewsNet?style=flat)](https://github.com/NewsNet-Benchmark/NewsNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_NewsNet_A_Novel_Dataset_for_Hierarchical_Temporal_Segmentation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dEGco30TBCk) |
| CLOTH4D: A Dataset for Clothed Human Reconstruction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg?style=flat)](https://github.com/AemikaChow/CLOTH4D) <br /> [![GitHub](https://img.shields.io/github/stars/NewsNet-Benchmark/NewsNet?style=flat)](https://github.com/AemikaChow/AiDLab-fAshIon-Data) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zou_CLOTH4D_A_Dataset_for_Clothed_Human_Reconstruction_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8Cc_kl55bFo) |
| Accelerating Dataset Distillation via Model Augmentation <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Accelerating_Dataset_Distillation_via_Model_Augmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.06152-b31b1b.svg)](http://arxiv.org/abs/2212.06152) | :heavy_minus_sign: |
| ImageNet-E: Benchmarking Neural Network Robustness via Attribute Editing | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg?style=flat)](https://github.com/alibaba/easyrobust/tree/main/benchmarks/imagenet-e) <br /> [![GitHub](https://img.shields.io/github/stars/alibaba/easyrobust?style=flat)](https://github.com/alibaba/easyrobust) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_ImageNet-E_Benchmarking_Neural_Network_Robustness_via_Attribute_Editing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17096-b31b1b.svg)](http://arxiv.org/abs/2303.17096) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=66Be-LDVHbc) |
| Visual Atoms: Pre-Training Vision Transformers with Sinusoidal Waves | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://masora1030.github.io/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves/) <br /> [![GitHub](https://img.shields.io/github/stars/masora1030/CVPR2023-FDSL-on-VisualAtom?style=flat)](https://github.com/masora1030/CVPR2023-FDSL-on-VisualAtom) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Takashima_Visual_Atoms_Pre-Training_Vision_Transformers_With_Sinusoidal_Waves_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.01112-b31b1b.svg)](http://arxiv.org/abs/2303.01112) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2reoDrFf0OA) |
| Infinite Photorealistic Worlds using Procedural Generation | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://infinigen.org/) <br /> [![GitHub](https://img.shields.io/github/stars/princeton-vl/infinigen?style=flat)](https://github.com/princeton-vl/infinigen) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Raistrick_Infinite_Photorealistic_Worlds_Using_Procedural_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.09310-b31b1b.svg)](http://arxiv.org/abs/2306.09310) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6tgspeI-GHY) |
| CelebV-Text: A Large-Scale Facial Text-Video Dataset | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://celebv-text.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/celebv-text/CelebV-Text?style=flat)](https://github.com/celebv-text/CelebV-Text) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_CelebV-Text_A_Large-Scale_Facial_Text-Video_Dataset_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14717-b31b1b.svg)](http://arxiv.org/abs/2303.14717) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0TS1hQwjNWw) |
| Spring: A High-Resolution High-Detail Dataset and Benchmark for Scene Flow, Optical Flow and Stereo | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://spring-benchmark.org/) <br /> [![GitHub](https://img.shields.io/github/stars/cv-stuttgart/sceneflow_from_blender?style=flat)](https://github.com/cv-stuttgart/sceneflow_from_blender) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Mehl_Spring_A_High-Resolution_High-Detail_Dataset_and_Benchmark_for_Scene_Flow_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.01943-b31b1b.svg)](http://arxiv.org/abs/2303.01943) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=omcntkTrFTg) |
| Connecting Vision and Language with Video Localized Narratives <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://google.github.io/video-localized-narratives/) <br /> [![GitHub](https://img.shields.io/github/stars/google/video-localized-narratives?style=flat)](https://github.com/google/video-localized-narratives) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Voigtlaender_Connecting_Vision_and_Language_With_Video_Localized_Narratives_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.11217-b31b1b.svg)](http://arxiv.org/abs/2302.11217) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=j1LUa-Cd4L8) |
| Towards Artistic Image Aesthetics Assessment: A Large-Scale Dataset and a New Method | [![GitHub](https://img.shields.io/github/stars/Dreemurr-T/BAID?style=flat)](https://github.com/Dreemurr-T/BAID) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_Towards_Artistic_Image_Aesthetics_Assessment_A_Large-Scale_Dataset_and_a_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15166-b31b1b.svg)](http://arxiv.org/abs/2303.15166) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=U1h4S7J2xnw) |
| MD-VQA: Multi-Dimensional Quality Assessment for UGC Live Videos | [![GitHub](https://img.shields.io/github/stars/zzc-1998/MD-VQA?style=flat)](https://github.com/zzc-1998/MD-VQA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_MD-VQA_Multi-Dimensional_Quality_Assessment_for_UGC_Live_Videos_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14933-b31b1b.svg)](http://arxiv.org/abs/2303.14933) | :heavy_minus_sign: |
| Toward RAW Object Detection: A New Benchmark and a New Model | [![Gitee Page](https://img.shields.io/badge/Gitee-Page-303643.svg)](https://gitee.com//mindspore/models/tree/master/research/cv/RAOD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Toward_RAW_Object_Detection_A_New_Benchmark_and_a_New_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dyudIByvYKc) |
| Objaverse: A Universe of Annotated 3D Objects | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://objaverse.allenai.org/) <br /> [![GitHub](https://img.shields.io/github/stars/allenai/objaverse-xl?style=flat)](https://github.com/allenai/objaverse-xl) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Deitke_Objaverse_A_Universe_of_Annotated_3D_Objects_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08051-b31b1b.svg)](http://arxiv.org/abs/2212.08051) | :heavy_minus_sign: |
| Habitat-Matterport 3D Semantics Dataset <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://aihabitat.org/datasets/hm3d-semantics/) <br /> [![GitHub](https://img.shields.io/github/stars/matterport/habitat-matterport-3dresearch?style=flat)](https://github.com/matterport/habitat-matterport-3dresearch) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yadav_Habitat-Matterport_3D_Semantics_Dataset_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.05633-b31b1b.svg)](http://arxiv.org/abs/2210.05633) | :heavy_minus_sign: |
| Similarity Metric Learning for RGB-Infrared Group Re-Identification | [![GitHub](https://img.shields.io/github/stars/WhollyOat/CM-Group?style=flat)](https://github.com/WhollyOat/CM-Group) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_Similarity_Metric_Learning_for_RGB-Infrared_Group_Re-Identification_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VUWEkJYvDO0) |
| MISC210K: A Large-Scale Dataset for Multi-Instance Semantic Correspondence | [![GitHub](https://img.shields.io/github/stars/YXSUNMADMAX/MISC210K?style=flat)](https://github.com/YXSUNMADMAX/MISC210K) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_MISC210K_A_Large-Scale_Dataset_for_Multi-Instance_Semantic_Correspondence_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| WeatherStream: Light Transport Automation of Single Image Deweathering | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://visual.ee.ucla.edu/wstream.htm/) <br /> [![GitHub](https://img.shields.io/github/stars/UCLA-VMG/WeatherStream?style=flat)](https://github.com/UCLA-VMG/WeatherStream) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_WeatherStream_Light_Transport_Automation_of_Single_Image_Deweathering_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=y2cQdAmegaY) |
| MobileBrick: Building LEGO for 3D Reconstruction on Mobile Devices | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://code.active.vision/MobileBrick/) <br /> [![GitHub](https://img.shields.io/github/stars/ActiveVisionLab/MobileBrick?style=flat)](https://github.com/ActiveVisionLab/MobileBrick) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_MobileBrick_Building_LEGO_for_3D_Reconstruction_on_Mobile_Devices_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.01932-b31b1b.svg)](http://arxiv.org/abs/2303.01932) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0NzsB3rdXlY) |
| GeoNet: Benchmarking Unsupervised Adaptation Across Geographies | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tarun005.github.io/GeoNet/) <br /> [![GitHub](https://img.shields.io/github/stars/ViLab-UCSD/GeoNet?style=flat)](https://github.com/ViLab-UCSD/GeoNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kalluri_GeoNet_Benchmarking_Unsupervised_Adaptation_Across_Geographies_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15443-b31b1b.svg)](http://arxiv.org/abs/2303.15443) | :heavy_minus_sign: |
| Logical Consistency and Greater Descriptive Power for Facial Hair Attribute Learning | [![GitHub](https://img.shields.io/github/stars/HaiyuWu/LogicalConsistency?style=flat)](https://github.com/HaiyuWu/LogicalConsistency) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Logical_Consistency_and_Greater_Descriptive_Power_for_Facial_Hair_Attribute_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.11102-b31b1b.svg)](http://arxiv.org/abs/2302.11102) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Pmpe3jJu8zQ) |
| PACO: Parts and Attributes of Common Objects <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/facebookresearch/paco?style=flat)](https://github.com/facebookresearch/paco) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ramanathan_PACO_Parts_and_Attributes_of_Common_Objects_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.01795-b31b1b.svg)](http://arxiv.org/abs/2301.01795) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=sIDX_2W9Wc8) |
| Understanding Deep Generative Models with Generalized Empirical Likelihoods <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/google-deepmind/understanding_deep_generative_models_with_generalized_empirical_likelihood?style=flat)](https://github.com/google-deepmind/understanding_deep_generative_models_with_generalized_empirical_likelihood) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ravuri_Understanding_Deep_Generative_Models_With_Generalized_Empirical_Likelihoods_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.09780-b31b1b.svg)](http://arxiv.org/abs/2306.09780) | :heavy_minus_sign: |
| BEDLAM: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://bedlam.is.tue.mpg.de/) <br /> [![GitHub](https://img.shields.io/github/stars/pixelite1201/BEDLAM?style=flat)](https://github.com/pixelite1201/BEDLAM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Black_BEDLAM_A_Synthetic_Dataset_of_Bodies_Exhibiting_Detailed_Lifelike_Animated_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.16940-b31b1b.svg)](http://arxiv.org/abs/2306.16940) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OBttHFwdtfI) |
| Unicode Analogies: An Anti-Objectivist Visual Reasoning Challenge | [![GitHub](https://img.shields.io/github/stars/SvenShade/UnicodeAnalogies?style=flat)](https://github.com/SvenShade/UnicodeAnalogies) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Spratley_Unicode_Analogies_An_Anti-Objectivist_Visual_Reasoning_Challenge_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| A New Comprehensive Benchmark for Semi-Supervised Video Anomaly Detection and Anticipation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://campusvad.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/zugexiaodui/campus_vad_code?style=flat)](https://github.com/zugexiaodui/campus_vad_code) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_A_New_Comprehensive_Benchmark_for_Semi-Supervised_Video_Anomaly_Detection_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.13611-b31b1b.svg)](http://arxiv.org/abs/2305.13611) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kdLapd4rBCc) |
| An In-Depth Exploration of Person Re-Identification and Gait Recognition in Cloth-Changing Conditions | [![GitHub](https://img.shields.io/github/stars/BNU-IVC/CCPG?style=flat)](https://github.com/BNU-IVC/CCPG) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_An_In-Depth_Exploration_of_Person_Re-Identification_and_Gait_Recognition_in_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Otani_Toward_Verifiable_and_Reproducible_Human_Evaluation_for_Text-to-Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01816-b31b1b.svg)](http://arxiv.org/abs/2304.01816) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=65QMukifH60) |
| BiasBed - Rigorous Texture Bias Evaluation | [![GitHub](https://img.shields.io/github/stars/D1noFuzi/BiasBed?style=flat)](https://github.com/D1noFuzi/BiasBed) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kalischek_BiasBed_-_Rigorous_Texture_Bias_Evaluation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13190-b31b1b.svg)](http://arxiv.org/abs/2211.13190) | :heavy_minus_sign: |
| A Large-Scale Homography Benchmark | [![GitHub](https://img.shields.io/github/stars/danini/homography-benchmark?style=flat)](https://github.com/danini/homography-benchmark) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Barath_A_Large-Scale_Homography_Benchmark_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.09997-b31b1b.svg)](http://arxiv.org/abs/2302.09997) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8RmTlekfKGY) |
| Exploring and Utilizing Pattern Imbalance | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Mei_Exploring_and_Utilizing_Pattern_Imbalance_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=w6AUguS5eQQ) |
| Full or Weak Annotations? An Adaptive Strategy for Budget-constrained Annotation Campaigns | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tejero_Full_or_Weak_Annotations_An_Adaptive_Strategy_for_Budget-Constrained_Annotation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11678-b31b1b.svg)](http://arxiv.org/abs/2303.11678) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PixUJ9Xl5_8) |
| ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of Real World Objects <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://eyecan-ai.github.io/rene/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Toschi_ReLight_My_NeRF_A_Dataset_for_Novel_View_Synthesis_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10448-b31b1b.svg)](http://arxiv.org/abs/2304.10448) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=23vZMbbZAHY) |
| Open-Vocabulary Attribute Detection | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ovad-benchmark.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/OVAD-Benchmark/ovad-benchmark-code?style=flat)](https://github.com/OVAD-Benchmark/ovad-benchmark-code) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Bravo_Open-Vocabulary_Attribute_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12914-b31b1b.svg)](http://arxiv.org/abs/2211.12914) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=cWOspjb9sPs) |
| Visual DNA: Representing and Comparing Images using Distributions of Neuron Activations | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://bramtoula.github.io/vdna/) <br /> [![GitHub](https://img.shields.io/github/stars/bramtoula/vdna?style=flat)](https://github.com/bramtoula/vdna) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ramtoula_Visual_DNA_Representing_and_Comparing_Images_Using_Distributions_of_Neuron_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10036-b31b1b.svg)](http://arxiv.org/abs/2304.10036) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dCbAiKrq1Jw) |
| Blind Image Quality Assessment via Vision-Language Correspondence: A Multitask Learning Perspective | [![GitHub](https://img.shields.io/github/stars/zwx8981/LIQE?style=flat)](https://github.com/zwx8981/LIQE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Blind_Image_Quality_Assessment_via_Vision-Language_Correspondence_A_Multitask_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14968-b31b1b.svg)](http://arxiv.org/abs/2303.14968) | :heavy_minus_sign: |
| An Image Quality Assessment Dataset for Portraits | [![GitHub](https://img.shields.io/github/stars/DXOMARK-Research/PIQ2023?style=flat)](https://github.com/DXOMARK-Research/PIQ2023) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05772-b31b1b.svg)](http://arxiv.org/abs/2304.05772) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=cvWjOWq5wnk) |
| Multi-Sensor Large-Scale Dataset for Multi-View 3D Reconstruction | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://skoltech3d.appliedai.tech/) <br /> [![GitHub](https://img.shields.io/github/stars/Skoltech-3D/sk3d_data?style=flat)](https://github.com/Skoltech-3D/sk3d_data) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Voynov_Multi-Sensor_Large-Scale_Dataset_for_Multi-View_3D_Reconstruction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2203.06111-b31b1b.svg)](http://arxiv.org/abs/2203.06111) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KPwghPyZWDE) |
| 3D-POP - An Automated Annotation Approach to Facilitate Markerless 2D-3D Tracking of Freely Moving Birds with Marker-based Motion Capture | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Naik_3D-POP_-_An_Automated_Annotation_Approach_to_Facilitate_Markerless_2D-3D_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13174-b31b1b.svg)](http://arxiv.org/abs/2303.13174) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=uGMsJ0qQZrA) |
| Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/aimagelab/pacscore?style=flat)](https://github.com/aimagelab/pacscore) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Sarto_Positive-Augmented_Contrastive_Learning_for_Image_and_Video_Captioning_Evaluation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12112-b31b1b.svg)](http://arxiv.org/abs/2303.12112) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=4PP9fCdBw88) |
| Visual Localization using Imperfect 3D Models from the Internet | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://v-pnk.github.io/cadloc/) <br /> [![GitHub](https://img.shields.io/github/stars/v-pnk/cadloc?style=flat)](https://github.com/v-pnk/cadloc) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Panek_Visual_Localization_Using_Imperfect_3D_Models_From_the_Internet_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05947-b31b1b.svg)](http://arxiv.org/abs/2304.05947) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=4Na026FMaP0) |
| <i>Fantastic Breaks</i>: A Dataset of Paired 3D Scans of Real-World Broken Objects and Their Complete Counterparts | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://terascale-all-sensing-research-studio.github.io/FantasticBreaks/) <br /> [![GitHub](https://img.shields.io/github/stars/Terascale-All-sensing-Research-Studio/FantasticBreaks?style=flat)](https://github.com/Terascale-All-sensing-Research-Studio/FantasticBreaks) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lamb_Fantastic_Breaks_A_Dataset_of_Paired_3D_Scans_of_Real-World_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14152-b31b1b.svg)](http://arxiv.org/abs/2303.14152) | :heavy_minus_sign: |
| StarCraftImage: A Dataset for Prototyping Spatial Reasoning Methods for Multi-Agent Environments | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://starcraftdata.davidinouye.com/) <br /> [![GitHub](https://img.shields.io/github/stars/inouye-lab/starcraftimage?style=flat)](https://github.com/inouye-lab/starcraftimage) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kulinski_StarCraftImage_A_Dataset_for_Prototyping_Spatial_Reasoning_Methods_for_Multi-Agent_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| MammalNet: A Large-Scale Video Benchmark for Mammal Recognition and Behavior Understanding | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mammal-net.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/Vision-CAIR/MammalNet?style=flat)](https://github.com/Vision-CAIR/MammalNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_MammalNet_A_Large-Scale_Video_Benchmark_for_Mammal_Recognition_and_Behavior_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.00576-b31b1b.svg)](http://arxiv.org/abs/2306.00576) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=sY71Oag2JMw) |
| A Large-Scale Robustness Analysis of Video Action Recognition Models | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Schiappa_A_Large-Scale_Robustness_Analysis_of_Video_Action_Recognition_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2207.01398-b31b1b.svg)](http://arxiv.org/abs/2207.01398) | :heavy_minus_sign: |
| Affection: Learning Affective Explanations for Real-World Visual Data | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://affective-explanations.org/) <br /> [![GitHub](https://img.shields.io/github/stars/affectivetools/eeai?style=flat)](https://github.com/affectivetools/eeai) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Achlioptas_Affection_Learning_Affective_Explanations_for_Real-World_Visual_Data_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.01946-b31b1b.svg)](http://arxiv.org/abs/2210.01946) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=b7dvTySVxXU) |
| ShapeTalk: A Language Dataset and Framework for 3D Shape Edits and Deformations | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://changeit3d.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/optas/changeit3d?style=flat)](https://github.com/optas/changeit3d) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Achlioptas_ShapeTalk_A_Language_Dataset_and_Framework_for_3D_Shape_Edits_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Deep Depth Estimation from Thermal Image | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sites.google.com/view/multi-spectral-stereo-dataset) <br /> [![GitHub](https://img.shields.io/github/stars/UkcheolShin/MS2-MultiSpectralStereoDataset?style=flat)](https://github.com/UkcheolShin/MS2-MultiSpectralStereoDataset) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Shin_Deep_Depth_Estimation_From_Thermal_Image_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| DF-Platter: Multi-Face Heterogeneous Deepfake Dataset | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://iab-rubric.org/df-platter-database) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Narayan_DF-Platter_Multi-Face_Heterogeneous_Deepfake_Dataset_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OeCc6mrP5uE) |
| A New Dataset based on Images Taken by Blind People for Testing the Robustness of Image Classification Models Trained for ImageNet Categories | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://vizwiz.org/tasks-and-datasets/image-classification/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Bafghi_A_New_Dataset_Based_on_Images_Taken_by_Blind_People_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bw7l09n7tZ8) |
| RealImpact: A Dataset of Impact Sound Fields for Real Objects <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://samuelpclarke.com/realimpact/) <br /> [![GitHub](https://img.shields.io/github/stars/samuel-clarke/RealImpact?style=flat)](https://github.com/samuel-clarke/RealImpact) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Clarke_RealImpact_A_Dataset_of_Impact_Sound_Fields_for_Real_Objects_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.09944-b31b1b.svg)](http://arxiv.org/abs/2306.09944) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OeZMeze-oIs) |
| NICO<sup>++</sup>: Towards Better Benchmarking for Domain Generalization | [![GitHub](https://img.shields.io/github/stars/xxgege/NICO-plus?style=flat)](https://github.com/xxgege/NICO-plus) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_NICO_Towards_Better_Benchmarking_for_Domain_Generalization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2204.08040-b31b1b.svg)](http://arxiv.org/abs/2204.08040) | :heavy_minus_sign: |
