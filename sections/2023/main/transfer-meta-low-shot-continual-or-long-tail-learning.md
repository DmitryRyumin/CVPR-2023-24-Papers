# CVPR-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
    <tr>
        <td><strong>New collections</strong></td>
        <td>
            <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README.md">
                <img src="http://img.shields.io/badge/CVPR-2024-0073AE.svg" alt="Conference">
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/sections/2023/main/humans-face-body-pose-gesture-movement.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/README_2023.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-24-Papers/blob/main/sections/2023/main/recognition-categorization-detection-retrieval.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Transfer, Meta, Low-Shot, Continual, or Long-Tail Learning

![Section Papers](https://img.shields.io/badge/Section%20Papers-153-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-113-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-118-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-109-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Dynamically Instance-Guided Adaptation: A Backward-Free Approach for Test-Time Domain Adaptive Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/Waybaba/DIGA?style=flat)](https://github.com/Waybaba/DIGA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Dynamically_Instance-Guided_Adaptation_A_Backward-Free_Approach_for_Test-Time_Domain_Adaptive_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Ikuwe0AS40A) |
| DETR With Additional Global Aggregation for Cross-Domain Weakly Supervised Object Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_DETR_With_Additional_Global_Aggregation_for_Cross-Domain_Weakly_Supervised_Object_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.07082-b31b1b.svg)](http://arxiv.org/abs/2304.07082) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=e8Aazn4A6aE) |
| Mind the Label Shift of Augmentation-Based Graph OOD Generalization | [![GitHub](https://img.shields.io/github/stars/samyu0304/lisa?style=flat)](https://github.com/samyu0304/lisa) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Mind_the_Label_Shift_of_Augmentation-Based_Graph_OOD_Generalization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14859-b31b1b.svg)](http://arxiv.org/abs/2303.14859) | :heavy_minus_sign: |
| Long-Tailed Visual Recognition via Self-Heterogeneous Integration With Knowledge Excavation | [![GitHub](https://img.shields.io/github/stars/jinyan-06/SHIKE?style=flat)](https://github.com/jinyan-06/SHIKE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Long-Tailed_Visual_Recognition_via_Self-Heterogeneous_Integration_With_Knowledge_Excavation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01279-b31b1b.svg)](http://arxiv.org/abs/2304.01279) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=InNbHCZ0pHU) |
| Understanding and Improving Visual Prompting: A Label-Mapping Perspective | [![GitHub](https://img.shields.io/github/stars/OPTML-Group/ILM-VP?style=flat)](https://github.com/OPTML-Group/ILM-VP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Understanding_and_Improving_Visual_Prompting_A_Label-Mapping_Perspective_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11635-b31b1b.svg)](http://arxiv.org/abs/2211.11635) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=x5qRQ4-my84) |
| A Whac-a-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others | [![GitHub](https://img.shields.io/github/stars/facebookresearch/Whac-A-Mole?style=flat)](https://github.com/facebookresearch/Whac-A-Mole) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_A_Whac-a-Mole_Dilemma_Shortcuts_Come_in_Multiples_Where_Mitigating_One_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04825-b31b1b.svg)](https://arxiv.org/abs/2212.04825) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=htYGHm53bJs) |
| Improved Distribution Matching for Dataset Condensation | [![GitHub](https://img.shields.io/github/stars/uitrbn/idm?style=flat)](https://github.com/uitrbn/idm) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Improved_Distribution_Matching_for_Dataset_Condensation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09742-b31b1b.svg)](https://arxiv.org/abs/2307.09742) | :heavy_minus_sign: |
| Divide and Adapt: Active Domain Adaptation via Customized Learning <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/Duojun-Huang/DiaNA-CVPR2023?style=flat)](https://github.com/Duojun-Huang/DiaNA-CVPR2023) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Divide_and_Adapt_Active_Domain_Adaptation_via_Customized_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11618-b31b1b.svg)](https://arxiv.org/abs/2307.11618) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TDPmStHFcvs) |
| Class Relationship Embedded Learning for Source-Free Unsupervised Domain Adaptation | [![GitHub](https://img.shields.io/github/stars/zhyx12/CRCo?style=flat)](https://github.com/zhyx12/CRCo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Class_Relationship_Embedded_Learning_for_Source-Free_Unsupervised_Domain_Adaptation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5jdbHfeUW5A) |
| Diversity-Aware Meta Visual Prompting | [![GitHub](https://img.shields.io/github/stars/shikiw/DAM-VP?style=flat)](https://github.com/shikiw/DAM-VP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Diversity-Aware_Meta_Visual_Prompting_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08138-b31b1b.svg)](http://arxiv.org/abs/2303.08138) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5f7AEAArizo) |
| Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection | [![GitHub](https://img.shields.io/github/stars/LuFan31/ET-OOD?style=flat)](https://github.com/LuFan31/ET-OOD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Uncertainty-Aware_Optimal_Transport_for_Semantically_Coherent_Out-of-Distribution_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10449-b31b1b.svg)](http://arxiv.org/abs/2303.10449) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9UldUmvwd5o) |
| Zero-Shot Object Counting | [![GitHub](https://img.shields.io/github/stars/cvlab-stonybrook/zero-shot-counting?style=flat)](https://github.com/cvlab-stonybrook/zero-shot-counting) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Zero-Shot_Object_Counting_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02001-b31b1b.svg)](http://arxiv.org/abs/2303.02001) | :heavy_minus_sign: |
| Learning With Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning | [![GitHub](https://img.shields.io/github/stars/zysong0113/SAVC?style=flat)](https://github.com/zysong0113/SAVC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Learning_With_Fantasy_Semantic-Aware_Virtual_Contrastive_Constraint_for_Few-Shot_Class-Incremental_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00426-b31b1b.svg)](http://arxiv.org/abs/2304.00426) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=s_Z608-foeo) |
| Distribution Shift Inversion for Out-of-Distribution Prediction | [![GitHub](https://img.shields.io/github/stars/yu-rp/Distribution-Shift-Iverson?style=flat)](https://github.com/yu-rp/Distribution-Shift-Iverson) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Distribution_Shift_Inversion_for_Out-of-Distribution_Prediction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.08328-b31b1b.svg)](https://arxiv.org/abs/2306.08328) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-HUC8e6OFMI) |
| Endpoints Weight Fusion for Class Incremental Semantic Segmentation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_Endpoints_Weight_Fusion_for_Class_Incremental_Semantic_Segmentation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=k-FKX67kOPk) |
| Promoting Semantic Connectivity: Dual Nearest Neighbors Contrastive Learning for Unsupervised Domain Generalization| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Promoting_Semantic_Connectivity_Dual_Nearest_Neighbors_Contrastive_Learning_for_Unsupervised_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Class-Conditional Sharpness-Aware Minimization for Deep Long-Tailed Recognition | [![GitHub](https://img.shields.io/github/stars/zzpustc/CC-SAM?style=flat)](https://github.com/zzpustc/CC-SAM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Class-Conditional_Sharpness-Aware_Minimization_for_Deep_Long-Tailed_Recognition_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Q5KJa1MzpTs) |
| Meta-Causal Learning for Single Domain Generalization | [![GitHub](https://img.shields.io/github/stars/junkunyuan/Awesome-Domain-Generalization?style=flat)](https://github.com/junkunyuan/Awesome-Domain-Generalization) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Meta-Causal_Learning_for_Single_Domain_Generalization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03709-b31b1b.svg)](http://arxiv.org/abs/2304.03709) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=QKlRPUFypss) |
| VoP: Text-Video Co-Operative Prompt Tuning for Cross-Modal Retrieval | [![GitHub](https://img.shields.io/github/stars/bighuang624/VoP?style=flat)](https://github.com/bighuang624/VoP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_VoP_Text-Video_Co-Operative_Prompt_Tuning_for_Cross-Modal_Retrieval_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12764-b31b1b.svg)](http://arxiv.org/abs/2211.12764) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ymdkiSSuOmI) |
| Learning Imbalanced Data With Vision Transformers | [![GitHub](https://img.shields.io/github/stars/XuZhengzhuo/LiVT?style=flat)](https://github.com/XuZhengzhuo/LiVT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Learning_Imbalanced_Data_With_Vision_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.02015-b31b1b.svg)](http://arxiv.org/abs/2212.02015) | :heavy_minus_sign: |
| Sharpness-Aware Gradient Matching for Domain Generalization | [![GitHub](https://img.shields.io/github/stars/Wang-pengfei/SAGM?style=flat)](https://github.com/Wang-pengfei/SAGM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Sharpness-Aware_Gradient_Matching_for_Domain_Generalization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10353-b31b1b.svg)](http://arxiv.org/abs/2303.10353) | :heavy_minus_sign: |
| Geometry and Uncertainty-Aware 3D Point Cloud Class-Incremental Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/leolyj/3DPC-CISS?style=flat)](https://github.com/leolyj/3DPC-CISS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Geometry_and_Uncertainty-Aware_3D_Point_Cloud_Class-Incremental_Semantic_Segmentation_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification & Segmentation | [![GitHub](https://img.shields.io/github/stars/dahyun-kang/cst?style=flat)](https://github.com/dahyun-kang/cst) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Distilling_Self-Supervised_Vision_Transformers_for_Weakly-Supervised_Few-Shot_Classification__Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.03407-b31b1b.svg)](https://arxiv.org/abs/2307.03407) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-_8DzMiwemc) |
| Regularizing Second-Order Influences for Continual Learning | [![GitHub](https://img.shields.io/github/stars/feifeiobama/InfluenceCL?style=flat)](https://github.com/feifeiobama/InfluenceCL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Regularizing_Second-Order_Influences_for_Continual_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10177-b31b1b.svg)](http://arxiv.org/abs/2304.10177) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6vbPquXtmrE) |
| I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/ferjad/I2DFormer?style=flat)](https://github.com/ferjad/I2DFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Naeem_I2MVFormer_Large_Language_Model_Generated_Multi-View_Document_Supervision_for_Zero-Shot_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.02291-b31b1b.svg)](http://arxiv.org/abs/2212.02291) | :heavy_minus_sign: |
| FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding | [![GitHub](https://img.shields.io/github/stars/uark-cviu/FREDOM?style=flat)](https://github.com/uark-cviu/FREDOM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Truong_FREDOM_Fairness_Domain_Adaptation_Approach_to_Semantic_Scene_Understanding_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02135-b31b1b.svg)](http://arxiv.org/abs/2304.02135) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Feo4UMd1eac) |
| Dense Network Expansion for Class Incremental Learning | [![GitHub](https://img.shields.io/github/stars/BinahHu/DNE?style=flat)](https://github.com/BinahHu/DNE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Dense_Network_Expansion_for_Class_Incremental_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12696-b31b1b.svg)](http://arxiv.org/abs/2303.12696) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7BtHR7vEBWU) |
| Batch Model Consolidation: A Multi-Task Model Consolidation Framework | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://iordanis.me/stream_benchmark/) <br /> [![GitHub](https://img.shields.io/github/stars/fostiropoulos/stream_benchmark?style=flat)](https://github.com/fostiropoulos/stream_benchmark) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Fostiropoulos_Batch_Model_Consolidation_A_Multi-Task_Model_Consolidation_Framework_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.16484-b31b1b.svg)](https://arxiv.org/abs/2305.16484) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=v5uw1XpCum4) |
| DiGeo: Discriminative Geometry-Aware Learning for Generalized Few-Shot Object Detection | [![GitHub](https://img.shields.io/github/stars/Phoenix-V/DiGeo?style=flat)](https://github.com/Phoenix-V/DiGeo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_DiGeo_Discriminative_Geometry-Aware_Learning_for_Generalized_Few-Shot_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09674-b31b1b.svg)](http://arxiv.org/abs/2303.09674) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pcZYjsL52So) |
| ALOFT: A Lightweight MLP-Like Architecture With Dynamic Low-Frequency Transform for Domain Generalization | [![GitHub](https://img.shields.io/github/stars/lingeringlight/ALOFT?style=flat)](https://github.com/lingeringlight/ALOFT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_ALOFT_A_Lightweight_MLP-Like_Architecture_With_Dynamic_Low-Frequency_Transform_for_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11674-b31b1b.svg)](http://arxiv.org/abs/2303.11674) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LwWsrO1THio) |
| ZegCLIP: Towards Adapting CLIP for Zero-Shot Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/ZiqinZhou66/ZegCLIP?style=flat)](https://github.com/ZiqinZhou66/ZegCLIP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_ZegCLIP_Towards_Adapting_CLIP_for_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03588-b31b1b.svg)](http://arxiv.org/abs/2212.03588) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8xeJsX2_h2w) |
| DiGA: <i>Di</i>stil to <i>G</i>eneralize and then <i>A</i>dapt for Domain Adaptive Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/fy-vision/DiGA?style=flat)](https://github.com/fy-vision/DiGA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_DiGA_Distil_To_Generalize_and_Then_Adapt_for_Domain_Adaptive_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02222-b31b1b.svg)](http://arxiv.org/abs/2304.02222) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=AJKyXExH1_g) |
| Adjustment and Alignment for Unbiased Open Set Domain Adaptation | [![GitHub](https://img.shields.io/github/stars/CityU-AIM-Group/Anna?style=flat)](https://github.com/CityU-AIM-Group/Anna) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Adjustment_and_Alignment_for_Unbiased_Open_Set_Domain_Adaptation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hFZn16ntyXw) |
| Adapting Shortcut With Normalizing Flow: An Efficient Tuning Framework for Visual Recognition | [![GitHub](https://img.shields.io/github/stars/wang-yaoming/snf?style=flat)](https://github.com/wang-yaoming/snf) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Adapting_Shortcut_With_Normalizing_Flow_An_Efficient_Tuning_Framework_for_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| CODA-Prompt: COntinual Decomposed Attention-Based Prompting for Rehearsal-Free Continual Learning | [![GitHub](https://img.shields.io/github/stars/GT-RIPL/CODA-Prompt?style=flat)](https://github.com/GT-RIPL/CODA-Prompt) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Smith_CODA-Prompt_COntinual_Decomposed_Attention-Based_Prompting_for_Rehearsal-Free_Continual_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13218-b31b1b.svg)](https://arxiv.org/abs/2211.13218) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WNekhd7ddBo) |
| ConStruct-VL: Data-Free Continual Structured VL Concepts Learning | [![GitHub](https://img.shields.io/github/stars/jamessealesmith/ConStruct-VL?style=flat)](https://github.com/jamessealesmith/ConStruct-VL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Smith_ConStruct-VL_Data-Free_Continual_Structured_VL_Concepts_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.09790-b31b1b.svg)](https://arxiv.org/abs/2211.09790) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kBMOEvfCO74) |
| Generalizing Dataset Distillation via Deep Generative Prior | [![GitHub](https://img.shields.io/github/stars/GeorgeCazenavette/glad?style=flat)](https://github.com/GeorgeCazenavette/glad) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cazenavette_Generalizing_Dataset_Distillation_via_Deep_Generative_Prior_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.01649-b31b1b.svg)](http://arxiv.org/abs/2305.01649) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Zpq4IarZBWk) |
| Few-Shot Learning With Visual Distribution Calibration and Cross-Modal Distribution Alignment | [![GitHub](https://img.shields.io/github/stars/bhrqw/SADA?style=flat)](https://github.com/bhrqw/SADA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Few-Shot_Learning_With_Visual_Distribution_Calibration_and_Cross-Modal_Distribution_Alignment_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.11439-b31b1b.svg)](https://arxiv.org/abs/2305.11439) | :heavy_minus_sign: |
| Multi-Centroid Task Descriptor for Dynamic Class Incremental Inference | [![GitHub](https://img.shields.io/github/stars/CSIncWW/MultiCentroidGate?style=flat)](https://github.com/CSIncWW/MultiCentroidGate) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_Multi-Centroid_Task_Descriptor_for_Dynamic_Class_Incremental_Inference_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| DAA: A Delta Age AdaIN Operation for Age Estimation via Binary Code Transformer | [![GitHub](https://img.shields.io/github/stars/redcping/Delta_Age_AdaIN?style=flat)](https://github.com/redcping/Delta_Age_AdaIN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_DAA_A_Delta_Age_AdaIN_Operation_for_Age_Estimation_via_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07929-b31b1b.svg)](http://arxiv.org/abs/2303.07929) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mSXWAd3akn4) |
| Bilateral Memory Consolidation for Continual Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Nie_Bilateral_Memory_Consolidation_for_Continual_Learning_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Texts as Images in Prompt Tuning for Multi-Label Image Recognition | [![GitHub](https://img.shields.io/github/stars/guozix/TaI-DPT?style=flat)](https://github.com/guozix/TaI-DPT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Texts_as_Images_in_Prompt_Tuning_for_Multi-Label_Image_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12739-b31b1b.svg)](http://arxiv.org/abs/2211.12739) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=UgV2aFMel7o) |
| Learning Transformations To Reduce the Geometric Shift in Object Detection| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Vidit_Learning_Transformations_To_Reduce_the_Geometric_Shift_in_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.05496-b31b1b.svg)](http://arxiv.org/abs/2301.05496) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=B7L8YVvNPnw) |
| CLIP the Gap: A Single Domain Generalization Approach for Object Detection | [![GitHub](https://img.shields.io/github/stars/vidit09/domaingen?style=flat)](https://github.com/vidit09/domaingen) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Vidit_CLIP_the_Gap_A_Single_Domain_Generalization_Approach_for_Object_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.05499-b31b1b.svg)](http://arxiv.org/abs/2301.05499) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WDUu3gar4hY) |
| Transfer Knowledge From Head to Tail: Uncertainty Calibration Under Long-Tailed Distribution | [![GitHub](https://img.shields.io/github/stars/JiahaoChen1/Calibration?style=flat)](https://github.com/JiahaoChen1/Calibration) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Transfer_Knowledge_From_Head_to_Tail_Uncertainty_Calibration_Under_Long-Tailed_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06537-b31b1b.svg)](http://arxiv.org/abs/2304.06537) | :heavy_minus_sign: |
| Bi-Directional Distribution Alignment for Transductive Zero-Shot Learning | [![GitHub](https://img.shields.io/github/stars/Zhicaiwww/Bi-VAEGAN?style=flat)](https://github.com/Zhicaiwww/Bi-VAEGAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Bi-Directional_Distribution_Alignment_for_Transductive_Zero-Shot_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08698-b31b1b.svg)](http://arxiv.org/abs/2303.08698) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xr8FzK9civ8) |
| DARE-GRAM: Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices | [![GitHub](https://img.shields.io/github/stars/ismailnejjar/DARE-GRAM?style=flat)](https://github.com/ismailnejjar/DARE-GRAM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Nejjar_DARE-GRAM_Unsupervised_Domain_Adaptation_Regression_by_Aligning_Inverse_Gram_Matrices_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13325-b31b1b.svg)](http://arxiv.org/abs/2303.13325) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=iwTe35fEQ3c) |
| LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models | [![GitHub](https://img.shields.io/github/stars/1adrianb/lasp?style=flat)](https://github.com/1adrianb/lasp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Bulat_LASP_Text-to-Text_Optimization_for_Language-Aware_Soft_Prompting_of_Vision__CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.01115-b31b1b.svg)](http://arxiv.org/abs/2210.01115) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=h0RS-UbCjGg) |
| Open-Set Likelihood Maximization for Few-Shot Learning | [![GitHub](https://img.shields.io/github/stars/ebennequin/few-shot-open-set?style=flat)](https://github.com/ebennequin/few-shot-open-set) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Boudiaf_Open-Set_Likelihood_Maximization_for_Few-Shot_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.08390-b31b1b.svg)](http://arxiv.org/abs/2301.08390) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EbgRdKDBvKQ) |
| WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation | [![GitHub](https://img.shields.io/github/stars/caoyunkang/WinClip?style=flat)](https://github.com/caoyunkang/WinClip) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jeong_WinCLIP_Zero-Few-Shot_Anomaly_Classification_and_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14814-b31b1b.svg)](https://arxiv.org/abs/2303.14814) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=z2v3FvhLELY) |
| Federated Domain Generalization With Generalization Adjustment | [![GitHub](https://img.shields.io/github/stars/MediaBrain-SJTU/FedDG-GA?style=flat)](https://github.com/MediaBrain-SJTU/FedDG-GA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Federated_Domain_Generalization_With_Generalization_Adjustment_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EESJrJGCSR4) |
| ProtoCon: Pseudo-Label Refinement via Online Clustering and Prototypical Consistency for Efficient Semi-Supervised Learning <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Nassar_ProtoCon_Pseudo-Label_Refinement_via_Online_Clustering_and_Prototypical_Consistency_for_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13556-b31b1b.svg)](http://arxiv.org/abs/2303.13556) | :heavy_minus_sign: |
| DA-DETR: Domain Adaptive Detection Transformer With Information Fusion | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_DA-DETR_Domain_Adaptive_Detection_Transformer_With_Information_Fusion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2103.17084-b31b1b.svg)](https://arxiv.org/abs/2103.17084) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=48WIZVhnJHU) |
| Harmonious Teacher for Cross-Domain Object Detection | [![GitHub](https://img.shields.io/github/stars/kinredon/Harmonious-Teacher?style=flat)](https://github.com/kinredon/Harmonious-Teacher) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Deng_Harmonious_Teacher_for_Cross-Domain_Object_Detection_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=puUr6R0id-w) |
| AutoLabel: CLIP-Based Framework for Open-Set Video Domain Adaptation | [![GitHub](https://img.shields.io/github/stars/gzaraunitn/autolabel?style=flat)](https://github.com/gzaraunitn/autolabel) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zara_AutoLabel_CLIP-Based_Framework_for_Open-Set_Video_Domain_Adaptation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01110-b31b1b.svg)](http://arxiv.org/abs/2304.01110) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WOXwZd1InAg) |
| Task Difficulty Aware Parameter Allocation & Regularization for Lifelong Learning| [![GitHub](https://img.shields.io/github/stars/WenjinW/PAR?style=flat)](https://github.com/WenjinW/PAR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Task_Difficulty_Aware_Parameter_Allocation__Regularization_for_Lifelong_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05288-b31b1b.svg)](http://arxiv.org/abs/2304.05288) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=R0jA9rHxIWI) |
| Revisiting Prototypical Network for Cross Domain Few-Shot Learning | [![GitHub](https://img.shields.io/github/stars/NWPUZhoufei/LDP-Net?style=flat)](https://github.com/NWPUZhoufei/LDP-Net) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Revisiting_Prototypical_Network_for_Cross_Domain_Few-Shot_Learning_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Federated Incremental Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/JiahuaDong/FISS?style=flat)](https://github.com/JiahuaDong/FISS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_Federated_Incremental_Semantic_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04620-b31b1b.svg)](http://arxiv.org/abs/2304.04620) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=uWIDgPfFMSA) |
| Semantic Prompt for Few-Shot Image Recognition | [![GitHub](https://img.shields.io/github/stars/WentaoChen0813/SemanticPrompt?style=flat)](https://github.com/WentaoChen0813/SemanticPrompt) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Semantic_Prompt_for_Few-Shot_Image_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14123-b31b1b.svg)](http://arxiv.org/abs/2303.14123) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7uem_CHedjM) |
| Rethinking Gradient Projection Continual Learning: Stability / Plasticity Feature Space Decoupling | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Rethinking_Gradient_Projection_Continual_Learning_Stability__Plasticity_Feature_Space_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| No One Left Behind: Improving the Worst Categories in Long-Tailed Learning| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Du_No_One_Left_Behind_Improving_the_Worst_Categories_in_Long-Tailed_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03630-b31b1b.svg)](http://arxiv.org/abs/2303.03630) | :heavy_minus_sign: |
| Meta Omnium: A Benchmark for General-Purpose Learning-To-Learn | [![GitHub](https://img.shields.io/github/stars/edi-meta-learning/meta-omnium?style=flat)](https://github.com/edi-meta-learning/meta-omnium) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Bohdal_Meta_Omnium_A_Benchmark_for_General-Purpose_Learning-To-Learn_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.07625-b31b1b.svg)](http://arxiv.org/abs/2305.07625) | :heavy_minus_sign: |
| Transductive Few-Shot Learning With Prototype-Based Label Propagation by Iterative Graph Refinement | [![GitHub](https://img.shields.io/github/stars/allenhaozhu/protoLP?style=flat)](https://github.com/allenhaozhu/protoLP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Transductive_Few-Shot_Learning_With_Prototype-Based_Label_Propagation_by_Iterative_Graph_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.11598-b31b1b.svg)](http://arxiv.org/abs/2304.11598) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=sfQQy0eU3RA) |
| COT: Unsupervised Domain Adaptation With Clustering and Optimal Transport | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_COT_Unsupervised_Domain_Adaptation_With_Clustering_and_Optimal_Transport_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Semi-Supervised Domain Adaptation With Source Label Adaptation | [![GitHub](https://img.shields.io/github/stars/chu0802/SLA?style=flat)](https://github.com/chu0802/SLA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Semi-Supervised_Domain_Adaptation_With_Source_Label_Adaptation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.02335-b31b1b.svg)](http://arxiv.org/abs/2302.02335) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=oVOQI9c1hYE) |
| MetaMix: Towards Corruption-Robust Continual Learning With Temporally Self-Adaptive Data Transformation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MetaMix_Towards_Corruption-Robust_Continual_Learning_With_Temporally_Self-Adaptive_Data_Transformation_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Visual-Language Prompt Tuning With Knowledge-Guided Context Optimization | [![GitHub](https://img.shields.io/github/stars/htyao89/KgCoOp?style=flat)](https://github.com/htyao89/KgCoOp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Visual-Language_Prompt_Tuning_With_Knowledge-Guided_Context_Optimization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13283-b31b1b.svg)](http://arxiv.org/abs/2303.13283) | :heavy_minus_sign: |
| Modeling Inter-Class and Intra-Class Constraints in Novel Class Discovery | [![GitHub](https://img.shields.io/github/stars/FanZhichen/NCD-IIC?style=flat)](https://github.com/FanZhichen/NCD-IIC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Modeling_Inter-Class_and_Intra-Class_Constraints_in_Novel_Class_Discovery_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.03591-b31b1b.svg)](http://arxiv.org/abs/2210.03591) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kpSOVAKwYaQ) |
| Real-Time Evaluation in Online Continual Learning: A New Hope <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/Yasir-Ghunaim/RealtimeOCL?style=flat)](https://github.com/Yasir-Ghunaim/RealtimeOCL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ghunaim_Real-Time_Evaluation_in_Online_Continual_Learning_A_New_Hope_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.01047-b31b1b.svg)](http://arxiv.org/abs/2302.01047) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vkIIIHK_F0E) |
| Partial Network Cloning | [![GitHub](https://img.shields.io/github/stars/JngwenYe/PNCloning?style=flat)](https://github.com/JngwenYe/PNCloning) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_Partial_Network_Cloning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10597-b31b1b.svg)](http://arxiv.org/abs/2303.10597) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JV8Ozatq56s) |
| Rebalancing Batch Normalization for Exemplar-Based Class-Incremental Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cha_Rebalancing_Batch_Normalization_for_Exemplar-Based_Class-Incremental_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2201.12559-b31b1b.svg)](http://arxiv.org/abs/2201.12559) | :heavy_minus_sign: |
| EcoTTA: Memory-Efficient Continual Test-Time Adaptation via Self-Distilled Regularization | [![GitHub](https://img.shields.io/github/stars/Lily-Le/EcoTTA?style=flat)](https://github.com/Lily-Le/EcoTTA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_EcoTTA_Memory-Efficient_Continual_Test-Time_Adaptation_via_Self-Distilled_Regularization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.01904-b31b1b.svg)](http://arxiv.org/abs/2303.01904) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lYdTfZmbPc4) |
| Feature Alignment and Uniformity for Test Time Adaptation | [![GitHub](https://img.shields.io/github/stars/SakurajimaMaiii/TSD?style=flat)](https://github.com/SakurajimaMaiii/TSD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Feature_Alignment_and_Uniformity_for_Test_Time_Adaptation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10902-b31b1b.svg)](http://arxiv.org/abs/2303.10902) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jdTTPwNhtT4) |
| Bootstrap Your Own Prior: Towards Distribution-Agnostic Novel Class Discovery | [![GitHub](https://img.shields.io/github/stars/muliyangm/BYOP?style=flat)](https://github.com/muliyangm/BYOP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Bootstrap_Your_Own_Prior_Towards_Distribution-Agnostic_Novel_Class_Discovery_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=oDhQIHHr9w8) |
| Towards Realistic Long-Tailed Semi-Supervised Learning: Consistency Is All You Need | [![GitHub](https://img.shields.io/github/stars/Gank0078/ACR?style=flat)](https://github.com/Gank0078/ACR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Towards_Realistic_Long-Tailed_Semi-Supervised_Learning_Consistency_Is_All_You_Need_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tYQb0fsI8bE) |
| Balanced Product of Calibrated Experts for Long-Tailed Recognition | [![GitHub](https://img.shields.io/github/stars/emasa/BalPoE-CalibratedLT?style=flat)](https://github.com/emasa/BalPoE-CalibratedLT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Aimar_Balanced_Product_of_Calibrated_Experts_for_Long-Tailed_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.05260-b31b1b.svg)](http://arxiv.org/abs/2206.05260) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=H664_EQq2cs) |
| Unsupervised Continual Semantic Adaptation Through Neural Rendering | [![GitHub](https://img.shields.io/github/stars/ethz-asl/ucsa_neural_rendering?style=flat)](https://github.com/ethz-asl/ucsa_neural_rendering) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Unsupervised_Continual_Semantic_Adaptation_Through_Neural_Rendering_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13969-b31b1b.svg)](http://arxiv.org/abs/2211.13969) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=XfNLsl8ATNY) |
| Computationally Budgeted Continual Learning: What Does Matter? | [![GitHub](https://img.shields.io/github/stars/drimpossible/BudgetCL?style=flat)](https://github.com/drimpossible/BudgetCL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Prabhu_Computationally_Budgeted_Continual_Learning_What_Does_Matter_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11165-b31b1b.svg)](http://arxiv.org/abs/2303.11165) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3nmRtFkF5fw) |
| Ground-Truth Free Meta-Learning for Deep Compressive Sampling | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Ground-Truth_Free_Meta-Learning_for_Deep_Compressive_Sampling_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LXE2eLzglUM) |
| Multi-Level Logit Distillation | [![GitHub](https://img.shields.io/github/stars/Jin-Ying/Multi-Level-Logit-Distillation?style=flat)](https://github.com/Jin-Ying/Multi-Level-Logit-Distillation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Multi-Level_Logit_Distillation_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| StyleAdv: Meta Style Adversarial Training for Cross-Domain Few-Shot Learning | [![GitHub](https://img.shields.io/github/stars/lovelyqian/StyleAdv-CDFSL?style=flat)](https://github.com/lovelyqian/StyleAdv-CDFSL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_StyleAdv_Meta_Style_Adversarial_Training_for_Cross-Domain_Few-Shot_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.09309-b31b1b.svg)](http://arxiv.org/abs/2302.09309) |  [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=YB-S2YF22mc) |
| MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation | [![GitHub](https://img.shields.io/github/stars/lhoyer/MIC?style=flat)](https://github.com/lhoyer/MIC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hoyer_MIC_Masked_Image_Consistency_for_Context-Enhanced_Domain_Adaptation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.01322-b31b1b.svg)](http://arxiv.org/abs/2212.01322) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=05s1zablJaY) |
| On the Stability-Plasticity Dilemma of Class-Incremental Learning| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_On_the_Stability-Plasticity_Dilemma_of_Class-Incremental_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01663-b31b1b.svg)](http://arxiv.org/abs/2304.01663) | :heavy_minus_sign: |
| TeSLA: Test-Time Self-Learning With Automatic Adversarial Augmentation | [![GitHub](https://img.shields.io/github/stars/devavratTomar/TeSLA?style=flat)](https://github.com/devavratTomar/TeSLA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tomar_TeSLA_Test-Time_Self-Learning_With_Automatic_Adversarial_Augmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09870-b31b1b.svg)](http://arxiv.org/abs/2303.09870) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_lLZm2v05dY) |
| MHPL: Minimum Happy Points Learning for Active Source Free Domain Adaptation| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MHPL_Minimum_Happy_Points_Learning_for_Active_Source_Free_Domain_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2205.10711-b31b1b.svg)](https://arxiv.org/abs/2205.10711) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=foefkl9tRJ4) |
| CIGAR: Cross-Modality Graph Reasoning for Domain Adaptive Object Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_CIGAR_Cross-Modality_Graph_Reasoning_for_Domain_Adaptive_Object_Detection_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=fgf8G_FwiQM) |
| Adaptive Plasticity Improvement for Continual Learning | [![GitHub](https://img.shields.io/github/stars/liangyanshuo/Adaptive-Plasticity-Improvement-for-Continual-Learning?style=flat)](https://github.com/liangyanshuo/Adaptive-Plasticity-Improvement-for-Continual-Learning) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_Adaptive_Plasticity_Improvement_for_Continual_Learning_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Achieving a Better Stability-Plasticity Trade-Off via Auxiliary Networks in Continual Learning | [![GitHub](https://img.shields.io/github/stars/kim-sanghwan/ANCL?style=flat)](https://github.com/kim-sanghwan/ANCL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Achieving_a_Better_Stability-Plasticity_Trade-Off_via_Auxiliary_Networks_in_Continual_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09483-b31b1b.svg)](http://arxiv.org/abs/2303.09483) | :heavy_minus_sign: |
| Few-Shot Geometry-Aware Keypoint Localization | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://xingzhehe.github.io/FewShot3DKP/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/He_Few-Shot_Geometry-Aware_Keypoint_Localization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17216-b31b1b.svg)](http://arxiv.org/abs/2303.17216) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=SurF_6SYcx4) |
| Spatio-Temporal Pixel-Level Contrastive Learning-Based Source-Free Domain Adaptation for Video Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/shaoyuanlo/STPL?style=flat)](https://github.com/shaoyuanlo/STPL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lo_Spatio-Temporal_Pixel-Level_Contrastive_Learning-Based_Source-Free_Domain_Adaptation_for_Video_Semantic_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14361-b31b1b.svg)](http://arxiv.org/abs/2303.14361) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=t_UP8CkKeZo) |
| Both Style and Distortion Matter: Dual-Path Unsupervised Domain Adaptation for Panoramic Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/zhengxuJosh/DPPASS?style=flat)](https://github.com/zhengxuJosh/DPPASS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Both_Style_and_Distortion_Matter_Dual-Path_Unsupervised_Domain_Adaptation_for_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14360-b31b1b.svg)](http://arxiv.org/abs/2303.14360) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=38-N7-DZqng) |
| Bi-Level Meta-Learning for Few-Shot Domain Generalization |  :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Bi-Level_Meta-Learning_for_Few-Shot_Domain_Generalization_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=MWEo0KMVVGE) |
| Few-Shot Referring Relationships in Videos| [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://vl2g.github.io/projects/refRelations/) <br /> [![GitHub](https://img.shields.io/github/stars/vl2g/RefRelations?style=flat)](https://github.com/vl2g/RefRelations) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kumar_Few-Shot_Referring_Relationships_in_Videos_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Exploring Data Geometry for Continual Learning| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Exploring_Data_Geometry_for_Continual_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03931-b31b1b.svg)](http://arxiv.org/abs/2304.03931) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hLTuPD2U09E) |
| Masked Images Are Counterfactual Samples for Robust Fine-Tuning | [![GitHub](https://img.shields.io/github/stars/Coxy7/robust-finetuning?style=flat)](https://github.com/Coxy7/robust-finetuning) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_Masked_Images_Are_Counterfactual_Samples_for_Robust_Fine-Tuning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03052-b31b1b.svg)](http://arxiv.org/abs/2303.03052) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KG8Xlux4_Wo) |
| DKT: Diverse Knowledge Transfer Transformer for Class Incremental Learning | [![GitHub](https://img.shields.io/github/stars/MIV-XJTU/DKT?style=flat)](https://github.com/MIV-XJTU/DKT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_DKT_Diverse_Knowledge_Transfer_Transformer_for_Class_Incremental_Learning_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mqa2aY2WGFU) |
| CoMFormer: Continual Learning in Semantic and Panoptic Segmentation | [![GitHub](https://img.shields.io/github/stars/fcdl94/CoMFormer?style=flat)](https://github.com/fcdl94/CoMFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cermelli_CoMFormer_Continual_Learning_in_Semantic_and_Panoptic_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13999-b31b1b.svg)](http://arxiv.org/abs/2211.13999) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=97qmCgzAFGQ) |
| Global and Local Mixture Consistency Cumulative Learning for Long-Tailed Visual Recognitions | [![GitHub](https://img.shields.io/github/stars/ynu-yangpeng/GLMC?style=flat)](https://github.com/ynu-yangpeng/GLMC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Global_and_Local_Mixture_Consistency_Cumulative_Learning_for_Long-Tailed_Visual_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.08661-b31b1b.svg)](http://arxiv.org/abs/2305.08661) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mTeWItl4k9k) |
| Class Attention Transfer Based Knowledge Distillation | [![GitHub](https://img.shields.io/github/stars/GzyAftermath/CAT-KD?style=flat)](https://github.com/GzyAftermath/CAT-KD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Class_Attention_Transfer_Based_Knowledge_Distillation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.12777-b31b1b.svg)](http://arxiv.org/abs/2304.12777) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=HWoqOglluCI) |
| Hard Sample Matters a Lot in Zero-Shot Quantization | [![GitHub](https://img.shields.io/github/stars/lihuantong/HAST?style=flat)](https://github.com/lihuantong/HAST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Hard_Sample_Matters_a_Lot_in_Zero-Shot_Quantization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13826-b31b1b.svg)](http://arxiv.org/abs/2303.13826) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rEKGhS5JYgE) |
| Back to the Source: Diffusion-Driven Adaptation To Test-Time Corruption | [![GitHub](https://img.shields.io/github/stars/shiyegao/DDA?style=flat)](https://github.com/shiyegao/DDA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Back_to_the_Source_Diffusion-Driven_Adaptation_To_Test-Time_Corruption_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2207.03442-b31b1b.svg)](https://arxiv.org/abs/2207.03442) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=YHRJVGal5FM) |
| SuperDisco: Super-Class Discovery Improves Visual Recognition for the Long-Tail | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Du_SuperDisco_Super-Class_Discovery_Improves_Visual_Recognition_for_the_Long-Tail_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00101-b31b1b.svg)](http://arxiv.org/abs/2304.00101) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=MZP0HoDBFJs) |
| Architecture, Dataset and Model-Scale Agnostic Data-Free Meta-Learning | [![GitHub](https://img.shields.io/github/stars/Egg-Hu/PURER?style=flat)](https://github.com/Egg-Hu/PURER) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Architecture_Dataset_and_Model-Scale_Agnostic_Data-Free_Meta-Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11183-b31b1b.svg)](http://arxiv.org/abs/2303.11183) |  [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lQiRUJ2BJII) |
| Preserving Linear Separability in Continual Learning by Backward Feature Projection | [![GitHub](https://img.shields.io/github/stars/rvl-lab-utoronto/BFP?style=flat)](https://github.com/rvl-lab-utoronto/BFP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Gu_Preserving_Linear_Separability_in_Continual_Learning_by_Backward_Feature_Projection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14595-b31b1b.svg)](http://arxiv.org/abs/2303.14595) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Y_68-p6_rw4) |
| Upcycling Models Under Domain and Category Shift | [![GitHub](https://img.shields.io/github/stars/ispc-lab/GLC?style=flat)](https://github.com/ispc-lab/GLC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_Upcycling_Models_Under_Domain_and_Category_Shift_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07110-b31b1b.svg)](https://arxiv.org/abs/2303.07110) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7rJaxChUfwA) |
| Class-Incremental Exemplar Compression for Class-Incremental Learning | [![GitHub](https://img.shields.io/github/stars/xfflzl/CIM-CIL?style=flat)](https://github.com/xfflzl/CIM-CIL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Class-Incremental_Exemplar_Compression_for_Class-Incremental_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14042-b31b1b.svg)](http://arxiv.org/abs/2303.14042) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kWhV00ehatE) |
| Learning Conditional Attributes for Compositional Zero-Shot Learning | [![GitHub](https://img.shields.io/github/stars/wqshmzh/CANet-CZSL?style=flat)](https://github.com/wqshmzh/CANet-CZSL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Learning_Conditional_Attributes_for_Compositional_Zero-Shot_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.17940-b31b1b.svg)](http://arxiv.org/abs/2305.17940) | :heavy_minus_sign: |
| BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning | [![GitHub](https://img.shields.io/github/stars/changdaeoh/BlackVIP?style=flat)](https://github.com/changdaeoh/BlackVIP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Oh_BlackVIP_Black-Box_Visual_Prompting_for_Robust_Transfer_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14773-b31b1b.svg)](http://arxiv.org/abs/2303.14773) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=sFsFwMXrmK8) |
| NoisyTwins: Class-Consistent and Diverse Image Generation Through StyleGANs | [![GitHub](https://img.shields.io/github/stars/val-iisc/NoisyTwins?style=flat)](https://github.com/val-iisc/NoisyTwins) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Rangwani_NoisyTwins_Class-Consistent_and_Diverse_Image_Generation_Through_StyleGANs_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05866-b31b1b.svg)](http://arxiv.org/abs/2304.05866) | :heavy_minus_sign: |
| Semi-Supervised Learning Made Simple With Self-Supervised Clustering | [![GitHub](https://img.shields.io/github/stars/pietroastolfi/suave-daino?style=flat)](https://github.com/pietroastolfi/suave-daino) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Fini_Semi-Supervised_Learning_Made_Simple_With_Self-Supervised_Clustering_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.07483-b31b1b.svg)](https://arxiv.org/abs/2306.07483) | :heavy_minus_sign: |
| Guiding Pseudo-Labels With Uncertainty Estimation for Source-Free Unsupervised Domain Adaptation | [![GitHub](https://img.shields.io/github/stars/MattiaLitrico/Guiding-Pseudo-labels-with-Uncertainty-Estimation-for-Source-free-Unsupervised-Domain-Adaptation?style=flat)](https://github.com/MattiaLitrico/Guiding-Pseudo-labels-with-Uncertainty-Estimation-for-Source-free-Unsupervised-Domain-Adaptation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Litrico_Guiding_Pseudo-Labels_With_Uncertainty_Estimation_for_Source-Free_Unsupervised_Domain_Adaptation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03770-b31b1b.svg)](http://arxiv.org/abs/2303.03770) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=gGng985-6PM) |
| PCR: Proxy-Based Contrastive Replay for Online Class-Incremental Continual Learning | [![GitHub](https://img.shields.io/github/stars/FelixHuiweiLin/PCR?style=flat)](https://github.com/FelixHuiweiLin/PCR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_PCR_Proxy-Based_Contrastive_Replay_for_Online_Class-Incremental_Continual_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04408-b31b1b.svg)](http://arxiv.org/abs/2304.04408) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=MmQ95EETwLg) |
| Modality-Agnostic Debiasing for Single Domain Generalization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_Modality-Agnostic_Debiasing_for_Single_Domain_Generalization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07123-b31b1b.svg)](http://arxiv.org/abs/2303.07123) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2sc_Wrqla-U) |
| Robust Mean Teacher for Continual and Gradual Test-Time Adaptation | [![GitHub](https://img.shields.io/github/stars/mariodoebler/test-time-adaptation?style=flat)](https://github.com/mariodoebler/test-time-adaptation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Dobler_Robust_Mean_Teacher_for_Continual_and_Gradual_Test-Time_Adaptation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13081-b31b1b.svg)](https://arxiv.org/abs/2211.13081) | :heavy_minus_sign: |
| Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Foundation_Model_Drives_Weakly_Incremental_Learning_for_Semantic_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.14250-b31b1b.svg)](http://arxiv.org/abs/2302.14250) | :heavy_minus_sign: |
| Hubs and Hyperspheres: Reducing Hubness and Improving Transductive Few-Shot Learning With Hyperspherical Embeddings | [![GitHub](https://img.shields.io/github/stars/uitml/noHub?style=flat)](https://github.com/uitml/noHub) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Trosten_Hubs_and_Hyperspheres_Reducing_Hubness_and_Improving_Transductive_Few-Shot_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09352-b31b1b.svg)](https://arxiv.org/abs/2303.09352) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FdJABwCa1qg) |
| Robust Test-Time Adaptation in Dynamic Scenarios | [![GitHub](https://img.shields.io/github/stars/BIT-DA/RoTTA?style=flat)](https://github.com/BIT-DA/RoTTA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yuan_Robust_Test-Time_Adaptation_in_Dynamic_Scenarios_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13899-b31b1b.svg)](http://arxiv.org/abs/2303.13899) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dEhpVOM0I6Q) |
| Source-Free Video Domain Adaptation With Spatial-Temporal-Historical Consistency Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Source-Free_Video_Domain_Adaptation_With_Spatial-Temporal-Historical_Consistency_Learning_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=1tTE9RoB3b0) |
| Heterogeneous Continual Learning <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/NVlabs/HCL?style=flat)](https://github.com/NVlabs/HCL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Madaan_Heterogeneous_Continual_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.08593-b31b1b.svg)](https://arxiv.org/abs/2306.08593) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8Il-bpWXeso) |
| Continual Detection Transformer for Incremental Object Detection | [![GitHub](https://img.shields.io/github/stars/yaoyao-liu/CL-DETR?style=flat)](https://github.com/yaoyao-liu/CL-DETR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Continual_Detection_Transformer_for_Incremental_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03110-b31b1b.svg)](http://arxiv.org/abs/2304.03110) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=BxOYhtmDb00) |
| NIFF: Alleviating Forgetting in Generalized Few-Shot Object Detection via Neural Instance Feature Forging | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Guirguis_NIFF_Alleviating_Forgetting_in_Generalized_Few-Shot_Object_Detection_via_Neural_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.04958-b31b1b.svg)](http://arxiv.org/abs/2303.04958) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mTeIrHbcEOY) |
| ViewNet: A Novel Projection-Based Backbone With View Pooling for Few-Shot Point Cloud Classification | [![GitHub](https://img.shields.io/github/stars/jiajingchen113322/ViewNet?style=flat)](https://github.com/jiajingchen113322/ViewNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_ViewNet_A_Novel_Projection-Based_Backbone_With_View_Pooling_for_Few-Shot_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| C-SFDA: A Curriculum Learning Aided Self-Training Framework for Efficient Source Free Domain Adaptation | [![GitHub](https://img.shields.io/github/stars/nazmul-karim170/C-SFDA_Source-Free-Domain-Adaptation?style=flat)](https://github.com/nazmul-karim170/C-SFDA_Source-Free-Domain-Adaptation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Karim_C-SFDA_A_Curriculum_Learning_Aided_Self-Training_Framework_for_Efficient_Source_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17132-b31b1b.svg)](https://arxiv.org/abs/2303.17132) | :heavy_minus_sign: |
| Train/Test-Time Adaptation With Retrieval | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zancato_TrainTest-Time_Adaptation_With_Retrieval_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14333-b31b1b.svg)](http://arxiv.org/abs/2303.14333) | :heavy_minus_sign: |
| Dealing With Cross-Task Class Discrimination in Online Continual Learning | [![GitHub](https://img.shields.io/github/stars/gydpku/GSA?style=flat)](https://github.com/gydpku/GSA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Dealing_With_Cross-Task_Class_Discrimination_in_Online_Continual_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.14657-b31b1b.svg)](https://arxiv.org/abs/2305.14657) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FfN-cpZqMD4) |
| Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning | [![GitHub](https://img.shields.io/github/stars/andytu28/VQT?style=flat)](https://github.com/andytu28/VQT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tu_Visual_Query_Tuning_Towards_Effective_Usage_of_Intermediate_Representations_for_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03220-b31b1b.svg)](http://arxiv.org/abs/2212.03220) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mjEd3YLcrKM) |
| Decoupling Learning and Remembering: A Bilevel Memory Framework With Knowledge Projection for Task-Incremental Learning | [![GitHub](https://img.shields.io/github/stars/SunWenJu123/BMKP?style=flat)](https://github.com/SunWenJu123/BMKP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Decoupling_Learning_and_Remembering_A_Bilevel_Memory_Framework_With_Knowledge_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://m.youtube.com/watch?v=YNb9HuWdfTM) |
| Neuro-Modulated Hebbian Learning for Fully Test-Time Adaptation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Neuro-Modulated_Hebbian_Learning_for_Fully_Test-Time_Adaptation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.00914-b31b1b.svg)](http://arxiv.org/abs/2303.00914) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=d1f4UDmAadk) |
| TIPI: Test Time Adaptation With Transformation Invariance | [![GitHub](https://img.shields.io/github/stars/atuannguyen/TIPI?style=flat)](https://github.com/atuannguyen/TIPI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Nguyen_TIPI_Test_Time_Adaptation_With_Transformation_Invariance_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Meta-Learning With a Geometry-Adaptive Preconditioner | [![GitHub](https://img.shields.io/github/stars/Suhyun777/CVPR23-GAP?style=flat)](https://github.com/Suhyun777/CVPR23-GAP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Meta-Learning_With_a_Geometry-Adaptive_Preconditioner_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01552-b31b1b.svg)](http://arxiv.org/abs/2304.01552) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LtmdVJEpXx4) |
| Meta-Tuning Loss Functions and Data Augmentation for Few-Shot Object Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.12161-b31b1b.svg)](http://arxiv.org/abs/2304.12161) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=yt3nYpichKU) |
| A Probabilistic Framework for Lifelong Test-Time Adaptation | [![GitHub](https://img.shields.io/github/stars/dhanajitb/petal?style=flat)](https://github.com/dhanajitb/petal) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Brahma_A_Probabilistic_Framework_for_Lifelong_Test-Time_Adaptation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09713-b31b1b.svg)](http://arxiv.org/abs/2212.09713) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=AdHwnYMHbkQ) |
| Few-Shot Class-Incremental Learning via Class-Aware Bilateral Distillation | [![GitHub](https://img.shields.io/github/stars/LinglanZhao/BiDistFSCIL?style=flat)](https://github.com/LinglanZhao/BiDistFSCIL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Few-Shot_Class-Incremental_Learning_via_Class-Aware_Bilateral_Distillation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lgT8SORUQmU) |
| CafeBoost: Causal Feature Boost To Eliminate Task-Induced Bias for Class Incremental Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Qiu_CafeBoost_Causal_Feature_Boost_To_Eliminate_Task-Induced_Bias_for_Class_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| A Strong Baseline for Generalized Few-Shot Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/sinahmr/DIaM?style=flat)](https://github.com/sinahmr/DIaM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hajimiri_A_Strong_Baseline_for_Generalized_Few-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14126-b31b1b.svg)](http://arxiv.org/abs/2211.14126) | :heavy_minus_sign: |
| Towards Better Stability and Adaptability: Improve Online Self-Training for Model Adaptation in Semantic Segmentation <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/DZhaoXd/DT-ST?style=flat)](https://github.com/DZhaoXd/DT-ST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Towards_Better_Stability_and_Adaptability_Improve_Online_Self-Training_for_Model_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| A New Benchmark: On the Utility of Synthetic Data With Blender for Bare Supervised Learning and Downstream Domain Adaptation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://huitangtang.github.io/On_the_Utility_of_Synthetic_Data/) <br /> [![GitHub](https://img.shields.io/github/stars/huitangtang/On_the_Utility_of_Synthetic_Data?style=flat)](https://github.com/huitangtang/On_the_Utility_of_Synthetic_Data) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_A_New_Benchmark_On_the_Utility_of_Synthetic_Data_With_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09165-b31b1b.svg)](http://arxiv.org/abs/2303.09165) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=XDr8v9sEK9c) |
| Cross-Image-Attention for Conditional Embeddings in Deep Metric Learning| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kotovenko_Cross-Image-Attention_for_Conditional_Embeddings_in_Deep_Metric_Learning_CVPR_2023_paper.pdf) |[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tD_JQI4ZvA0) |
| Principles of Forgetting in Domain-Incremental Semantic Segmentation in Adverse Weather Conditions | [![GitHub](https://img.shields.io/github/stars/tobiaskalb/feature-reuse-css?style=flat)](https://github.com/tobiaskalb/feature-reuse-css) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kalb_Principles_of_Forgetting_in_Domain-Incremental_Semantic_Segmentation_in_Adverse_Weather_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14115-b31b1b.svg)](http://arxiv.org/abs/2303.14115) | :heavy_minus_sign: |
| Data-Free Knowledge Distillation via Feature Exchange and Activation Region Constraint | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/skgyu/SpaceshipNet) <br /> [![GitHub](https://img.shields.io/github/stars/skgyu/SpaceshipNet?style=flat)](https://github.com/skgyu/SpaceshipNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Data-Free_Knowledge_Distillation_via_Feature_Exchange_and_Activation_Region_Constraint_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9kCiP_IyuO0) |
| (ML)<sup>2</sup>P-Encoder: On Exploration of Channel-Class Correlation for Multi-Label Zero-Shot Learning | [![GitHub](https://img.shields.io/github/stars/simonzmliu/cvpr23_mlzsl?style=flat)](https://github.com/simonzmliu/cvpr23_mlzsl) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_ML2P-Encoder_On_Exploration_of_Channel-Class_Correlation_for_Multi-Label_Zero-Shot_Learning_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=cDFeHsKx4Lk) |
| Finetune Like You Pretrain: Improved Finetuning of Zero-Shot Vision Models | [![GitHub](https://img.shields.io/github/stars/locuslab/FLYP?style=flat)](https://github.com/locuslab/FLYP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Goyal_Finetune_Like_You_Pretrain_Improved_Finetuning_of_Zero-Shot_Vision_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.00638-b31b1b.svg)](http://arxiv.org/abs/2212.00638) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jzJHgL9VGV4) |
| Simulated Annealing in Early Layers Leads to Better Generalization | [![GitHub](https://img.shields.io/github/stars/amiiir-sarfi/SEAL?style=flat)](https://github.com/amiiir-sarfi/SEAL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Sarfi_Simulated_Annealing_in_Early_Layers_Leads_to_Better_Generalization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04858-b31b1b.svg)](http://arxiv.org/abs/2304.04858) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VkQWI2exSdI) |
| A Data-Based Perspective on Transfer Learning | [![GitHub](https://img.shields.io/github/stars/MadryLab/data-transfer?style=flat)](https://github.com/MadryLab/data-transfer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_A_Data-Based_Perspective_on_Transfer_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2207.05739-b31b1b.svg)](http://arxiv.org/abs/2207.05739) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=i6YdYrkCP_0) |
| Learning Expressive Prompting With Residuals for Vision Transformers | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Das_Learning_Expressive_Prompting_With_Residuals_for_Vision_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15591-b31b1b.svg)](http://arxiv.org/abs/2303.15591) | :heavy_minus_sign: |
| Boosting Transductive Few-Shot Fine-Tuning With Margin-Based Uncertainty Weighting and Probability Regularization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tao_Boosting_Transductive_Few-Shot_Fine-Tuning_With_Margin-Based_Uncertainty_Weighting_and_Probability_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Improving Generalization With Domain Convex Game | [![GitHub](https://img.shields.io/github/stars/BIT-DA/DCG?style=flat)](https://github.com/BIT-DA/DCG) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lv_Improving_Generalization_With_Domain_Convex_Game_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13297-b31b1b.svg)](http://arxiv.org/abs/2303.13297) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8ao-C1Bjcjo) |
| Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game Perspective | [![GitHub](https://img.shields.io/github/stars/JinjingZhu/PMTrans?style=flat)](https://github.com/JinjingZhu/PMTrans) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Patch-Mix_Transformer_for_Unsupervised_Domain_Adaptation_A_Game_Perspective_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13434-b31b1b.svg)](http://arxiv.org/abs/2303.13434) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WNFlX0WFAO8) |
| Guided Recommendation for Model Fine-Tuning |  :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Guided_Recommendation_for_Model_Fine-Tuning_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Q4IktU97w-A) |
| Improving Generalization of Meta-Learning With Inverted Regularization at Inner-Level |:heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Improving_Generalization_of_Meta-Learning_With_Inverted_Regularization_at_Inner-Level_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DbLGnKn3Iv4) |
| Hint-Aug: Drawing Hints From Foundation Vision Transformers Towards Boosted Few-Shot Parameter-Efficient Tuning | [![GitHub](https://img.shields.io/github/stars/GATECH-EIC/Hint-Aug?style=flat)](https://github.com/GATECH-EIC/Hint-Aug) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Hint-Aug_Drawing_Hints_From_Foundation_Vision_Transformers_Towards_Boosted_Few-Shot_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.12520-b31b1b.svg)](https://arxiv.org/abs/2304.12520) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Ben48mkV5JY) |
