# CVPR-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/2023/main/humans-face-body-pose-gesture-movement.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/2023/main/recognition-categorization-detection-retrieval.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Transfer, Meta, Low-Shot, Continual, or Long-Tail Learning

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Dynamically Instance-Guided Adaptation: A Backward-Free Approach for Test-Time Domain Adaptive Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/Waybaba/DIGA?style=flat)](https://github.com/Waybaba/DIGA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Dynamically_Instance-Guided_Adaptation_A_Backward-Free_Approach_for_Test-Time_Domain_Adaptive_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Ikuwe0AS40A) |
| DETR With Additional Global Aggregation for Cross-Domain Weakly Supervised Object Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tang_DETR_With_Additional_Global_Aggregation_for_Cross-Domain_Weakly_Supervised_Object_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.07082-b31b1b.svg)](http://arxiv.org/abs/2304.07082) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=e8Aazn4A6aE) |
| Mind the Label Shift of Augmentation-Based Graph OOD Generalization | [![GitHub](https://img.shields.io/github/stars/samyu0304/lisa?style=flat)](https://github.com/samyu0304/lisa) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_Mind_the_Label_Shift_of_Augmentation-Based_Graph_OOD_Generalization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14859-b31b1b.svg)](http://arxiv.org/abs/2303.14859) | :heavy_minus_sign: |
| Long-Tailed Visual Recognition via Self-Heterogeneous Integration With Knowledge Excavation | [![GitHub](https://img.shields.io/github/stars/jinyan-06/SHIKE?style=flat)](https://github.com/jinyan-06/SHIKE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jin_Long-Tailed_Visual_Recognition_via_Self-Heterogeneous_Integration_With_Knowledge_Excavation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01279-b31b1b.svg)](http://arxiv.org/abs/2304.01279) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=InNbHCZ0pHU) |
| Understanding and Improving Visual Prompting: A Label-Mapping Perspective | [![GitHub](https://img.shields.io/github/stars/OPTML-Group/ILM-VP?style=flat)](https://github.com/OPTML-Group/ILM-VP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Understanding_and_Improving_Visual_Prompting_A_Label-Mapping_Perspective_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11635-b31b1b.svg)](http://arxiv.org/abs/2211.11635) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=x5qRQ4-my84) |
| A Whac-a-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others | [![GitHub](https://img.shields.io/github/stars/facebookresearch/Whac-A-Mole?style=flat)](https://github.com/facebookresearch/Whac-A-Mole) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_A_Whac-a-Mole_Dilemma_Shortcuts_Come_in_Multiples_Where_Mitigating_One_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04825-b31b1b.svg)](https://arxiv.org/abs/2212.04825) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=htYGHm53bJs) |
| Improved Distribution Matching for Dataset Condensation | [![GitHub](https://img.shields.io/github/stars/uitrbn/idm?style=flat)](https://github.com/uitrbn/idm) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhao_Improved_Distribution_Matching_for_Dataset_Condensation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09742-b31b1b.svg)](https://arxiv.org/abs/2307.09742) | :heavy_minus_sign: |
| Divide and Adapt: Active Domain Adaptation via Customized Learning <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/Duojun-Huang/DiaNA-CVPR2023?style=flat)](https://github.com/Duojun-Huang/DiaNA-CVPR2023) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Divide_and_Adapt_Active_Domain_Adaptation_via_Customized_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11618-b31b1b.svg)](https://arxiv.org/abs/2307.11618) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TDPmStHFcvs) |
| Class Relationship Embedded Learning for Source-Free Unsupervised Domain Adaptation | [![GitHub](https://img.shields.io/github/stars/zhyx12/CRCo?style=flat)](https://github.com/zhyx12/CRCo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Class_Relationship_Embedded_Learning_for_Source-Free_Unsupervised_Domain_Adaptation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5jdbHfeUW5A) |
| Diversity-Aware Meta Visual Prompting | [![GitHub](https://img.shields.io/github/stars/shikiw/DAM-VP?style=flat)](https://github.com/shikiw/DAM-VP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Diversity-Aware_Meta_Visual_Prompting_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08138-b31b1b.svg)](http://arxiv.org/abs/2303.08138) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5f7AEAArizo) |
| Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection | [![GitHub](https://img.shields.io/github/stars/LuFan31/ET-OOD?style=flat)](https://github.com/LuFan31/ET-OOD)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lu_Uncertainty-Aware_Optimal_Transport_for_Semantically_Coherent_Out-of-Distribution_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10449-b31b1b.svg)](http://arxiv.org/abs/2303.10449) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9UldUmvwd5o) |
| Zero-Shot Object Counting | [![GitHub](https://img.shields.io/github/stars/cvlab-stonybrook/zero-shot-counting?style=flat)](https://github.com/cvlab-stonybrook/zero-shot-counting)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Zero-Shot_Object_Counting_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02001-b31b1b.svg)](http://arxiv.org/abs/2303.02001)  | :heavy_minus_sign: |
| Learning With Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning | [![GitHub](https://img.shields.io/github/stars/zysong0113/SAVC?style=flat)](https://github.com/zysong0113/SAVC)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Song_Learning_With_Fantasy_Semantic-Aware_Virtual_Contrastive_Constraint_for_Few-Shot_Class-Incremental_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00426-b31b1b.svg)](http://arxiv.org/abs/2304.00426)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=s_Z608-foeo) |
| Distribution Shift Inversion for Out-of-Distribution Prediction | [![GitHub](https://img.shields.io/github/stars/yu-rp/Distribution-Shift-Iverson?style=flat)](https://github.com/yu-rp/Distribution-Shift-Iverson)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_Distribution_Shift_Inversion_for_Out-of-Distribution_Prediction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.08328-b31b1b.svg)](https://arxiv.org/abs/2306.08328)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-HUC8e6OFMI) |
| Endpoints Weight Fusion for Class Incremental Semantic Segmentation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xiao_Endpoints_Weight_Fusion_for_Class_Incremental_Semantic_Segmentation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=k-FKX67kOPk) |
| Promoting Semantic Connectivity: Dual Nearest Neighbors Contrastive Learning for Unsupervised Domain Generalization| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_Promoting_Semantic_Connectivity_Dual_Nearest_Neighbors_Contrastive_Learning_for_Unsupervised_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Class-Conditional Sharpness-Aware Minimization for Deep Long-Tailed Recognition | [![GitHub](https://img.shields.io/github/stars/zzpustc/CC-SAM?style=flat)](https://github.com/zzpustc/CC-SAM)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhou_Class-Conditional_Sharpness-Aware_Minimization_for_Deep_Long-Tailed_Recognition_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Q5KJa1MzpTs) |
| Meta-Causal Learning for Single Domain Generalization | [![GitHub](https://img.shields.io/github/stars/junkunyuan/Awesome-Domain-Generalization?style=flat)](https://github.com/junkunyuan/Awesome-Domain-Generalization)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Meta-Causal_Learning_for_Single_Domain_Generalization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03709-b31b1b.svg)](http://arxiv.org/abs/2304.03709)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=QKlRPUFypss) |
| VoP: Text-Video Co-Operative Prompt Tuning for Cross-Modal Retrieval | [![GitHub](https://img.shields.io/github/stars/bighuang624/VoP?style=flat)](https://github.com/bighuang624/VoP)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_VoP_Text-Video_Co-Operative_Prompt_Tuning_for_Cross-Modal_Retrieval_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12764-b31b1b.svg)](http://arxiv.org/abs/2211.12764) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ymdkiSSuOmI) |
| Learning Imbalanced Data With Vision Transformers | [![GitHub](https://img.shields.io/github/stars/XuZhengzhuo/LiVT?style=flat)](https://github.com/XuZhengzhuo/LiVT)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Learning_Imbalanced_Data_With_Vision_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.02015-b31b1b.svg)](http://arxiv.org/abs/2212.02015) | :heavy_minus_sign: |
| Sharpness-Aware Gradient Matching for Domain Generalization | [![GitHub](https://img.shields.io/github/stars/Wang-pengfei/SAGM?style=flat)](https://github.com/Wang-pengfei/SAGM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Sharpness-Aware_Gradient_Matching_for_Domain_Generalization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10353-b31b1b.svg)](http://arxiv.org/abs/2303.10353) | :heavy_minus_sign: |
| Geometry and Uncertainty-Aware 3D Point Cloud Class-Incremental Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/leolyj/3DPC-CISS?style=flat)](https://github.com/leolyj/3DPC-CISS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_Geometry_and_Uncertainty-Aware_3D_Point_Cloud_Class-Incremental_Semantic_Segmentation_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification & Segmentation | [![GitHub](https://img.shields.io/github/stars/dahyun-kang/cst?style=flat)](https://github.com/dahyun-kang/cst) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kang_Distilling_Self-Supervised_Vision_Transformers_for_Weakly-Supervised_Few-Shot_Classification__Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.03407-b31b1b.svg)](https://arxiv.org/abs/2307.03407) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-_8DzMiwemc) |
| Regularizing Second-Order Influences for Continual Learning | [![GitHub](https://img.shields.io/github/stars/feifeiobama/InfluenceCL?style=flat)](https://github.com/feifeiobama/InfluenceCL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sun_Regularizing_Second-Order_Influences_for_Continual_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10177-b31b1b.svg)](http://arxiv.org/abs/2304.10177) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6vbPquXtmrE) |
| I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/ferjad/I2DFormer?style=flat)](https://github.com/ferjad/I2DFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Naeem_I2MVFormer_Large_Language_Model_Generated_Multi-View_Document_Supervision_for_Zero-Shot_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.02291-b31b1b.svg)](http://arxiv.org/abs/2212.02291) | :heavy_minus_sign: |
| FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding | [![GitHub](https://img.shields.io/github/stars/uark-cviu/FREDOM?style=flat)](https://github.com/uark-cviu/FREDOM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Truong_FREDOM_Fairness_Domain_Adaptation_Approach_to_Semantic_Scene_Understanding_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02135-b31b1b.svg)](http://arxiv.org/abs/2304.02135) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Feo4UMd1eac) |
| Dense Network Expansion for Class Incremental Learning | [![GitHub](https://img.shields.io/github/stars/BinahHu/DNE?style=flat)](https://github.com/BinahHu/DNE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hu_Dense_Network_Expansion_for_Class_Incremental_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12696-b31b1b.svg)](http://arxiv.org/abs/2303.12696) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7BtHR7vEBWU) |
| Batch Model Consolidation: A Multi-Task Model Consolidation Framework | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://iordanis.me/stream_benchmark/) <br /> [![GitHub](https://img.shields.io/github/stars/fostiropoulos/stream_benchmark?style=flat)](https://github.com/fostiropoulos/stream_benchmark) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Fostiropoulos_Batch_Model_Consolidation_A_Multi-Task_Model_Consolidation_Framework_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.16484-b31b1b.svg)](https://arxiv.org/abs/2305.16484) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=v5uw1XpCum4) |
| DiGeo: Discriminative Geometry-Aware Learning for Generalized Few-Shot Object Detection | [![GitHub](https://img.shields.io/github/stars/Phoenix-V/DiGeo?style=flat)](https://github.com/Phoenix-V/DiGeo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ma_DiGeo_Discriminative_Geometry-Aware_Learning_for_Generalized_Few-Shot_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09674-b31b1b.svg)](http://arxiv.org/abs/2303.09674) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pcZYjsL52So) |
| ALOFT: A Lightweight MLP-Like Architecture With Dynamic Low-Frequency Transform for Domain Generalization | [![GitHub](https://img.shields.io/github/stars/lingeringlight/ALOFT?style=flat)](https://github.com/lingeringlight/ALOFT)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Guo_ALOFT_A_Lightweight_MLP-Like_Architecture_With_Dynamic_Low-Frequency_Transform_for_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11674-b31b1b.svg)](http://arxiv.org/abs/2303.11674) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LwWsrO1THio) |
| ZegCLIP: Towards Adapting CLIP for Zero-Shot Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/ZiqinZhou66/ZegCLIP?style=flat)](https://github.com/ZiqinZhou66/ZegCLIP)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhou_ZegCLIP_Towards_Adapting_CLIP_for_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03588-b31b1b.svg)](http://arxiv.org/abs/2212.03588)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8xeJsX2_h2w) |
| DiGA: <i>Di</i>stil to <i>G</i>eneralize and then <i>A</i>dapt for Domain Adaptive Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/fy-vision/DiGA?style=flat)](https://github.com/fy-vision/DiGA)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shen_DiGA_Distil_To_Generalize_and_Then_Adapt_for_Domain_Adaptive_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02222-b31b1b.svg)](http://arxiv.org/abs/2304.02222) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=AJKyXExH1_g) |
| Adjustment and Alignment for Unbiased Open Set Domain Adaptation | [![GitHub](https://img.shields.io/github/stars/CityU-AIM-Group/Anna?style=flat)](https://github.com/CityU-AIM-Group/Anna)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Adjustment_and_Alignment_for_Unbiased_Open_Set_Domain_Adaptation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hFZn16ntyXw) |
| Adapting Shortcut With Normalizing Flow: An Efficient Tuning Framework for Visual Recognition | [![GitHub](https://img.shields.io/github/stars/wang-yaoming/snf?style=flat)](https://github.com/wang-yaoming/snf)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Adapting_Shortcut_With_Normalizing_Flow_An_Efficient_Tuning_Framework_for_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| CODA-Prompt: COntinual Decomposed Attention-Based Prompting for Rehearsal-Free Continual Learning | [![GitHub](https://img.shields.io/github/stars/GT-RIPL/CODA-Prompt?style=flat)](https://github.com/GT-RIPL/CODA-Prompt)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Smith_CODA-Prompt_COntinual_Decomposed_Attention-Based_Prompting_for_Rehearsal-Free_Continual_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13218-b31b1b.svg)](https://arxiv.org/abs/2211.13218) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WNekhd7ddBo) |
| ConStruct-VL: Data-Free Continual Structured VL Concepts Learning | [![GitHub](https://img.shields.io/github/stars/jamessealesmith/ConStruct-VL?style=flat)](https://github.com/jamessealesmith/ConStruct-VL)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Smith_ConStruct-VL_Data-Free_Continual_Structured_VL_Concepts_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.09790-b31b1b.svg)](https://arxiv.org/abs/2211.09790) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kBMOEvfCO74) |
| Generalizing Dataset Distillation via Deep Generative Prior | [![GitHub](https://img.shields.io/github/stars/GeorgeCazenavette/glad?style=flat)](https://github.com/GeorgeCazenavette/glad)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cazenavette_Generalizing_Dataset_Distillation_via_Deep_Generative_Prior_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.01649-b31b1b.svg)](http://arxiv.org/abs/2305.01649) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Zpq4IarZBWk) |
| Few-Shot Learning With Visual Distribution Calibration and Cross-Modal Distribution Alignment | [![GitHub](https://img.shields.io/github/stars/bhrqw/SADA?style=flat)](https://github.com/bhrqw/SADA)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Few-Shot_Learning_With_Visual_Distribution_Calibration_and_Cross-Modal_Distribution_Alignment_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.11439-b31b1b.svg)](https://arxiv.org/abs/2305.11439) | :heavy_minus_sign: |
| Multi-Centroid Task Descriptor for Dynamic Class Incremental Inference | [![GitHub](https://img.shields.io/github/stars/CSIncWW/MultiCentroidGate?style=flat)](https://github.com/CSIncWW/MultiCentroidGate)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cai_Multi-Centroid_Task_Descriptor_for_Dynamic_Class_Incremental_Inference_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| DAA: A Delta Age AdaIN Operation for Age Estimation via Binary Code Transformer | [![GitHub](https://img.shields.io/github/stars/redcping/Delta_Age_AdaIN?style=flat)](https://github.com/redcping/Delta_Age_AdaIN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_DAA_A_Delta_Age_AdaIN_Operation_for_Age_Estimation_via_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07929-b31b1b.svg)](http://arxiv.org/abs/2303.07929) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mSXWAd3akn4) |
| Bilateral Memory Consolidation for Continual Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Nie_Bilateral_Memory_Consolidation_for_Continual_Learning_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Texts as Images in Prompt Tuning for Multi-Label Image Recognition | [![GitHub](https://img.shields.io/github/stars/guozix/TaI-DPT?style=flat)](https://github.com/guozix/TaI-DPT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Guo_Texts_as_Images_in_Prompt_Tuning_for_Multi-Label_Image_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12739-b31b1b.svg)](http://arxiv.org/abs/2211.12739) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=UgV2aFMel7o) |
| Learning Transformations To Reduce the Geometric Shift in Object Detection| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Vidit_Learning_Transformations_To_Reduce_the_Geometric_Shift_in_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.05496-b31b1b.svg)](http://arxiv.org/abs/2301.05496) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=B7L8YVvNPnw) |
| CLIP the Gap: A Single Domain Generalization Approach for Object Detection | [![GitHub](https://img.shields.io/github/stars/vidit09/domaingen?style=flat)](https://github.com/vidit09/domaingen) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Vidit_CLIP_the_Gap_A_Single_Domain_Generalization_Approach_for_Object_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.05499-b31b1b.svg)](http://arxiv.org/abs/2301.05499) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WDUu3gar4hY) |
| Transfer Knowledge From Head to Tail: Uncertainty Calibration Under Long-Tailed Distribution | [![GitHub](https://img.shields.io/github/stars/JiahaoChen1/Calibration?style=flat)](https://github.com/JiahaoChen1/Calibration) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Transfer_Knowledge_From_Head_to_Tail_Uncertainty_Calibration_Under_Long-Tailed_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06537-b31b1b.svg)](http://arxiv.org/abs/2304.06537) | :heavy_minus_sign: |
| Bi-Directional Distribution Alignment for Transductive Zero-Shot Learning | [![GitHub](https://img.shields.io/github/stars/Zhicaiwww/Bi-VAEGAN?style=flat)](https://github.com/Zhicaiwww/Bi-VAEGAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Bi-Directional_Distribution_Alignment_for_Transductive_Zero-Shot_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08698-b31b1b.svg)](http://arxiv.org/abs/2303.08698) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xr8FzK9civ8) |
| DARE-GRAM: Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices | [![GitHub](https://img.shields.io/github/stars/ismailnejjar/DARE-GRAM?style=flat)](https://github.com/ismailnejjar/DARE-GRAM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Nejjar_DARE-GRAM_Unsupervised_Domain_Adaptation_Regression_by_Aligning_Inverse_Gram_Matrices_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13325-b31b1b.svg)](http://arxiv.org/abs/2303.13325) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=iwTe35fEQ3c) |
| LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models | [![GitHub](https://img.shields.io/github/stars/1adrianb/lasp?style=flat)](https://github.com/1adrianb/lasp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Bulat_LASP_Text-to-Text_Optimization_for_Language-Aware_Soft_Prompting_of_Vision__CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.01115-b31b1b.svg)](http://arxiv.org/abs/2210.01115) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=h0RS-UbCjGg) |
| Open-Set Likelihood Maximization for Few-Shot Learning | [![GitHub](https://img.shields.io/github/stars/ebennequin/few-shot-open-set?style=flat)](https://github.com/ebennequin/few-shot-open-set) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Boudiaf_Open-Set_Likelihood_Maximization_for_Few-Shot_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.08390-b31b1b.svg)](http://arxiv.org/abs/2301.08390) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EbgRdKDBvKQ) |
| WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation | [![GitHub](https://img.shields.io/github/stars/caoyunkang/WinClip?style=flat)](https://github.com/caoyunkang/WinClip)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jeong_WinCLIP_Zero-Few-Shot_Anomaly_Classification_and_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14814-b31b1b.svg)](https://arxiv.org/abs/2303.14814)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=z2v3FvhLELY) |
| Federated Domain Generalization With Generalization Adjustment | [![GitHub](https://img.shields.io/github/stars/MediaBrain-SJTU/FedDG-GA?style=flat)](https://github.com/MediaBrain-SJTU/FedDG-GA)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Federated_Domain_Generalization_With_Generalization_Adjustment_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EESJrJGCSR4) |
| ProtoCon: Pseudo-Label Refinement via Online Clustering and Prototypical Consistency for Efficient Semi-Supervised Learning <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Nassar_ProtoCon_Pseudo-Label_Refinement_via_Online_Clustering_and_Prototypical_Consistency_for_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13556-b31b1b.svg)](http://arxiv.org/abs/2303.13556) | :heavy_minus_sign: |
| DA-DETR: Domain Adaptive Detection Transformer With Information Fusion | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_DA-DETR_Domain_Adaptive_Detection_Transformer_With_Information_Fusion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2103.17084-b31b1b.svg)](https://arxiv.org/abs/2103.17084) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=48WIZVhnJHU) |
| Harmonious Teacher for Cross-Domain Object Detection | [![GitHub](https://img.shields.io/github/stars/kinredon/Harmonious-Teacher?style=flat)](https://github.com/kinredon/Harmonious-Teacher)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Deng_Harmonious_Teacher_for_Cross-Domain_Object_Detection_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=puUr6R0id-w) |
| AutoLabel: CLIP-Based Framework for Open-Set Video Domain Adaptation | [![GitHub](https://img.shields.io/github/stars/gzaraunitn/autolabel?style=flat)](https://github.com/gzaraunitn/autolabel)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zara_AutoLabel_CLIP-Based_Framework_for_Open-Set_Video_Domain_Adaptation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01110-b31b1b.svg)](http://arxiv.org/abs/2304.01110) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WOXwZd1InAg) |
| Task Difficulty Aware Parameter Allocation & Regularization for Lifelong Learning| [![GitHub](https://img.shields.io/github/stars/WenjinW/PAR?style=flat)](https://github.com/WenjinW/PAR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Task_Difficulty_Aware_Parameter_Allocation__Regularization_for_Lifelong_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05288-b31b1b.svg)](http://arxiv.org/abs/2304.05288) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=R0jA9rHxIWI) |
| Revisiting Prototypical Network for Cross Domain Few-Shot Learning | [![GitHub](https://img.shields.io/github/stars/NWPUZhoufei/LDP-Net?style=flat)](https://github.com/NWPUZhoufei/LDP-Net)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhou_Revisiting_Prototypical_Network_for_Cross_Domain_Few-Shot_Learning_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Federated Incremental Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/JiahuaDong/FISS?style=flat)](https://github.com/JiahuaDong/FISS)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Dong_Federated_Incremental_Semantic_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04620-b31b1b.svg)](http://arxiv.org/abs/2304.04620) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=uWIDgPfFMSA) |
| Semantic Prompt for Few-Shot Image Recognition | [![GitHub](https://img.shields.io/github/stars/WentaoChen0813/SemanticPrompt?style=flat)](https://github.com/WentaoChen0813/SemanticPrompt)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Semantic_Prompt_for_Few-Shot_Image_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14123-b31b1b.svg)](http://arxiv.org/abs/2303.14123) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7uem_CHedjM) |
| Rethinking Gradient Projection Continual Learning: Stability / Plasticity Feature Space Decoupling | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhao_Rethinking_Gradient_Projection_Continual_Learning_Stability__Plasticity_Feature_Space_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| No One Left Behind: Improving the Worst Categories in Long-Tailed Learning| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Du_No_One_Left_Behind_Improving_the_Worst_Categories_in_Long-Tailed_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03630-b31b1b.svg)](http://arxiv.org/abs/2303.03630) | :heavy_minus_sign: |
| Meta Omnium: A Benchmark for General-Purpose Learning-To-Learn | [![GitHub](https://img.shields.io/github/stars/edi-meta-learning/meta-omnium?style=flat)](https://github.com/edi-meta-learning/meta-omnium) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Bohdal_Meta_Omnium_A_Benchmark_for_General-Purpose_Learning-To-Learn_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.07625-b31b1b.svg)](http://arxiv.org/abs/2305.07625) | :heavy_minus_sign: |
| Transductive Few-Shot Learning With Prototype-Based Label Propagation by Iterative Graph Refinement | [![GitHub](https://img.shields.io/github/stars/allenhaozhu/protoLP?style=flat)](https://github.com/allenhaozhu/protoLP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhu_Transductive_Few-Shot_Learning_With_Prototype-Based_Label_Propagation_by_Iterative_Graph_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.11598-b31b1b.svg)](http://arxiv.org/abs/2304.11598) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=sfQQy0eU3RA) |
| COT: Unsupervised Domain Adaptation With Clustering and Optimal Transport | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_COT_Unsupervised_Domain_Adaptation_With_Clustering_and_Optimal_Transport_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Semi-Supervised Domain Adaptation With Source Label Adaptation | [![GitHub](https://img.shields.io/github/stars/chu0802/SLA?style=flat)](https://github.com/chu0802/SLA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_Semi-Supervised_Domain_Adaptation_With_Source_Label_Adaptation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.02335-b31b1b.svg)](http://arxiv.org/abs/2302.02335) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=oVOQI9c1hYE) |
| MetaMix: Towards Corruption-Robust Continual Learning With Temporally Self-Adaptive Data Transformation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_MetaMix_Towards_Corruption-Robust_Continual_Learning_With_Temporally_Self-Adaptive_Data_Transformation_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Visual-Language Prompt Tuning With Knowledge-Guided Context Optimization | [![GitHub](https://img.shields.io/github/stars/htyao89/KgCoOp?style=flat)](https://github.com/htyao89/KgCoOp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yao_Visual-Language_Prompt_Tuning_With_Knowledge-Guided_Context_Optimization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13283-b31b1b.svg)](http://arxiv.org/abs/2303.13283) | :heavy_minus_sign: |
| Modeling Inter-Class and Intra-Class Constraints in Novel Class Discovery | [![GitHub](https://img.shields.io/github/stars/FanZhichen/NCD-IIC?style=flat)](https://github.com/FanZhichen/NCD-IIC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Modeling_Inter-Class_and_Intra-Class_Constraints_in_Novel_Class_Discovery_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.03591-b31b1b.svg)](http://arxiv.org/abs/2210.03591) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kpSOVAKwYaQ) |
| Real-Time Evaluation in Online Continual Learning: A New Hope <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/Yasir-Ghunaim/RealtimeOCL?style=flat)](https://github.com/Yasir-Ghunaim/RealtimeOCL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ghunaim_Real-Time_Evaluation_in_Online_Continual_Learning_A_New_Hope_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.01047-b31b1b.svg)](http://arxiv.org/abs/2302.01047) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vkIIIHK_F0E) |
| Partial Network Cloning | [![GitHub](https://img.shields.io/github/stars/JngwenYe/PNCloning?style=flat)](https://github.com/JngwenYe/PNCloning)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ye_Partial_Network_Cloning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10597-b31b1b.svg)](http://arxiv.org/abs/2303.10597)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JV8Ozatq56s) |
| Rebalancing Batch Normalization for Exemplar-Based Class-Incremental Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cha_Rebalancing_Batch_Normalization_for_Exemplar-Based_Class-Incremental_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2201.12559-b31b1b.svg)](http://arxiv.org/abs/2201.12559) | :heavy_minus_sign: |
| EcoTTA: Memory-Efficient Continual Test-Time Adaptation via Self-Distilled Regularization | [![GitHub](https://img.shields.io/github/stars/Lily-Le/EcoTTA?style=flat)](https://github.com/Lily-Le/EcoTTA)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Song_EcoTTA_Memory-Efficient_Continual_Test-Time_Adaptation_via_Self-Distilled_Regularization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.01904-b31b1b.svg)](http://arxiv.org/abs/2303.01904) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lYdTfZmbPc4) |
| Feature Alignment and Uniformity for Test Time Adaptation | [![GitHub](https://img.shields.io/github/stars/SakurajimaMaiii/TSD?style=flat)](https://github.com/SakurajimaMaiii/TSD)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Feature_Alignment_and_Uniformity_for_Test_Time_Adaptation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10902-b31b1b.svg)](http://arxiv.org/abs/2303.10902)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jdTTPwNhtT4) |
| Bootstrap Your Own Prior: Towards Distribution-Agnostic Novel Class Discovery | [![GitHub](https://img.shields.io/github/stars/muliyangm/BYOP?style=flat)](https://github.com/muliyangm/BYOP)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_Bootstrap_Your_Own_Prior_Towards_Distribution-Agnostic_Novel_Class_Discovery_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=oDhQIHHr9w8) |
| Towards Realistic Long-Tailed Semi-Supervised Learning: Consistency Is All You Need | [![GitHub](https://img.shields.io/github/stars/Gank0078/ACR?style=flat)](https://github.com/Gank0078/ACR)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wei_Towards_Realistic_Long-Tailed_Semi-Supervised_Learning_Consistency_Is_All_You_Need_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tYQb0fsI8bE) |
| Balanced Product of Calibrated Experts for Long-Tailed Recognition | [![GitHub](https://img.shields.io/github/stars/emasa/BalPoE-CalibratedLT?style=flat)](https://github.com/emasa/BalPoE-CalibratedLT)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Aimar_Balanced_Product_of_Calibrated_Experts_for_Long-Tailed_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.05260-b31b1b.svg)](http://arxiv.org/abs/2206.05260) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=H664_EQq2cs) |
| Unsupervised Continual Semantic Adaptation Through Neural Rendering | [![GitHub](https://img.shields.io/github/stars/ethz-asl/ucsa_neural_rendering?style=flat)](https://github.com/ethz-asl/ucsa_neural_rendering)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_Unsupervised_Continual_Semantic_Adaptation_Through_Neural_Rendering_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13969-b31b1b.svg)](http://arxiv.org/abs/2211.13969)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=XfNLsl8ATNY) |
| Computationally Budgeted Continual Learning: What Does Matter? | [![GitHub](https://img.shields.io/github/stars/drimpossible/BudgetCL?style=flat)](https://github.com/drimpossible/BudgetCL)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Prabhu_Computationally_Budgeted_Continual_Learning_What_Does_Matter_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11165-b31b1b.svg)](http://arxiv.org/abs/2303.11165)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3nmRtFkF5fw) |
| Ground-Truth Free Meta-Learning for Deep Compressive Sampling | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Qin_Ground-Truth_Free_Meta-Learning_for_Deep_Compressive_Sampling_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LXE2eLzglUM) |
| Multi-Level Logit Distillation | [![GitHub](https://img.shields.io/github/stars/Jin-Ying/Multi-Level-Logit-Distillation?style=flat)](https://github.com/Jin-Ying/Multi-Level-Logit-Distillation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jin_Multi-Level_Logit_Distillation_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| StyleAdv: Meta Style Adversarial Training for Cross-Domain Few-Shot Learning | [![GitHub](https://img.shields.io/github/stars/lovelyqian/StyleAdv-CDFSL?style=flat)](https://github.com/lovelyqian/StyleAdv-CDFSL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Fu_StyleAdv_Meta_Style_Adversarial_Training_for_Cross-Domain_Few-Shot_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.09309-b31b1b.svg)](http://arxiv.org/abs/2302.09309) |  [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=YB-S2YF22mc) |
| MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation | [![GitHub](https://img.shields.io/github/stars/lhoyer/MIC?style=flat)](https://github.com/lhoyer/MIC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hoyer_MIC_Masked_Image_Consistency_for_Context-Enhanced_Domain_Adaptation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.01322-b31b1b.svg)](http://arxiv.org/abs/2212.01322) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=05s1zablJaY) |
| On the Stability-Plasticity Dilemma of Class-Incremental Learning| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kim_On_the_Stability-Plasticity_Dilemma_of_Class-Incremental_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01663-b31b1b.svg)](http://arxiv.org/abs/2304.01663) | :heavy_minus_sign: |
| TeSLA: Test-Time Self-Learning With Automatic Adversarial Augmentation | [![GitHub](https://img.shields.io/github/stars/devavratTomar/TeSLA?style=flat)](https://github.com/devavratTomar/TeSLA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tomar_TeSLA_Test-Time_Self-Learning_With_Automatic_Adversarial_Augmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09870-b31b1b.svg)](http://arxiv.org/abs/2303.09870) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_lLZm2v05dY) |
| MHPL: Minimum Happy Points Learning for Active Source Free Domain Adaptation| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_MHPL_Minimum_Happy_Points_Learning_for_Active_Source_Free_Domain_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2205.10711-b31b1b.svg)](https://arxiv.org/abs/2205.10711) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=foefkl9tRJ4) |
| CIGAR: Cross-Modality Graph Reasoning for Domain Adaptive Object Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_CIGAR_Cross-Modality_Graph_Reasoning_for_Domain_Adaptive_Object_Detection_CVPR_2023_paper.pdf)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=fgf8G_FwiQM) |
| Adaptive Plasticity Improvement for Continual Learning | [![GitHub](https://img.shields.io/github/stars/liangyanshuo/Adaptive-Plasticity-Improvement-for-Continual-Learning?style=flat)](https://github.com/liangyanshuo/Adaptive-Plasticity-Improvement-for-Continual-Learning) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liang_Adaptive_Plasticity_Improvement_for_Continual_Learning_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Achieving a Better Stability-Plasticity Trade-Off via Auxiliary Networks in Continual Learning | [![GitHub](https://img.shields.io/github/stars/kim-sanghwan/ANCL?style=flat)](https://github.com/kim-sanghwan/ANCL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kim_Achieving_a_Better_Stability-Plasticity_Trade-Off_via_Auxiliary_Networks_in_Continual_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09483-b31b1b.svg)](http://arxiv.org/abs/2303.09483) | :heavy_minus_sign: |
| Few-Shot Geometry-Aware Keypoint Localization |  |  |  |
| Spatio-Temporal Pixel-Level Contrastive Learning-based Source-Free Domain Adaptation for Video Semantic Segmentation |  |  |  |
| Both Style and Distortion Matter: Dual-Path Unsupervised Domain Adaptation for Panoramic Semantic Segmentation |  |  |  |
| Bi-Level Meta-Learning for Few-Shot Domain Generalization |  |  |  |
| Few-Shot Referring Relationships in Videos |  |  |  |
| Exploring Data Geometry for Continual Learning |  |  |  |
| Masked Images Are Counterfactual Samples for Robust Fine-Tuning |  |  |  |
| DKT: Diverse Knowledge Transfer Transformer for Class Incremental Learning |  |  |  |
| CoMFormer: Continual Learning in Semantic and Panoptic Segmentation |  |  |  |
| Global and Local Mixture Consistency Cumulative Learning for Long-tailed Visual Recognitions |  |  |  |
| Class Attention Transfer based Knowledge Distillation |  |  |  |
| Hard Sample Matters a Lot in Zero-Shot Quantization |  |  |  |
| Back to the Source: Diffusion-Driven Adaptation to Test-Time Corruption |  |  |  |
| SuperDisco: Super-Class Discovery Improves Visual Recognition for the Long-Tail |  |  |  |
| Architecture, Dataset and Model-Scale Agnostic Data-Free Meta-Learning |  |  |  |
| Preserving Linear Separability in Continual Learning by Backward Feature Projection|  |  |  |
| Upcycling Models under Domain and Category Shift |  |  |  |
| Class-Incremental Exemplar Compression for Class-Incremental Learning |  |  |  |
| Learning Conditional Attributes for Compositional Zero-Shot Learning |  |  |  |
| BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning |  |  |  |
| NoisyTwins: Class-Consistent and Diverse Image Generation through StyleGANs |  |  |  |
| Semi-Supervised Learning Made Simple with Self-Supervised Clustering |  |  |  |
| Guiding Pseudo-Labels with Uncertainty Estimation for Source-Free Unsupervised Domain Adaptation |  |  |  |
| PCR: Proxy-based Contrastive Replay for Online Class-Incremental Continual Learning |  |  |  |
| Modality-Agnostic Debiasing for Single Domain Generalization |  |  |  |
| Robust Mean Teacher for Continual and Gradual Test-Time Adaptation |  |  |  |
| Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation |  |  |  |
| Hubs and Hyperspheres: Reducing Hubness and Improving Transductive Few-Shot Learning with Hyperspherical Embeddings |  |  |  |
| Robust Test-Time Adaptation in Dynamic Scenarios |  |  |  |
| Source-Free Video Domain Adaptation with Spatial-Temporal-Historical Consistency Learning |  |  |  |
| Heterogeneous Continual Learning |  |  |  |
| Continual Detection Transformer for Incremental Object Detection |  |  |  |
| NIFF: Alleviating Forgetting in Generalized Few-Shot Object Detection via Neural Instance Feature Forging |  |  |  |
| ViewNet: A Novel Projection-based Backbone with View Pooling for Few-Shot Point Cloud Classification |  |  |  |
| C-SFDA: A Curriculum Learning Aided Self-Training Framework for Efficient Source Free Domain Adaptation |  |  |  |
| Train/Test-Time Adaptation with Retrieval |  |  |  |
| Dealing with Cross-Task Class Discrimination in Online Continual Learning |  |  |  |
| Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning |  |  |  |
| Decoupling Learning and Remembering: A Bilevel Memory Framework with Knowledge Projection for Task-Incremental Learning |  |  |  |
| Neuro-Modulated Hebbian Learning for Fully Test-Time Adaptation |  |  |  |
| TIPI: Test Time Adaptation with Transformation Invariance |  |  |  |
| Meta-Learning with a Geometry-Adaptive Preconditioner |  |  |  |
| Meta-Tuning Loss Functions and Data Augmentation for Few-Shot Object Detection |  |  |  |
| A Probabilistic Framework for Lifelong Test-Time Adaptation |  |  |  |
| Few-Shot Class-Incremental Learning via Class-Aware Bilateral Distillation |  |  |  |
| CafeBoost: Causal Feature Boost to Eliminate Task-Induced Bias for Class Incremental Learning |  |  |  |
| A Strong Baseline for Generalized Few-Shot Semantic Segmentation |  |  |  |
| Towards Better Stability and Adaptability: Improve Online Self-Training for Model Adaptation in Semantic Segmentation |  |  |  |
| A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation |  |  |  |
| Cross-Image-Attention for Conditional Embeddings in Deep Metric Learning |  |  |  |
| Principles of Forgetting in Domain-Incremental Semantic Segmentation in Adverse Weather Conditions |  |  |  |
| Data-Free Knowledge Distillation via Feature Exchange and Activation Region Constraint |  |  |  |
| (ML)<sup>2</sup>P-Encoder: On Exploration of Channel-Class Correlation for Multi-Label Zero-Shot Learning |  |  |  |
| Finetune Like You Pretrain: Improved Finetuning of Zero-Shot Vision Models |  |  |  |
| Simulated Annealing in Early Layers Leads to Better Generalization |  |  |  |
| A Data-based Perspective on Transfer Learning |  |  |  |
| Learning Expressive Prompting with Residuals for Vision Transformers |  |  |  |
| Boosting Transductive Few-Shot Fine-Tuning with Margin-based Uncertainty Weighting and Probability Regularization |  |  |  |
| Improving Generalization with Domain Convex Game |  |  |  |
| Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game Perspective |  |  |  |
| Guided Recommendation for Model Fine-Tuning |  |  |  |
| Improving Generalization of Meta-Learning with Inverted Regularization at Inner-Level |  |  |  |
| Hint-Aug: Drawing Hints from Foundation Vision Transformers towards Boosted Few-Shot Parameter-Efficient Tuning |  |  |  |
