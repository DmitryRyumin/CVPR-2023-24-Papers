# CVPR-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/2023/main/humans-face-body-pose-gesture-movement.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/2023/main/recognition-categorization-detection-retrieval.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Transfer, Meta, Low-Shot, Continual, or Long-Tail Learning

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Dynamically Instance-Guided Adaptation: A Backward-Free Approach for Test-Time Domain Adaptive Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/Waybaba/DIGA?style=flat)](https://github.com/Waybaba/DIGA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Dynamically_Instance-Guided_Adaptation_A_Backward-Free_Approach_for_Test-Time_Domain_Adaptive_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Ikuwe0AS40A) |
| DETR With Additional Global Aggregation for Cross-Domain Weakly Supervised Object Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tang_DETR_With_Additional_Global_Aggregation_for_Cross-Domain_Weakly_Supervised_Object_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.07082-b31b1b.svg)](http://arxiv.org/abs/2304.07082) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=e8Aazn4A6aE) |
| Mind the Label Shift of Augmentation-Based Graph OOD Generalization | [![GitHub](https://img.shields.io/github/stars/samyu0304/lisa?style=flat)](https://github.com/samyu0304/lisa) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_Mind_the_Label_Shift_of_Augmentation-Based_Graph_OOD_Generalization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14859-b31b1b.svg)](http://arxiv.org/abs/2303.14859) | :heavy_minus_sign: |
| Long-Tailed Visual Recognition via Self-Heterogeneous Integration With Knowledge Excavation | [![GitHub](https://img.shields.io/github/stars/jinyan-06/SHIKE?style=flat)](https://github.com/jinyan-06/SHIKE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jin_Long-Tailed_Visual_Recognition_via_Self-Heterogeneous_Integration_With_Knowledge_Excavation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01279-b31b1b.svg)](http://arxiv.org/abs/2304.01279) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=InNbHCZ0pHU) |
| Understanding and Improving Visual Prompting: A Label-Mapping Perspective | [![GitHub](https://img.shields.io/github/stars/OPTML-Group/ILM-VP?style=flat)](https://github.com/OPTML-Group/ILM-VP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Understanding_and_Improving_Visual_Prompting_A_Label-Mapping_Perspective_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11635-b31b1b.svg)](http://arxiv.org/abs/2211.11635) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=x5qRQ4-my84) |
| A Whac-a-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others | [![GitHub](https://img.shields.io/github/stars/facebookresearch/Whac-A-Mole?style=flat)](https://github.com/facebookresearch/Whac-A-Mole) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_A_Whac-a-Mole_Dilemma_Shortcuts_Come_in_Multiples_Where_Mitigating_One_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04825-b31b1b.svg)](https://arxiv.org/abs/2212.04825) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=htYGHm53bJs) |
| Improved Distribution Matching for Dataset Condensation | [![GitHub](https://img.shields.io/github/stars/uitrbn/idm?style=flat)](https://github.com/uitrbn/idm) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhao_Improved_Distribution_Matching_for_Dataset_Condensation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09742-b31b1b.svg)](https://arxiv.org/abs/2307.09742) | :heavy_minus_sign: |
| Divide and Adapt: Active Domain Adaptation via Customized Learning <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/Duojun-Huang/DiaNA-CVPR2023?style=flat)](https://github.com/Duojun-Huang/DiaNA-CVPR2023) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Divide_and_Adapt_Active_Domain_Adaptation_via_Customized_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11618-b31b1b.svg)](https://arxiv.org/abs/2307.11618) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TDPmStHFcvs) |
| Class Relationship Embedded Learning for Source-Free Unsupervised Domain Adaptation | [![GitHub](https://img.shields.io/github/stars/zhyx12/CRCo?style=flat)](https://github.com/zhyx12/CRCo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Class_Relationship_Embedded_Learning_for_Source-Free_Unsupervised_Domain_Adaptation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5jdbHfeUW5A) |
| Diversity-Aware Meta Visual Prompting | [![GitHub](https://img.shields.io/github/stars/shikiw/DAM-VP?style=flat)](https://github.com/shikiw/DAM-VP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Diversity-Aware_Meta_Visual_Prompting_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08138-b31b1b.svg)](http://arxiv.org/abs/2303.08138) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5f7AEAArizo) |
| Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection | [![GitHub](https://img.shields.io/github/stars/LuFan31/ET-OOD?style=flat)](https://github.com/LuFan31/ET-OOD)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lu_Uncertainty-Aware_Optimal_Transport_for_Semantically_Coherent_Out-of-Distribution_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10449-b31b1b.svg)](http://arxiv.org/abs/2303.10449) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9UldUmvwd5o) |
| Zero-Shot Object Counting | [![GitHub](https://img.shields.io/github/stars/cvlab-stonybrook/zero-shot-counting?style=flat)](https://github.com/cvlab-stonybrook/zero-shot-counting)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Zero-Shot_Object_Counting_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02001-b31b1b.svg)](http://arxiv.org/abs/2303.02001)  | :heavy_minus_sign: |
| Learning With Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning | [![GitHub](https://img.shields.io/github/stars/zysong0113/SAVC?style=flat)](https://github.com/zysong0113/SAVC)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Song_Learning_With_Fantasy_Semantic-Aware_Virtual_Contrastive_Constraint_for_Few-Shot_Class-Incremental_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00426-b31b1b.svg)](http://arxiv.org/abs/2304.00426)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=s_Z608-foeo) |
| Distribution Shift Inversion for Out-of-Distribution Prediction | [![GitHub](https://img.shields.io/github/stars/yu-rp/Distribution-Shift-Iverson?style=flat)](https://github.com/yu-rp/Distribution-Shift-Iverson)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_Distribution_Shift_Inversion_for_Out-of-Distribution_Prediction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.08328-b31b1b.svg)](https://arxiv.org/abs/2306.08328)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-HUC8e6OFMI) |
| Endpoints Weight Fusion for Class Incremental Semantic Segmentation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xiao_Endpoints_Weight_Fusion_for_Class_Incremental_Semantic_Segmentation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=k-FKX67kOPk) |
| Promoting Semantic Connectivity: Dual Nearest Neighbors Contrastive Learning for Unsupervised Domain Generalization| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_Promoting_Semantic_Connectivity_Dual_Nearest_Neighbors_Contrastive_Learning_for_Unsupervised_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Class-Conditional Sharpness-Aware Minimization for Deep Long-Tailed Recognition | [![GitHub](https://img.shields.io/github/stars/zzpustc/CC-SAM?style=flat)](https://github.com/zzpustc/CC-SAM)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhou_Class-Conditional_Sharpness-Aware_Minimization_for_Deep_Long-Tailed_Recognition_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Q5KJa1MzpTs) |
| Meta-Causal Learning for Single Domain Generalization | [![GitHub](https://img.shields.io/github/stars/junkunyuan/Awesome-Domain-Generalization?style=flat)](https://github.com/junkunyuan/Awesome-Domain-Generalization)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Meta-Causal_Learning_for_Single_Domain_Generalization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03709-b31b1b.svg)](http://arxiv.org/abs/2304.03709)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=QKlRPUFypss) |
| VoP: Text-Video Co-Operative Prompt Tuning for Cross-Modal Retrieval | [![GitHub](https://img.shields.io/github/stars/bighuang624/VoP?style=flat)](https://github.com/bighuang624/VoP)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_VoP_Text-Video_Co-Operative_Prompt_Tuning_for_Cross-Modal_Retrieval_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12764-b31b1b.svg)](http://arxiv.org/abs/2211.12764) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ymdkiSSuOmI) |
| Learning Imbalanced Data With Vision Transformers | [![GitHub](https://img.shields.io/github/stars/XuZhengzhuo/LiVT?style=flat)](https://github.com/XuZhengzhuo/LiVT)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Learning_Imbalanced_Data_With_Vision_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.02015-b31b1b.svg)](http://arxiv.org/abs/2212.02015) | :heavy_minus_sign: |
| Sharpness-Aware Gradient Matching for Domain Generalization | [![GitHub](https://img.shields.io/github/stars/Wang-pengfei/SAGM?style=flat)](https://github.com/Wang-pengfei/SAGM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Sharpness-Aware_Gradient_Matching_for_Domain_Generalization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10353-b31b1b.svg)](http://arxiv.org/abs/2303.10353) | :heavy_minus_sign: |
| Geometry and Uncertainty-Aware 3D Point Cloud Class-Incremental Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/leolyj/3DPC-CISS?style=flat)](https://github.com/leolyj/3DPC-CISS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_Geometry_and_Uncertainty-Aware_3D_Point_Cloud_Class-Incremental_Semantic_Segmentation_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification & Segmentation | [![GitHub](https://img.shields.io/github/stars/dahyun-kang/cst?style=flat)](https://github.com/dahyun-kang/cst) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kang_Distilling_Self-Supervised_Vision_Transformers_for_Weakly-Supervised_Few-Shot_Classification__Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.03407-b31b1b.svg)](https://arxiv.org/abs/2307.03407) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-_8DzMiwemc) |
| Regularizing Second-Order Influences for Continual Learning | [![GitHub](https://img.shields.io/github/stars/feifeiobama/InfluenceCL?style=flat)](https://github.com/feifeiobama/InfluenceCL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sun_Regularizing_Second-Order_Influences_for_Continual_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10177-b31b1b.svg)](http://arxiv.org/abs/2304.10177) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6vbPquXtmrE) |
| I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/ferjad/I2DFormer?style=flat)](https://github.com/ferjad/I2DFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Naeem_I2MVFormer_Large_Language_Model_Generated_Multi-View_Document_Supervision_for_Zero-Shot_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.02291-b31b1b.svg)](http://arxiv.org/abs/2212.02291) | :heavy_minus_sign: |
| FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding | [![GitHub](https://img.shields.io/github/stars/uark-cviu/FREDOM?style=flat)](https://github.com/uark-cviu/FREDOM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Truong_FREDOM_Fairness_Domain_Adaptation_Approach_to_Semantic_Scene_Understanding_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02135-b31b1b.svg)](http://arxiv.org/abs/2304.02135) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Feo4UMd1eac) |
| Dense Network Expansion for Class Incremental Learning | [![GitHub](https://img.shields.io/github/stars/BinahHu/DNE?style=flat)](https://github.com/BinahHu/DNE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hu_Dense_Network_Expansion_for_Class_Incremental_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12696-b31b1b.svg)](http://arxiv.org/abs/2303.12696) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7BtHR7vEBWU) |
| Batch Model Consolidation: A Multi-Task Model Consolidation Framework | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://iordanis.me/stream_benchmark/) <br /> [![GitHub](https://img.shields.io/github/stars/fostiropoulos/stream_benchmark?style=flat)](https://github.com/fostiropoulos/stream_benchmark) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Fostiropoulos_Batch_Model_Consolidation_A_Multi-Task_Model_Consolidation_Framework_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.16484-b31b1b.svg)](https://arxiv.org/abs/2305.16484) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=v5uw1XpCum4) |
| DiGeo: Discriminative Geometry-Aware Learning for Generalized Few-Shot Object Detection | [![GitHub](https://img.shields.io/github/stars/Phoenix-V/DiGeo?style=flat)](https://github.com/Phoenix-V/DiGeo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ma_DiGeo_Discriminative_Geometry-Aware_Learning_for_Generalized_Few-Shot_Object_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09674-b31b1b.svg)](http://arxiv.org/abs/2303.09674) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pcZYjsL52So) |
| Supervised Masked Knowledge Distillation for Few-Shot Transformers | [![GitHub](https://img.shields.io/github/stars/HL-hanlin/SMKD?style=flat)](https://github.com/HL-hanlin/SMKD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_Supervised_Masked_Knowledge_Distillation_for_Few-Shot_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15466-b31b1b.svg)](http://arxiv.org/abs/2303.15466) | :heavy_minus_sign: |
| ALOFT: A Lightweight MLP-Like Architecture with Dynamic Low-Frequency Transform for Domain Generalization |  |  |  |
| ZegCLIP: Towards Adapting CLIP for Zero-Shot Semantic Segmentation |  |  |  |
| DiGA: <i>Di</i>stil to <i>G</i>eneralize and then <i>A</i>dapt for Domain Adaptive Semantic Segmentation |  |  |  |
| Adjustment and Alignment for Unbiased Open Set Domain Adaptation |  |  |  |
| Adapting Shortcut with Normalizing Flow: An Efficient Tuning Framework for Visual Recognition |  |  |  |
| CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning |  |  |  |
| ConStruct-VL: Data-Free Continual Structured VL Concepts Learning |  |  |  |
| Generalizing Dataset Distillation via Deep Generative Prior |  |  |  |
| Few-Shot Learning with Visual Distribution Calibration and Cross-Modal Distribution Alignment |  |  |  |
| Multi-Centroid Task Descriptor for Dynamic Class Incremental Inference |  |  |  |
| DAA: A Delta Age AdaIN Operation for Age Estimation via Binary Code Transformer |  |  |  |
| Bilateral Memory Consolidation for Continual Learning |  |  |  |
| Texts as Images in Prompt Tuning for Multi-Label Image Recognition |  |  |  |
| Learning Transformations To Reduce the Geometric Shift in Object Detection |  |  |  |
| CLIP the Gap: A Single Domain Generalization Approach for Object Detection |  |  |  |
| Transfer Knowledge from Head to Tail: Uncertainty Calibration under Long-tailed Distribution |  |  |  |
| Bi-Directional Distribution Alignment for Transductive Zero-Shot Learning |  |  |  |
| DARE-GRAM: Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices |  |  |  |
| LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models |  |  |  |
| Open-Set Likelihood Maximization for Few-Shot Learning |  |  |  |
| WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation |  |  |  |
| Federated Domain Generalization with Generalization Adjustment |  |  |  |
| ProtoCon: Pseudo-Label Refinement via Online Clustering and <i>Proto</i>typical <i>Con</i>sistency for Efficient Semi-Supervised Learning |  |  |  |
| DA-DETR: Domain Adaptive Detection Transformer with Information Fusion |  |  |  |
| Harmonious Teacher for Cross-Domain Object Detection |  |  |  |
| AutoLabel: CLIP-based Framework for Open-Set Video Domain Adaptation |  |  |  |
| Task Difficulty Aware Parameter Allocation & Regularization for Lifelong Learning |  |  |  |
| Revisiting Prototypical Network for Cross Domain Few-Shot Learning |  |  |  |
| Federated Incremental Semantic Segmentation |  |  |  |
| Semantic Prompt for Few-Shot Image Recognition |  |  |  |
| Rethinking Gradient Projection Continual Learning: Stability/Plasticity Feature Space Decoupling |  |  |  |
| No One Left Behind: Improving the Worst Categories in Long-Tailed Learning |  |  |  |
| Meta Omnium: A Benchmark for General-Purpose Learning-to-Learn |  |  |  |
| Transductive Few-Shot Learning with Prototype-based Label Propagation by Iterative Graph Refinement |  |  |  |
| COT: Unsupervised Domain Adaptation with Clustering and Optimal Transport |  |  |  |
| Semi-Supervised Domain Adaptation with Source Label Adaptation |  |  |  |
| MetaMix: Towards Corruption-Robust Continual Learning with Temporally Self-Adaptive Data Transformation |  |  |  |
| Visual-Language Prompt Tuning with Knowledge-guided Context Optimization |  |  |  |
| Modeling Inter-Class and Intra-Class Constraints in Novel Class Discovery |  |  |  |
| Real-Time Evaluation in Online Continual Learning: A New Hope |  |  |  |
| Partial Network Cloning |  |  |  |
| Rebalancing Batch Normalization for Exemplar-based Class-Incremental Learning |  |  |  |
| EcoTTA: Memory-Efficient Continual Test-Time Adaptation via Self-Distilled Regularization |  |  |  |
| Feature Alignment and Uniformity for Test Time Adaptation |  |  |  |
| Bootstrap Your Own Prior: Towards Distribution-Agnostic Novel Class Discovery |  |  |  |
| Towards Realistic Long-Tailed Semi-Supervised Learning: Consistency Is All You Need |  |  |  |
| Balanced Product of Calibrated Experts for Long-Tailed Recognition |  |  |  |
| Unsupervised Continual Semantic Adaptation through Neural Rendering |  |  |  |
| Computationally Budgeted Continual Learning: What Does Matter? |  |  |  |
| AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning |  |  |  |
| Ground-Truth Free Meta-Learning for Deep Compressive Sampling |  |  |  |
| Multi-Level Logit Distillation |  |  |  |
| StyleAdv: Meta Style Adversarial Training for Cross-Domain Few-Shot Learning |  |  |  |
| MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation |  |  |  |
| On the Stability-Plasticity Dilemma of Class-Incremental Learning |  |  |  |
| TeSLA: Test-Time Self-Learning With Automatic Adversarial Augmentation |  |  |  |
| MHPL: Minimum Happy Points Learning for Active Source Free Domain Adaptation |  |  |  |
| CIGAR: Cross-Modality Graph Reasoning for Domain Adaptive Object Detection |  |  |  |
| Adaptive Plasticity Improvement for Continual Learning |  |  |  |
| Achieving a Better Stability-Plasticity Trade-Off via Auxiliary Networks in Continual Learning |  |  |  |
| Few-Shot Geometry-Aware Keypoint Localization |  |  |  |
| Spatio-Temporal Pixel-Level Contrastive Learning-based Source-Free Domain Adaptation for Video Semantic Segmentation |  |  |  |
| Both Style and Distortion Matter: Dual-Path Unsupervised Domain Adaptation for Panoramic Semantic Segmentation |  |  |  |
| Bi-Level Meta-Learning for Few-Shot Domain Generalization |  |  |  |
| Few-Shot Referring Relationships in Videos |  |  |  |
| Exploring Data Geometry for Continual Learning |  |  |  |
| Masked Images Are Counterfactual Samples for Robust Fine-Tuning |  |  |  |
| DKT: Diverse Knowledge Transfer Transformer for Class Incremental Learning |  |  |  |
| CoMFormer: Continual Learning in Semantic and Panoptic Segmentation |  |  |  |
| Global and Local Mixture Consistency Cumulative Learning for Long-tailed Visual Recognitions |  |  |  |
| Class Attention Transfer based Knowledge Distillation |  |  |  |
| Hard Sample Matters a Lot in Zero-Shot Quantization |  |  |  |
| Back to the Source: Diffusion-Driven Adaptation to Test-Time Corruption |  |  |  |
| SuperDisco: Super-Class Discovery Improves Visual Recognition for the Long-Tail |  |  |  |
| Architecture, Dataset and Model-Scale Agnostic Data-Free Meta-Learning |  |  |  |
| Preserving Linear Separability in Continual Learning by Backward Feature Projection|  |  |  |
| Upcycling Models under Domain and Category Shift |  |  |  |
| Class-Incremental Exemplar Compression for Class-Incremental Learning |  |  |  |
| Learning Conditional Attributes for Compositional Zero-Shot Learning |  |  |  |
| BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning |  |  |  |
| NoisyTwins: Class-Consistent and Diverse Image Generation through StyleGANs |  |  |  |
| Semi-Supervised Learning Made Simple with Self-Supervised Clustering |  |  |  |
| Guiding Pseudo-Labels with Uncertainty Estimation for Source-Free Unsupervised Domain Adaptation |  |  |  |
| PCR: Proxy-based Contrastive Replay for Online Class-Incremental Continual Learning |  |  |  |
| Modality-Agnostic Debiasing for Single Domain Generalization |  |  |  |
| Robust Mean Teacher for Continual and Gradual Test-Time Adaptation |  |  |  |
| Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation |  |  |  |
| Hubs and Hyperspheres: Reducing Hubness and Improving Transductive Few-Shot Learning with Hyperspherical Embeddings |  |  |  |
| Robust Test-Time Adaptation in Dynamic Scenarios |  |  |  |
| Source-Free Video Domain Adaptation with Spatial-Temporal-Historical Consistency Learning |  |  |  |
| Heterogeneous Continual Learning |  |  |  |
| Continual Detection Transformer for Incremental Object Detection |  |  |  |
| NIFF: Alleviating Forgetting in Generalized Few-Shot Object Detection via Neural Instance Feature Forging |  |  |  |
| ViewNet: A Novel Projection-based Backbone with View Pooling for Few-Shot Point Cloud Classification |  |  |  |
| C-SFDA: A Curriculum Learning Aided Self-Training Framework for Efficient Source Free Domain Adaptation |  |  |  |
| Train/Test-Time Adaptation with Retrieval |  |  |  |
| Dealing with Cross-Task Class Discrimination in Online Continual Learning |  |  |  |
| Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning |  |  |  |
| Decoupling Learning and Remembering: A Bilevel Memory Framework with Knowledge Projection for Task-Incremental Learning |  |  |  |
| Neuro-Modulated Hebbian Learning for Fully Test-Time Adaptation |  |  |  |
| TIPI: Test Time Adaptation with Transformation Invariance |  |  |  |
| Meta-Learning with a Geometry-Adaptive Preconditioner |  |  |  |
| Meta-Tuning Loss Functions and Data Augmentation for Few-Shot Object Detection |  |  |  |
| A Probabilistic Framework for Lifelong Test-Time Adaptation |  |  |  |
| Few-Shot Class-Incremental Learning via Class-Aware Bilateral Distillation |  |  |  |
| CafeBoost: Causal Feature Boost to Eliminate Task-Induced Bias for Class Incremental Learning |  |  |  |
| A Strong Baseline for Generalized Few-Shot Semantic Segmentation |  |  |  |
| Towards Better Stability and Adaptability: Improve Online Self-Training for Model Adaptation in Semantic Segmentation |  |  |  |
| A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation |  |  |  |
| Cross-Image-Attention for Conditional Embeddings in Deep Metric Learning |  |  |  |
| Principles of Forgetting in Domain-Incremental Semantic Segmentation in Adverse Weather Conditions |  |  |  |
| Data-Free Knowledge Distillation via Feature Exchange and Activation Region Constraint |  |  |  |
| (ML)<sup>2</sup>P-Encoder: On Exploration of Channel-Class Correlation for Multi-Label Zero-Shot Learning |  |  |  |
| Finetune Like You Pretrain: Improved Finetuning of Zero-Shot Vision Models |  |  |  |
| Simulated Annealing in Early Layers Leads to Better Generalization |  |  |  |
| A Data-based Perspective on Transfer Learning |  |  |  |
| Learning Expressive Prompting with Residuals for Vision Transformers |  |  |  |
| Boosting Transductive Few-Shot Fine-Tuning with Margin-based Uncertainty Weighting and Probability Regularization |  |  |  |
| Improving Generalization with Domain Convex Game |  |  |  |
| Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game Perspective |  |  |  |
| Guided Recommendation for Model Fine-Tuning |  |  |  |
| Improving Generalization of Meta-Learning with Inverted Regularization at Inner-Level |  |  |  |
| Hint-Aug: Drawing Hints from Foundation Vision Transformers towards Boosted Few-Shot Parameter-Efficient Tuning |  |  |  |
