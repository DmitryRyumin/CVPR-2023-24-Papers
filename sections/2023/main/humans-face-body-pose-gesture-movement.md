# CVPR-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/2023/main/image-and-video-synthesis-and-generation.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/2023/main/transfer-meta-low-shot-continual-or-long-tail-learning.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Humans: Face, Body, Pose, Gesture, Movement

![Section Papers](https://img.shields.io/badge/Section%20Papers-166-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-123-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-114-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-139-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Micron-BERT: BERT-Based Facial Micro-Expression Recognition | [![GitHub](https://img.shields.io/github/stars/uark-cviu/Micron-BERT?style=flat)](https://github.com/uark-cviu/Micron-BERT)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Nguyen_Micron-BERT_BERT-Based_Facial_Micro-Expression_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03195-b31b1b.svg)](https://arxiv.org/abs/2304.03195) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vHN6EYtCo50) |
| NIKI: Neural Inverse Kinematics With Invertible Neural Networks for 3D Human Pose and Shape Estimation | [![GitHub](https://img.shields.io/github/stars/Jeff-sjtu/NIKI?style=flat)](https://github.com/Jeff-sjtu/NIKI)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_NIKI_Neural_Inverse_Kinematics_With_Invertible_Neural_Networks_for_3D_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.08590-b31b1b.svg)](http://arxiv.org/abs/2305.08590) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2tPAYLtat4I) |
| A Characteristic Function-Based Method for Bottom-Up Human Pose Estimation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Qu_A_Characteristic_Function-Based_Method_for_Bottom-Up_Human_Pose_Estimation_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Executing Your Commands via Motion Diffusion in Latent Space | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://chenxin.tech/mld/) <br /> [![GitHub](https://img.shields.io/github/stars/ChenFengYe/motion-latent-diffusion?style=flat)](https://github.com/ChenFengYe/motion-latent-diffusion)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Executing_Your_Commands_via_Motion_Diffusion_in_Latent_Space_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04048-b31b1b.svg)](http://arxiv.org/abs/2212.04048)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Z-nXPfi3p8U) |
| MSINet: Twins Contrastive Search of Multi-Scale Interaction for Object ReID | [![GitHub](https://img.shields.io/github/stars/vimar-gu/MSINet?style=flat)](https://github.com/vimar-gu/MSINet)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gu_MSINet_Twins_Contrastive_Search_of_Multi-Scale_Interaction_for_Object_ReID_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07065-b31b1b.svg)](http://arxiv.org/abs/2303.07065) | :heavy_minus_sign: |
| Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation | [![GitHub](https://img.shields.io/github/stars/Advocate99/DiffGesture?style=flat)](https://github.com/Advocate99/DiffGesture) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhu_Taming_Diffusion_Models_for_Audio-Driven_Co-Speech_Gesture_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09119-b31b1b.svg)](http://arxiv.org/abs/2303.09119) | :heavy_minus_sign: |
| Global-to-Local Modeling for Video-Based 3D Human Pose and Shape Estimation | [![GitHub](https://img.shields.io/github/stars/sxl142/GLoT?style=flat)](https://github.com/sxl142/GLoT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shen_Global-to-Local_Modeling_for_Video-Based_3D_Human_Pose_and_Shape_Estimation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14747-b31b1b.svg)](http://arxiv.org/abs/2303.14747) | :heavy_minus_sign: |
| Dynamic Aggregated Network for Gait Recognition | [![GitHub](https://img.shields.io/github/stars/XKMar/FastGait?style=flat)](https://github.com/XKMar/FastGait) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ma_Dynamic_Aggregated_Network_for_Gait_Recognition_CVPR_2023_paper.pdf) |  [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PiYIo_DZ3I8) |
| Object Pop-Up: Can We Infer 3D Objects and Their Poses From Human Interactions Alone? |  [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://virtualhumans.mpi-inf.mpg.de/object_popup/) <br /> [![GitHub](https://img.shields.io/github/stars/ptrvilya/object-popup?style=flat)](https://github.com/ptrvilya/object-popup) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Petrov_Object_Pop-Up_Can_We_Infer_3D_Objects_and_Their_Poses_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.00777-b31b1b.svg)](https://arxiv.org/abs/2306.00777) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=buEz9ES-R_o) |
| Unsupervised Sampling Promoting for Stochastic Human Trajectory Prediction | [![GitHub](https://img.shields.io/github/stars/viewsetting/Unsupervised_sampling_promoting?style=flat)](https://github.com/viewsetting/Unsupervised_sampling_promoting) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Unsupervised_Sampling_Promoting_for_Stochastic_Human_Trajectory_Prediction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04298-b31b1b.svg)](http://arxiv.org/abs/2304.04298) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hGb2f0zzXs8) |
| ECON: Explicit Clothed Humans Optimized via Normal Integration <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]()  | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://xiuyuliang.cn/econ) <br /> [![GitHub](https://img.shields.io/github/stars/YuliangXiu/ECON?style=flat)](https://github.com/YuliangXiu/ECON)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xiu_ECON_Explicit_Clothed_Humans_Optimized_via_Normal_Integration_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.07422-b31b1b.svg)](http://arxiv.org/abs/2212.07422) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5PEd_p90kS0) |
| Neuron Structure Modeling for Generalizable Remote Physiological Measurement | [![GitHub](https://img.shields.io/github/stars/LuPaoPao/NEST?style=flat)](https://github.com/LuPaoPao/NEST)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lu_Neuron_Structure_Modeling_for_Generalizable_Remote_Physiological_Measurement_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05955-b31b1b.svg)](http://arxiv.org/abs/2303.05955) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=f_4vtE8nTMg) |
| Continuous Sign Language Recognition With Correlation Network | [![GitHub](https://img.shields.io/github/stars/hulianyuyy/CorrNet?style=flat)](https://github.com/hulianyuyy/CorrNet)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hu_Continuous_Sign_Language_Recognition_With_Correlation_Network_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03202-b31b1b.svg)](http://arxiv.org/abs/2303.03202)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kNdEvkPgWhk) |
| Parametric Implicit Face Representation for Audio-Driven Facial Reenactment | [![GitHub](https://img.shields.io/github/stars/JosephPai/Awesome-Talking-Face?style=flat)](https://github.com/JosephPai/Awesome-Talking-Face)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Parametric_Implicit_Face_Representation_for_Audio-Driven_Facial_Reenactment_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.07579-b31b1b.svg)](https://arxiv.org/abs/2306.07579)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OASrT7qm7hc) |
| CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model | [![GitHub](https://img.shields.io/github/stars/dk-liang/CrowdCLIP?style=flat)](https://github.com/dk-liang/CrowdCLIP)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liang_CrowdCLIP_Unsupervised_Crowd_Counting_via_Vision-Language_Model_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04231-b31b1b.svg)](http://arxiv.org/abs/2304.04231) | :heavy_minus_sign: |
| PoseExaminer: Automated Testing of Out-of-Distribution Robustness in Human Pose and Shape Estimation | [![GitHub](https://img.shields.io/github/stars/qihao067/PoseExaminer?style=flat)](https://github.com/qihao067/PoseExaminer)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_PoseExaminer_Automated_Testing_of_Out-of-Distribution_Robustness_in_Human_Pose_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07337-b31b1b.svg)](http://arxiv.org/abs/2303.07337) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PmQ4_VPGLEY) |
| 3D Human Mesh Estimation From Virtual Markers | [![GitHub](https://img.shields.io/github/stars/ShirleyMaxx/VirtualMarker?style=flat)](https://github.com/ShirleyMaxx/VirtualMarker)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ma_3D_Human_Mesh_Estimation_From_Virtual_Markers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11726-b31b1b.svg)](http://arxiv.org/abs/2303.11726)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=w-a0QQi4zjM) |
| 3D Human Pose Estimation via Intuitive Physics | [![GitHub](https://img.shields.io/github/stars/sha2nkt/ipman-r?style=flat)](https://github.com/sha2nkt/ipman-r)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tripathi_3D_Human_Pose_Estimation_via_Intuitive_Physics_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.18246-b31b1b.svg)](https://arxiv.org/abs/2303.18246)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Dufvp_O0ziU) |
| ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation | [![GitHub](https://img.shields.io/github/stars/zc-alexfan/arctic?style=flat)](https://github.com/zc-alexfan/arctic)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Fan_ARCTIC_A_Dataset_for_Dexterous_Bimanual_Hand-Object_Manipulation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2204.13662-b31b1b.svg)](http://arxiv.org/abs/2204.13662)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bvMm8gfFbZ8) |
| Generating Holistic 3D Human Motion From Speech | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://talkshow.is.tue.mpg.de/) <br /> [![GitHub](https://img.shields.io/github/stars/yhw-yhw/TalkSHOW?style=flat)](https://github.com/yhw-yhw/TalkSHOW)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yi_Generating_Holistic_3D_Human_Motion_From_Speech_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04420-b31b1b.svg)](http://arxiv.org/abs/2212.04420) | :heavy_minus_sign: |
| HARP: Personalized Hand Reconstruction From a Monocular RGB Video | [![GitHub](https://img.shields.io/github/stars/korrawe/harp?style=flat)](https://github.com/korrawe/harp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Karunratanakul_HARP_Personalized_Hand_Reconstruction_From_a_Monocular_RGB_Video_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09530-b31b1b.svg)](http://arxiv.org/abs/2212.09530) | :heavy_minus_sign: |
| Learning Locally Editable Virtual Humans | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://custom-humans.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/custom-humans/editable-humans?style=flat)](https://github.com/custom-humans/editable-humans) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ho_Learning_Locally_Editable_Virtual_Humans_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.00121-b31b1b.svg)](http://arxiv.org/abs/2305.00121) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=aT8ql5hB3ZM) |
| Reconstructing Signing Avatars From Video Using Linguistic Priors | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sgnify.is.tue.mpg.de/) <br /> [![GitHub](https://img.shields.io/github/stars/MPForte/SGNify?style=flat)](https://github.com/MPForte/SGNify) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Forte_Reconstructing_Signing_Avatars_From_Video_Using_Linguistic_Priors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10482-b31b1b.svg)](http://arxiv.org/abs/2304.10482) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FLzkGhlsAHw) |
| DrapeNet: Garment Generation and Self-Supervised Draping | [![GitHub](https://img.shields.io/github/stars/liren2515/DrapeNet?style=flat)](https://github.com/liren2515/DrapeNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/De_Luigi_DrapeNet_Garment_Generation_and_Self-Supervised_Draping_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11277-b31b1b.svg)](http://arxiv.org/abs/2211.11277) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vmBITSFNHD0) |
| X-Avatar: Expressive Human Avatars | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://skype-line.github.io/projects/X-Avatar/) <br /> [![GitHub](https://img.shields.io/github/stars/Skype-line/X-Avatar?style=flat)](https://github.com/Skype-line/X-Avatar) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shen_X-Avatar_Expressive_Human_Avatars_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.04805-b31b1b.svg)](https://arxiv.org/abs/2303.04805) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JniKF88aPcs) |
| Hi4D: 4D Instance Segmentation of Close Human Interaction | [![GitHub](https://img.shields.io/github/stars/yifeiyin04/Hi4D?style=flat)](https://github.com/yifeiyin04/Hi4D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yin_Hi4D_4D_Instance_Segmentation_of_Close_Human_Interaction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15380-b31b1b.svg)](http://arxiv.org/abs/2303.15380) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DrvL2XkW7rw) |
| Vid2Avatar: 3D Avatar Reconstruction From Videos in the Wild via Self-Supervised Scene Decomposition | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://moygcc.github.io/vid2avatar/) <br /> [![GitHub](https://img.shields.io/github/stars/MoyGcc/vid2avatar?style=flat)](https://github.com/MoyGcc/vid2avatar) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Guo_Vid2Avatar_3D_Avatar_Reconstruction_From_Videos_in_the_Wild_via_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.11566-b31b1b.svg)](http://arxiv.org/abs/2302.11566) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LXsCGQigteE) |
| CloSET: Modeling Clothed Humans on Continuous Surface With Explicit Template Decomposition | [![GitHub](https://img.shields.io/github/stars/HongwenZhang/CloSET?style=flat)](https://github.com/HongwenZhang/CloSET) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_CloSET_Modeling_Clothed_Humans_on_Continuous_Surface_With_Explicit_Template_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03167-b31b1b.svg)](http://arxiv.org/abs/2304.03167) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=T6h9SBEBS6s) |
| Graphics Capsule: Learning Hierarchical 3D Face Representations From 2D Images | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_Graphics_Capsule_Learning_Hierarchical_3D_Face_Representations_From_2D_Images_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10896-b31b1b.svg)](http://arxiv.org/abs/2303.10896) | :heavy_minus_sign: |
| Rethinking the Learning Paradigm for Dynamic Facial Expression Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Rethinking_the_Learning_Paradigm_for_Dynamic_Facial_Expression_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.15402-b31b1b.svg)](https://arxiv.org/abs/2209.15402) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tANjlVTpsz8) |
| HandNeRF: Neural Radiance Fields for Animatable Interacting Hands | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Guo_HandNeRF_Neural_Radiance_Fields_for_Animatable_Interacting_Hands_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13825-b31b1b.svg)](http://arxiv.org/abs/2303.13825) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=1SMnEyGia9Y) |
| Relightable Neural Human Assets From Multi-View Gradient Illuminations | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://miaoing.github.io/RNHA/)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhou_Relightable_Neural_Human_Assets_From_Multi-View_Gradient_Illuminations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.07648-b31b1b.svg)](http://arxiv.org/abs/2212.07648)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EFcgBDbxJHA) |
| Being Comes From Not-Being: Open-Vocabulary Text-to-Motion Generation With Wordless Training <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/junfanlin/oohmg?style=flat)](junfanlin/oohmg) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_Being_Comes_From_Not-Being_Open-Vocabulary_Text-to-Motion_Generation_With_Wordless_Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.15929-b31b1b.svg)](https://arxiv.org/abs/2210.15929) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=v8MLXgX3R7Y) |
| DeFeeNet: Consecutive 3D Human Motion Prediction With Deviation Feedback | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sun_DeFeeNet_Consecutive_3D_Human_Motion_Prediction_With_Deviation_Feedback_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04496-b31b1b.svg)](http://arxiv.org/abs/2304.04496) | :heavy_minus_sign: |
| BioNet: A Biologically-Inspired Network for Face Recognition | [![GitHub](https://img.shields.io/github/stars/bdevans/BioNet?style=flat)](https://github.com/bdevans/BioNet)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_BioNet_A_Biologically-Inspired_Network_for_Face_Recognition_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_M4qm7rYumo) |
| Boosting Detection in Crowd Analysis via Underutilized Output Features | [![GitHub](https://img.shields.io/github/stars/wskingdom/Crowd-Hat?style=flat)](https://github.com/wskingdom/Crowd-Hat)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Boosting_Detection_in_Crowd_Analysis_via_Underutilized_Output_Features_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.16187-b31b1b.svg)](http://arxiv.org/abs/2308.16187) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6xl2rCVS52I) |
| Learning Analytical Posterior Probability for Human Mesh Recovery | [![GitHub](https://img.shields.io/github/stars/NetEase-GameAI/ProPose?style=flat)](https://github.com/NetEase-GameAI/ProPose)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Fang_Learning_Analytical_Posterior_Probability_for_Human_Mesh_Recovery_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=uS9R82ldkqc) |
| Listening Human Behavior: 3D Human Pose Estimation With Acoustic Signals | [![GitHub](https://img.shields.io/github/stars/YutoShibata07/AcousticPose_Public?style=flat)](https://github.com/YutoShibata07/AcousticPose_Public)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shibata_Listening_Human_Behavior_3D_Human_Pose_Estimation_With_Acoustic_Signals_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=IDvrSUautCI) |
| Detecting and Grounding Multi-Modal Media Manipulation | [![GitHub](https://img.shields.io/github/stars/rshaojimmy/MultiModal-DeepFake?style=flat)](https://github.com/rshaojimmy/MultiModal-DeepFake)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shao_Detecting_and_Grounding_Multi-Modal_Media_Manipulation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02556-b31b1b.svg)](http://arxiv.org/abs/2304.02556) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EortO0cqnGE) |
| RelightableHands: Efficient Neural Relighting of Articulated Hand Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sh8.io/#/relightable_hands) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Iwase_RelightableHands_Efficient_Neural_Relighting_of_Articulated_Hand_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.04866-b31b1b.svg)](http://arxiv.org/abs/2302.04866) | :heavy_minus_sign: |
| MEGANE: Morphable Eyeglass and Avatar Network | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://junxuan-li.github.io/megane/)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_MEGANE_Morphable_Eyeglass_and_Avatar_Network_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.04868-b31b1b.svg)](http://arxiv.org/abs/2302.04868)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xV3sXJN7RxM) |
| SunStage: Portrait Reconstruction and Relighting Using the Sun as a Light Stage | [![GitHub](https://img.shields.io/github/stars/adobe-research/sunstage?style=flat)](https://github.com/adobe-research/sunstage)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_SunStage_Portrait_Reconstruction_and_Relighting_Using_the_Sun_as_a_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2204.03648-b31b1b.svg)](http://arxiv.org/abs/2204.03648) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bc0RTQs5K0E) |
| TryOnDiffusion: A Tale of Two UNets | [![GitHub](https://img.shields.io/github/stars/tryonlabs/tryondiffusion?style=flat)](https://github.com/tryonlabs/tryondiffusion)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhu_TryOnDiffusion_A_Tale_of_Two_UNets_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.08276-b31b1b.svg)](https://arxiv.org/abs/2306.08276)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nMwBVLjRdcc) |
| Semi-Supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhao_Semi-Supervised_Hand_Appearance_Recovery_via_Structure_Disentanglement_and_Dual_Adversarial_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06380-b31b1b.svg)](http://arxiv.org/abs/2303.06380) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PSdDKl8EYS8) |
| POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery | [![GitHub](https://img.shields.io/github/stars/zczcwh/POTTER?style=flat)](https://github.com/zczcwh/POTTER)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zheng_POTTER_Pooling_Attention_Transformer_for_Efficient_Human_Mesh_Recovery_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13357-b31b1b.svg)](http://arxiv.org/abs/2303.13357) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-O4V-yqJmms) |
| Scene-Aware Egocentric 3D Human Pose Estimation | [![GitHub](https://img.shields.io/github/stars/jianwang-mpi/SceneEgo?style=flat)](https://github.com/jianwang-mpi/SceneEgo)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Scene-Aware_Egocentric_3D_Human_Pose_Estimation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.11684-b31b1b.svg)](http://arxiv.org/abs/2212.11684)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bImUBtuZeY8) |
| PSVT: End-to-End Multi-Person 3D Pose and Shape Estimation With Progressive Video Transformers| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Qiu_PSVT_End-to-End_Multi-Person_3D_Pose_and_Shape_Estimation_With_Progressive_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09187-b31b1b.svg)](http://arxiv.org/abs/2303.09187) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KoOKD1FO_tY) |
| Trajectory-Aware Body Interaction Transformer for Multi-Person Pose Forecasting | [![GitHub](https://img.shields.io/github/stars/xiaogangpeng/TBIFormer?style=flat)](https://github.com/xiaogangpeng/TBIFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Peng_Trajectory-Aware_Body_Interaction_Transformer_for_Multi-Person_Pose_Forecasting_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05095-b31b1b.svg)](http://arxiv.org/abs/2303.05095) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=iyf1kzpRiHs) |
| A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation From a Single RGB Image | [![GitHub](https://img.shields.io/github/stars/ChanglongJiangGit/A2J-Transformer?style=flat)](https://github.com/ChanglongJiangGit/A2J-Transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jiang_A2J-Transformer_Anchor-to-Joint_Transformer_Network_for_3D_Interacting_Hand_Pose_Estimation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03635-b31b1b.svg)](https://arxiv.org/abs/2304.03635) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Q40E1ASRXaQ) |
| TRACE: 5D Temporal Regression of Avatars With Dynamic Cameras in 3D Environments | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://www.yusun.work/TRACE/TRACE.html) <br /> [![GitHub](https://img.shields.io/github/stars/Arthur151/ROMP?style=flat)](https://github.com/Arthur151/ROMP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sun_TRACE_5D_Temporal_Regression_of_Avatars_With_Dynamic_Cameras_in_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.02850-b31b1b.svg)](https://arxiv.org/abs/2306.02850) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=l8aLHDXWQRw) |
| Skinned Motion Retargeting With Residual Perception of Motion Semantics & Geometry | [![GitHub](https://img.shields.io/github/stars/Kebii/R2ET?style=flat)](https://github.com/Kebii/R2ET) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Skinned_Motion_Retargeting_With_Residual_Perception_of_Motion_Semantics__CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08658-b31b1b.svg)](http://arxiv.org/abs/2303.08658) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TzyRLrdNSg8) |
| Generating Human Motion From Textual Descriptions With Discrete Representations | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mael-zys.github.io/T2M-GPT/) <br /> [![GitHub](https://img.shields.io/github/stars/Mael-zys/T2M-GPT?style=flat)](https://github.com/Mael-zys/T2M-GPT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Generating_Human_Motion_From_Textual_Descriptions_With_Discrete_Representations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.06052-b31b1b.svg)](http://arxiv.org/abs/2301.06052) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0PR-cq1x-DU) |
| Learning Human Mesh Recovery in 3D Scenes | [![GitHub](https://img.shields.io/github/stars/zju3dv/SA-HMR?style=flat)](https://github.com/zju3dv/SA-HMR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shen_Learning_Human_Mesh_Recovery_in_3D_Scenes_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.03847-b31b1b.svg)](https://arxiv.org/abs/2306.03847) | :heavy_minus_sign: |
| AVFace: Towards Detailed Audio-Visual 4D Face Reconstruction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://aggelinacha.github.io/AVFace/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chatziagapi_AVFace_Towards_Detailed_Audio-Visual_4D_Face_Reconstruction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.13115-b31b1b.svg)](http://arxiv.org/abs/2304.13115) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-nt2FPFetLg) |
| 3D-Aware Face Swapping | [![GitHub](https://img.shields.io/github/stars/lyx0208/3dSwap?style=flat)](https://github.com/lyx0208/3dSwap) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_3D-Aware_Face_Swapping_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=njeNGZRYVyE) |
| Neural Residual Radiance Fields for Streamably Free-Viewpoint Videos | [![GitHub](https://img.shields.io/github/stars/aoliao12138/ReRF_Dataset?style=flat)](https://github.com/aoliao12138/ReRF_Dataset) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Neural_Residual_Radiance_Fields_for_Streamably_Free-Viewpoint_Videos_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04452-b31b1b.svg)](http://arxiv.org/abs/2304.04452) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lts9AeBjzuU) |
| GFPose: Learning 3D Human Pose Prior With Gradient Fields | [![GitHub](https://img.shields.io/github/stars/Embracing/GFPose?style=flat)](https://github.com/Embracing/GFPose)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ci_GFPose_Learning_3D_Human_Pose_Prior_With_Gradient_Fields_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08641-b31b1b.svg)](http://arxiv.org/abs/2212.08641) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=edAvCKr_bOE) |
| Rethinking Feature-Based Knowledge Distillation for Face Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Rethinking_Feature-Based_Knowledge_Distillation_for_Face_Recognition_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| One-Stage 3D Whole-Body Mesh Recovery With Component Aware Transformer | [![GitHub](https://img.shields.io/github/stars/IDEA-Research/OSX?style=flat)](https://github.com/IDEA-Research/OSX)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_One-Stage_3D_Whole-Body_Mesh_Recovery_With_Component_Aware_Transformer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16160-b31b1b.svg)](http://arxiv.org/abs/2303.16160)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=s0cG3OVXQUo) |
| Towards Stable Human Pose Estimation via Cross-View Fusion and Foot Stabilization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhuo_Towards_Stable_Human_Pose_Estimation_via_Cross-View_Fusion_and_Foot_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CKuAENukmc8) |
| Ego-Body Pose Estimation via Ego-Head Pose Estimation <br/> [![CVPR - Award](https://img.shields.io/badge/CVPR-Award-294A7C)]() | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lijiaman.github.io/projects/egoego/) <br /> [![GitHub](https://img.shields.io/github/stars/lijiaman/egoego_release?style=flat)](https://github.com/lijiaman/egoego_release) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Ego-Body_Pose_Estimation_via_Ego-Head_Pose_Estimation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04636-b31b1b.svg)](http://arxiv.org/abs/2212.04636) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Dg66DY2sGus) |
| TOPLight: Lightweight Neural Networks With Task-Oriented Pretraining for Visible-Infrared Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_TOPLight_Lightweight_Neural_Networks_With_Task-Oriented_Pretraining_for_Visible-Infrared_Recognition_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| StyleIPSB: Identity-Preserving Semantic Basis of StyleGAN for High Fidelity Face Swapping | [![GitHub](https://img.shields.io/github/stars/a686432/StyleIPSB?style=flat)](https://github.com/a686432/StyleIPSB)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jiang_StyleIPSB_Identity-Preserving_Semantic_Basis_of_StyleGAN_for_High_Fidelity_Face_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=R7BH4Y4ca90) |
| Improving Fairness in Facial Albedo Estimation via Visual-Textual Cues <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ren_Improving_Fairness_in_Facial_Albedo_Estimation_via_Visual-Textual_Cues_CVPR_2023_paper.pdf) |  [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2XvqhwWWBsI) |
| FLEX: Full-Body Grasping Without Full-Body Grasps | [![GitHub](https://img.shields.io/github/stars/purvaten/FLEX?style=flat)](https://github.com/purvaten/FLEX)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tendulkar_FLEX_Full-Body_Grasping_Without_Full-Body_Grasps_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11903-b31b1b.svg)](https://arxiv.org/abs/2211.11903) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5Uu9L-eBnX0) |
| EDGE: Editable Dance Generation From Music | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://edge-dance.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/Stanford-TML/EDGE?style=flat)](https://github.com/Stanford-TML/EDGE)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tseng_EDGE_Editable_Dance_Generation_From_Music_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.10658-b31b1b.svg)](http://arxiv.org/abs/2211.10658)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TjOQ_EnKiG0) |
| Complete 3D Human Reconstruction From a Single Incomplete Image | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Complete_3D_Human_Reconstruction_From_a_Single_Incomplete_Image_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6lMbnZp6vNQ) |
| Zero-Shot Pose Transfer for Unrigged Stylized 3D Characters | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jiashunwang.github.io/ZPT/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Zero-Shot_Pose_Transfer_for_Unrigged_Stylized_3D_Characters_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.00200-b31b1b.svg)](http://arxiv.org/abs/2306.00200) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ynT65hp92SE) |
| Hand Avatar: Free-Pose Hand Animation and Rendering From Monocular Video | [![GitHub](https://img.shields.io/github/stars/SeanChenxy/HandAvatar?style=flat)](https://github.com/SeanChenxy/HandAvatar) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Hand_Avatar_Free-Pose_Hand_Animation_and_Rendering_From_Monocular_Video_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12782-b31b1b.svg)](http://arxiv.org/abs/2211.12782) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DQUYYry-NX8) |
| Human-Art: A Versatile Human-Centric Dataset Bridging Natural and Artificial Scenes | [![GitHub](https://img.shields.io/github/stars/IDEA-Research/HumanArt?style=flat)](https://github.com/IDEA-Research/HumanArt) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ju_Human-Art_A_Versatile_Human-Centric_Dataset_Bridging_Natural_and_Artificial_Scenes_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02760-b31b1b.svg)](https://arxiv.org/abs/2303.02760) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=djmTKVlw53E) |
| Learning Neural Proto-Face Field for Disentangled 3D Face Modeling in the Wild | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Learning_Neural_Proto-Face_Field_for_Disentangled_3D_Face_Modeling_in_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| CLAMP: Prompt-Based Contrastive Learning for Connecting Language and Animal Pose | [![GitHub](https://img.shields.io/github/stars/xuzhang1199/CLAMP?style=flat)](https://github.com/xuzhang1199/CLAMP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_CLAMP_Prompt-Based_Contrastive_Learning_for_Connecting_Language_and_Animal_Pose_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.11752-b31b1b.svg)](http://arxiv.org/abs/2206.11752) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bOIl2efyz7E) |
| Invertible Neural Skinning | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yashkant.github.io/invertible-neural-skinning/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kant_Invertible_Neural_Skinning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.09227-b31b1b.svg)](http://arxiv.org/abs/2302.09227) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=il-xVIl6JU4) |
| DiffusionRig: Learning Personalized Priors for Facial Appearance Editing | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://diffusionrig.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/adobe-research/diffusion-rig?style=flat)](https://github.com/adobe-research/diffusion-rig)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ding_DiffusionRig_Learning_Personalized_Priors_for_Facial_Appearance_Editing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06711-b31b1b.svg)](http://arxiv.org/abs/2304.06711) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6ZQbiNiJJEE) |
| Harmonious Feature Learning for Interactive Hand-Object Pose Estimation | [![GitHub](https://img.shields.io/github/stars/lzfff12/HFL-Net?style=flat)](https://github.com/lzfff12/HFL-Net) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_Harmonious_Feature_Learning_for_Interactive_Hand-Object_Pose_Estimation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CRLUkwRr08k) |
| Leapfrog Diffusion Model for Stochastic Trajectory Prediction | [![GitHub](https://img.shields.io/github/stars/MediaBrain-SJTU/LED?style=flat)](https://github.com/MediaBrain-SJTU/LED) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Mao_Leapfrog_Diffusion_Model_for_Stochastic_Trajectory_Prediction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10895-b31b1b.svg)](http://arxiv.org/abs/2303.10895) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=l1gGWVHtBjw) |
| NeuFace: Realistic 3D Neural Face Rendering From Multi-View Images | [![GitHub](https://img.shields.io/github/stars/aejion/NeuFace?style=flat)](https://github.com/aejion/NeuFace)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zheng_NeuFace_Realistic_3D_Neural_Face_Rendering_From_Multi-View_Images_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14092-b31b1b.svg)](http://arxiv.org/abs/2303.14092)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mETP-2hTBQ4) |
| DiffSwap: High-Fidelity and Controllable Face Swapping via 3D-Aware Masked Diffusion | [![GitHub](https://img.shields.io/github/stars/wl-zhao/DiffSwap?style=flat)](https://github.com/wl-zhao/DiffSwap)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhao_DiffSwap_High-Fidelity_and_Controllable_Face_Swapping_via_3D-Aware_Masked_Diffusion_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| GFIE: A Dataset and Baseline for Gaze-Following From 2D to 3D in Indoor Environments | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sites.google.com/view/gfie) <br /> [![GitHub](https://img.shields.io/github/stars/nkuhzx/GFIE?style=flat)](https://github.com/nkuhzx/GFIE)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hu_GFIE_A_Dataset_and_Baseline_for_Gaze-Following_From_2D_to_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9f4NABNrM2g) |
| Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition From Egocentric RGB Videos | [![GitHub](https://img.shields.io/github/stars/fylwen/HTT?style=flat)](https://github.com/fylwen/HTT)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wen_Hierarchical_Temporal_Transformer_for_3D_Hand_Pose_Estimation_and_Action_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.09484-b31b1b.svg)](http://arxiv.org/abs/2209.09484) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lsFjkK-wmMA) |
| Decompose More and Aggregate Better: Two Closer Looks at Frequency Representation Learning for Human Motion Prediction | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_Decompose_More_and_Aggregate_Better_Two_Closer_Looks_at_Frequency_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zNWaywq7cGI) |
| Human Pose As Compositional Tokens | [![GitHub](https://img.shields.io/github/stars/Gengzigang/PCT?style=flat)](https://github.com/Gengzigang/PCT)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Geng_Human_Pose_As_Compositional_Tokens_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11638-b31b1b.svg)](http://arxiv.org/abs/2303.11638)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=eXt_ONASk5E) |
| Normal-Guided Garment UV Prediction for Human Re-Texturing <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jafarian_Normal-Guided_Garment_UV_Prediction_for_Human_Re-Texturing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06504-b31b1b.svg)](http://arxiv.org/abs/2303.06504) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=smXAY890YJQ) |
| Dynamic Graph Learning With Content-Guided Spatial-Frequency Relation Reasoning for Deepfake Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Dynamic_Graph_Learning_With_Content-Guided_Spatial-Frequency_Relation_Reasoning_for_Deepfake_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=W8Hl3p2T50U) |
| VGFlow: Visibility Guided Flow Network for Human Reposing | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jain_VGFlow_Visibility_Guided_Flow_Network_for_Human_Reposing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.08540-b31b1b.svg)](http://arxiv.org/abs/2211.08540) | :heavy_minus_sign: |
| Mutual Information-Based Temporal Difference Learning for Human Pose Estimation in Video | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Feng_Mutual_Information-Based_Temporal_Difference_Learning_for_Human_Pose_Estimation_in_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08475-b31b1b.svg)](http://arxiv.org/abs/2303.08475)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=4VXZMo8N7Ms) |
| PREIM3D: 3D Consistent Precise Image Attribute Editing From a Single Image | [![GitHub](https://img.shields.io/github/stars/mybabyyh/Preim3D?style=flat)](https://github.com/mybabyyh/Preim3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_PREIM3D_3D_Consistent_Precise_Image_Attribute_Editing_From_a_Single_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10263-b31b1b.svg)](http://arxiv.org/abs/2304.10263) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3H8cEzIgqzE) |
| HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation | [![GitHub](https://img.shields.io/github/stars/akashsengupta1997/HuManiFlow?style=flat)](https://github.com/akashsengupta1997/HuManiFlow) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sengupta_HuManiFlow_Ancestor-Conditioned_Normalising_Flows_on_SO3_Manifolds_for_Human_Pose_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.06968-b31b1b.svg)](http://arxiv.org/abs/2305.06968) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6xDiJNzPYNI) |
| Implicit Identity Driven Deepfake Face Swapping Detection | [![GitHub](https://img.shields.io/github/stars/megvii-research/CADDM?style=flat)](https://github.com/megvii-research/CADDM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Implicit_Identity_Driven_Deepfake_Face_Swapping_Detection_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TRTMNs3XH1o) |
| Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion | [![GitHub](https://img.shields.io/github/stars/nv-tlabs/trace?style=flat)](https://github.com/nv-tlabs/trace) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Rempe_Trace_and_Pace_Controllable_Pedestrian_Animation_via_Guided_Trajectory_Diffusion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01893-b31b1b.svg)](http://arxiv.org/abs/2304.01893) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=225c52QDkzg) |
| 3D-Aware Facial Landmark Detection via Multi-View Consistent Training on Synthetic Data | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zeng_3D-Aware_Facial_Landmark_Detection_via_Multi-View_Consistent_Training_on_Synthetic_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2xgFXjU3X24) |
| SLOPER4D: A Scene-Aware Dataset for Global 4D Human Pose Estimation in Urban Environments | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](http://www.lidarhumanmotion.net/sloper4d/) <br /> [![GitHub](https://img.shields.io/github/stars/climbingdaily/SLOPER4D?style=flat)](https://github.com/climbingdaily/SLOPER4D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Dai_SLOPER4D_A_Scene-Aware_Dataset_for_Global_4D_Human_Pose_Estimation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09095-b31b1b.svg)](http://arxiv.org/abs/2303.09095) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_0TqN12e56U) |
| Zero-Shot Text-to-Parameter Translation for Game Character Auto-Creation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhao_Zero-Shot_Text-to-Parameter_Translation_for_Game_Character_Auto-Creation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.01311-b31b1b.svg)](http://arxiv.org/abs/2303.01311) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bWojwQZ47iM) |
| AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation | [![GitHub](https://img.shields.io/github/stars/facebookresearch/assemblyhands-toolkit?style=flat)](https://github.com/facebookresearch/assemblyhands-toolkit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ohkawa_AssemblyHands_Towards_Egocentric_Activity_Understanding_via_3D_Hand_Pose_Estimation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.12301-b31b1b.svg)](http://arxiv.org/abs/2304.12301) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8p6TlAxa3vk) |
| UDE: A Unified Driving Engine for Human Motion Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zixiangzhou916.github.io/UDE/) <br /> [![GitHub](https://img.shields.io/github/stars/zixiangzhou916/UDE?style=flat)](https://github.com/zixiangzhou916/UDE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhou_UDE_A_Unified_Driving_Engine_for_Human_Motion_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.16016-b31b1b.svg)](http://arxiv.org/abs/2211.16016) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=J2RngmuYrG4) |
| CodeTalker: Speech-Driven 3D Facial Animation With Discrete Motion Prior | [![GitHub](https://img.shields.io/github/stars/Doubiiu/CodeTalker?style=flat)](https://github.com/Doubiiu/CodeTalker) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xing_CodeTalker_Speech-Driven_3D_Facial_Animation_With_Discrete_Motion_Prior_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.02379-b31b1b.svg)](http://arxiv.org/abs/2301.02379) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=J2RngmuYrG4) |
| Semi-Supervised 2D Human Pose Estimation Driven by Position Inconsistency Pseudo Label Correction Module | [![GitHub](https://img.shields.io/github/stars/hlz0606/SSPCM?style=flat)](https://github.com/hlz0606/SSPCM)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Semi-Supervised_2D_Human_Pose_Estimation_Driven_by_Position_Inconsistency_Pseudo_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.04346-b31b1b.svg)](http://arxiv.org/abs/2303.04346) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=z1q-4KbGgUM) |
| Learning Personalized High Quality Volumetric Head Avatars From Monocular RGB Videos | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://augmentedperception.github.io/monoavatar/)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Bai_Learning_Personalized_High_Quality_Volumetric_Head_Avatars_From_Monocular_RGB_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01436-b31b1b.svg)](http://arxiv.org/abs/2304.01436)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jnPB-llYRyQ) |
| HOOD: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics | [![GitHub](https://img.shields.io/github/stars/Dolorousrtur/HOOD?style=flat)](https://github.com/Dolorousrtur/HOOD)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Grigorev_HOOD_Hierarchical_Graphs_for_Generalized_Modelling_of_Clothing_Dynamics_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.07242-b31b1b.svg)](http://arxiv.org/abs/2212.07242)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=cBttMDPrUYY) |
| ACR: Attention Collaboration-Based Regressor for Arbitrary Two-Hand Reconstruction | [![GitHub](https://img.shields.io/github/stars/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction?style=flat)](https://github.com/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_ACR_Attention_Collaboration-Based_Regressor_for_Arbitrary_Two-Hand_Reconstruction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05938-b31b1b.svg)](http://arxiv.org/abs/2303.05938) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wTPqojwLew0) |
| HumanBench: Towards General Human-Centric Perception With Projector Assisted Pretraining | [![GitHub](https://img.shields.io/github/stars/OpenGVLab/HumanBench?style=flat)](https://github.com/OpenGVLab/HumanBench)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tang_HumanBench_Towards_General_Human-Centric_Perception_With_Projector_Assisted_Pretraining_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05675-b31b1b.svg)](http://arxiv.org/abs/2303.05675) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6u_0jdWHigA) |
| CIMI4D: A Large Multimodal Climbing Motion Dataset Under Human-Scene Interactions | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](http://www.lidarhumanmotion.net/cimi4d/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yan_CIMI4D_A_Large_Multimodal_Climbing_Motion_Dataset_Under_Human-Scene_Interactions_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17948-b31b1b.svg)](http://arxiv.org/abs/2303.17948) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rnI6weRjHuA) |
| Human Pose Estimation in Extremely Low-Light Conditions| [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cg.postech.ac.kr/research/ExLPose/) <br /> [![GitHub](https://img.shields.io/github/stars/sohyun-l/ExLPose?style=flat)](https://github.com/sohyun-l/ExLPose) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lee_Human_Pose_Estimation_in_Extremely_Low-Light_Conditions_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15410-b31b1b.svg)](http://arxiv.org/abs/2303.15410) | :heavy_minus_sign: |
| DistilPose: Tokenized Pose Regression With Heatmap Distillation | [![GitHub](https://img.shields.io/github/stars/yshMars/DistilPose?style=flat)](https://github.com/yshMars/DistilPose)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ye_DistilPose_Tokenized_Pose_Regression_With_Heatmap_Distillation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02455-b31b1b.svg)](http://arxiv.org/abs/2303.02455) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zzMo4a3LoWk) |
| Human Body Shape Completion With Implicit Shape and Flow Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhou_Human_Body_Shape_Completion_With_Implicit_Shape_and_Flow_Learning_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=SkakwN42iNo) |
| Source-Free Adaptive Gaze Estimation by Uncertainty Reduction | [![GitHub](https://img.shields.io/github/stars/caixin1998/UnReGA?style=flat)](https://github.com/caixin1998/UnReGA)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cai_Source-Free_Adaptive_Gaze_Estimation_by_Uncertainty_Reduction_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=uEcunGrs62M) |
| Music-Driven Group Choreography | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://aioz-ai.github.io/AIOZ-GDANCE/) <br /> [![GitHub](https://img.shields.io/github/stars/aioz-ai/AIOZ-GDANCE?style=flat)](https://github.com/aioz-ai/AIOZ-GDANCE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Le_Music-Driven_Group_Choreography_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12337-b31b1b.svg)](http://arxiv.org/abs/2303.12337) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=g1WRv7NLng8) |
| Robust Model-Based Face Reconstruction Through Weakly-Supervised Outlier Segmentation | [![GitHub](https://img.shields.io/github/stars/unibas-gravis/Occlusion-Robust-MoFA?style=flat)](https://github.com/unibas-gravis/Occlusion-Robust-MoFA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Robust_Model-Based_Face_Reconstruction_Through_Weakly-Supervised_Outlier_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2106.09614-b31b1b.svg)](http://arxiv.org/abs/2106.09614) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7nKbNmupViM) |
| MARLIN: Masked Autoencoder for Facial Video Representation LearnINg | [![GitHub](https://img.shields.io/github/stars/ControlNet/MARLIN?style=flat)](https://github.com/ControlNet/MARLIN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cai_MARLIN_Masked_Autoencoder_for_Facial_Video_Representation_LearnINg_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.06627-b31b1b.svg)](http://arxiv.org/abs/2211.06627) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mHFX-33BfPw) |
| Transformer-Based Unified Recognition of Two Hands Manipulating Objects | [![GitHub](https://img.shields.io/github/stars/chohoseong/H2OTR?style=flat)](https://github.com/chohoseong/H2OTR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cho_Transformer-Based_Unified_Recognition_of_Two_Hands_Manipulating_Objects_CVPR_2023_paper.pdf) |[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](hhttps://www.youtube.com/watch?v=EfRG-W_69Xs) |
| Implicit Identity Leakage: The Stumbling Block to Improving Deepfake Detection Generalization | [![GitHub](https://img.shields.io/github/stars/megvii-research/CADDM?style=flat)](https://github.com/megvii-research/CADDM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Dong_Implicit_Identity_Leakage_The_Stumbling_Block_to_Improving_Deepfake_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.14457-b31b1b.svg)](http://arxiv.org/abs/2210.14457) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=gUYkea56MKI) |
| ScarceNet: Animal Pose Estimation With Scarce Annotations | [![GitHub](https://img.shields.io/github/stars/chaneyddtt/ScarceNet?style=flat)](https://github.com/chaneyddtt/ScarceNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_ScarceNet_Animal_Pose_Estimation_With_Scarce_Annotations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15023-b31b1b.svg)](http://arxiv.org/abs/2303.15023) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7OIW_RYlkkA) |
| FFHQ-UV: Normalized Facial UV-Texture Dataset for 3D Face Reconstruction | [![GitHub](https://img.shields.io/github/stars/csbhr/FFHQ-UV?style=flat)](https://github.com/csbhr/FFHQ-UV) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Bai_FFHQ-UV_Normalized_Facial_UV-Texture_Dataset_for_3D_Face_Reconstruction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13874-b31b1b.svg)](https://arxiv.org/abs/2211.13874) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CFQcZ8Iptqo) |
| MoDi: Unconditional Motion Synthesis From Diverse Data | [![GitHub](https://img.shields.io/github/stars/sigal-raab/MoDi?style=flat)](https://github.com/sigal-raab/MoDi) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Raab_MoDi_Unconditional_Motion_Synthesis_From_Diverse_Data_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.08010-b31b1b.svg)](http://arxiv.org/abs/2206.08010) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=O1sVzwrsNUg) |
| Feature Representation Learning With Adaptive Displacement Generation and Transformer Fusion for Micro-Expression Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhai_Feature_Representation_Learning_With_Adaptive_Displacement_Generation_and_Transformer_Fusion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04420-b31b1b.svg)](http://arxiv.org/abs/2304.04420) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=sElcBHzmPr4) |
| MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand Reconstruction | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_MeMaHand_Exploiting_Mesh-Mano_Interaction_for_Single_Image_Two-Hand_Reconstruction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15718-b31b1b.svg)](http://arxiv.org/abs/2303.15718) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=IdxiRsH_ejQ) |
| Stimulus Verification Is a Universal and Effective Sampler in Multi-Modal Human Trajectory Prediction | [![GitHub](https://img.shields.io/github/stars/ApeironY/Modular-Trajectory-Prediction-Toolkit?style=flat)](https://github.com/ApeironY/Modular-Trajectory-Prediction-Toolkit)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sun_Stimulus_Verification_Is_a_Universal_and_Effective_Sampler_in_Multi-Modal_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=SiqGKqqCmSQ) |
| TokenHPE: Learning Orientation Tokens for Efficient Head Pose Estimation via Transformers | [![GitHub](https://img.shields.io/github/stars/zc2023/TokenHPE?style=flat)](https://github.com/zc2023/TokenHPE)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_TokenHPE_Learning_Orientation_Tokens_for_Efficient_Head_Pose_Estimation_via_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dO2vb0ScCH4) |
| Handy: Towards a High Fidelity 3D Hand Shape and Appearance Model | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rolpotamias.github.io/Handy/) <br /> [![GitHub](https://img.shields.io/github/stars/rolpotamias/handy?style=flat)](https://github.com/rolpotamias/handy)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Potamias_Handy_Towards_a_High_Fidelity_3D_Hand_Shape_and_Appearance_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=F4hTHKZ8JNM) |
| CIRCLE: Capture in Rich Contextual Environments | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://stanford-tml.github.io/circle_dataset/) <br /> [![GitHub](https://img.shields.io/github/stars/Stanford-TML/circle_dataset?style=flat)](https://github.com/Stanford-TML/circle_dataset)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Araujo_CIRCLE_Capture_in_Rich_Contextual_Environments_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17912-b31b1b.svg)](https://arxiv.org/abs/2303.17912) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JvrKDysrihY) |
| Gazeformer: Scalable, Effective and Fast Prediction of Goal-Directed Human Attention | [![GitHub](https://img.shields.io/github/stars/cvlab-stonybrook/Gazeformer?style=flat)](https://github.com/cvlab-stonybrook/Gazeformer)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Mondal_Gazeformer_Scalable_Effective_and_Fast_Prediction_of_Goal-Directed_Human_Attention_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15274-b31b1b.svg)](http://arxiv.org/abs/2303.15274)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5ACbxDmvLZU) |
| Implicit Neural Head Synthesis via Controllable Local Deformation Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://imaging.cs.cmu.edu/local_deformation_fields/)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Implicit_Neural_Head_Synthesis_via_Controllable_Local_Deformation_Fields_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.11113-b31b1b.svg)](http://arxiv.org/abs/2304.11113) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Bg87Fg2VYfw) |
| Continuous Intermediate Token Learning With Implicit Motion Manifold for Keyframe Based Motion Interpolation | [![GitHub](https://img.shields.io/github/stars/MiniEval/citl?style=flat)](https://github.com/MiniEval/citl)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Mo_Continuous_Intermediate_Token_Learning_With_Implicit_Motion_Manifold_for_Keyframe_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14926-b31b1b.svg)](http://arxiv.org/abs/2303.14926) | :heavy_minus_sign: |
| JRDB-Pose: A Large-Scale Dataset for Multi-Person Pose Estimation and Tracking| [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://jrdb.erc.monash.edu/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Vendrow_JRDB-Pose_A_Large-Scale_Dataset_for_Multi-Person_Pose_Estimation_and_Tracking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.11940-b31b1b.svg)](https://arxiv.org/abs/2210.11940) | :heavy_minus_sign: |
| STAR Loss: Reducing Semantic Ambiguity in Facial Landmark Detection | [![GitHub](https://img.shields.io/github/stars/ZhenglinZhou/STAR?style=flat)](https://github.com/ZhenglinZhou/STAR)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhou_STAR_Loss_Reducing_Semantic_Ambiguity_in_Facial_Landmark_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.02763-b31b1b.svg)](https://arxiv.org/abs/2306.02763) | :heavy_minus_sign: |
| GM-NeRF: Learning Generalizable Model-Based Neural Radiance Fields From Multi-View Images | [![GitHub](https://img.shields.io/github/stars/JanaldoChen/GM-NeRF?style=flat)](https://github.com/JanaldoChen/GM-NeRF)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_GM-NeRF_Learning_Generalizable_Model-Based_Neural_Radiance_Fields_From_Multi-View_Images_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13777-b31b1b.svg)](https://arxiv.org/abs/2303.13777) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=p4vSrBd9JIs) |
| Decoupled Multimodal Distilling for Emotion Recognition <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]()  | [![GitHub](https://img.shields.io/github/stars/mdswyz/DMD?style=flat)](https://github.com/mdswyz/DMD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Decoupled_Multimodal_Distilling_for_Emotion_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13802-b31b1b.svg)](http://arxiv.org/abs/2303.13802) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=djy3UmssUlM) |
| HaLP: Hallucinating Latent Positives for Skeleton-Based Self-Supervised Learning of Actions| [![GitHub](https://img.shields.io/github/stars/anshulbshah/HaLP?style=flat)](https://github.com/anshulbshah/HaLP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shah_HaLP_Hallucinating_Latent_Positives_for_Skeleton-Based_Self-Supervised_Learning_of_Actions_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00387-b31b1b.svg)](http://arxiv.org/abs/2304.00387) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=O5pFnUmRciQ) |
| ReDirTrans: Latent-to-Latent Translation for Gaze and Head Redirection| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jin_ReDirTrans_Latent-to-Latent_Translation_for_Gaze_and_Head_Redirection_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6gum8406w0k) |
| QPGesture: Quantization-Based and Phase-Guided Motion Matching for Natural Speech-Driven Gesture Generation <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/YoungSeng/QPGesture?style=flat)](https://github.com/YoungSeng/QPGesture) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_QPGesture_Quantization-Based_and_Phase-Guided_Motion_Matching_for_Natural_Speech-Driven_Gesture_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.11094-b31b1b.svg)](http://arxiv.org/abs/2305.11094) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5GKjFclT618) |
| Multi-Modal Gait Recognition via Effective Spatial-Temporal Feature Fusion | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cui_Multi-Modal_Gait_Recognition_via_Effective_Spatial-Temporal_Feature_Fusion_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wqd27pvkKo4) |
| Probabilistic Knowledge Distillation of Face Ensembles | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xu_Probabilistic_Knowledge_Distillation_of_Face_Ensembles_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=udi1X-F2rIU) |
| Learning Semantic-Aware Disentangled Representation for Flexible 3D Human Body Editing | [![GitHub](https://img.shields.io/github/stars/2017211801/SemanticHuman?style=flat)](https://github.com/2017211801/SemanticHuman) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sun_Learning_Semantic-Aware_Disentangled_Representation_for_Flexible_3D_Human_Body_Editing_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hnrIv1bnZVw) |
| Parameter Efficient Local Implicit Image Function Network for Face Segmentation| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sarkar_Parameter_Efficient_Local_Implicit_Image_Function_Network_for_Face_Segmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15122-b31b1b.svg)](http://arxiv.org/abs/2303.15122) | :heavy_minus_sign: |
| HumanGen: Generating Human Radiance Fields With Explicit Priors | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jiang_HumanGen_Generating_Human_Radiance_Fields_With_Explicit_Priors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.05321-b31b1b.svg)](http://arxiv.org/abs/2212.05321) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=sC_-Q5EeYec) |
| Biomechanics-Guided Facial Action Unit Detection Through Force Modeling | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cui_Biomechanics-Guided_Facial_Action_Unit_Detection_Through_Force_Modeling_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Decoupling Human and Camera Motion From Videos in the Wild | [![GitHub](https://img.shields.io/github/stars/vye16/slahmr?style=flat)](https://github.com/vye16/slahmr)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ye_Decoupling_Human_and_Camera_Motion_From_Videos_in_the_Wild_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.12827-b31b1b.svg)](http://arxiv.org/abs/2302.12827) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=BRdaEI0N87o) |
| Overcoming the Trade-Off Between Accuracy and Plausibility in 3D Hand Shape Reconstruction | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_Overcoming_the_Trade-Off_Between_Accuracy_and_Plausibility_in_3D_Hand_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.00646-b31b1b.svg)](http://arxiv.org/abs/2305.00646) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=yR0W7td1eSs) |
| Instant-NVR: Instant Neural Volumetric Rendering for Human-Object Interactions From Monocular RGBD Stream | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zju3dv.github.io/instant_nvr) <br /> [![GitHub](https://img.shields.io/github/stars/zju3dv/instant-nvr?style=flat)](https://github.com/zju3dv/instant-nvr)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jiang_Instant-NVR_Instant_Neural_Volumetric_Rendering_for_Human-Object_Interactions_From_Monocular_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03184-b31b1b.svg)](https://arxiv.org/abs/2304.03184) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8F-D7kaZmJk) |
| PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation | [![GitHub](https://img.shields.io/github/stars/QitaoZhao/PoseFormerV2?style=flat)](https://github.com/QitaoZhao/PoseFormerV2)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhao_PoseFormerV2_Exploring_Frequency_Domain_for_Efficient_and_Robust_3D_Human_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17472-b31b1b.svg)](http://arxiv.org/abs/2303.17472)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2xVNrGpGldM) |
| Analyzing and Diagnosing Pose Estimation With Attributions| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/He_Analyzing_and_Diagnosing_Pose_Estimation_With_Attributions_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://m.youtube.com/watch?v=Btn4ayp-17k) |
| Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning | [![GitHub](https://img.shields.io/github/stars/zesenwu23/USL-VI-ReID?style=flat)](https://github.com/zesenwu23/USL-VI-ReID)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Unsupervised_Visible-Infrared_Person_Re-Identification_via_Progressive_Graph_Matching_and_Alternate_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Shape-Erased Feature Learning for Visible-Infrared Person Re-Identification | [![GitHub](https://img.shields.io/github/stars/jiawei151/SGIEL_VIReID?style=flat)](https://github.com/jiawei151/SGIEL_VIReID)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Feng_Shape-Erased_Feature_Learning_for_Visible-Infrared_Person_Re-Identification_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04205-b31b1b.svg)](http://arxiv.org/abs/2304.04205) | :heavy_minus_sign: |
| Distilling Cross-Temporal Contexts for Continuous Sign Language Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Guo_Distilling_Cross-Temporal_Contexts_for_Continuous_Sign_Language_Recognition_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Avatars Grow Legs: Generating Smooth Human Motion From Sparse Tracking Inputs With Diffusion Model | [![GitHub](https://img.shields.io/github/stars/facebookresearch/AGRoL?style=flat)](https://github.com/facebookresearch/AGRoL)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Du_Avatars_Grow_Legs_Generating_Smooth_Human_Motion_From_Sparse_Tracking_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08577-b31b1b.svg)](http://arxiv.org/abs/2304.08577) | :heavy_minus_sign: |
| Local Connectivity-Based Density Estimation for Face Clustering | [![GitHub](https://img.shields.io/github/stars/illian01/LCE-PCENet?style=flat)](https://github.com/illian01/LCE-PCENet)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shin_Local_Connectivity-Based_Density_Estimation_for_Face_Clustering_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=adkG0zx34kg) |
| SelfME: Self-Supervised Motion Learning for Micro-Expression Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Fan_SelfME_Self-Supervised_Motion_Learning_for_Micro-Expression_Recognition_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9vkil0gJ6B4) |
| Detecting Human-Object Contact in Images | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hot.is.tue.mpg.de/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Detecting_Human-Object_Contact_in_Images_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03373-b31b1b.svg)](http://arxiv.org/abs/2303.03373) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=MB1OxAxHlIw) |
| Controllable Light Diffusion for Portraits | [![GitHub](https://img.shields.io/github/stars/HighCWu/ControlLoRA?style=flat)](https://github.com/HighCWu/ControlLoRA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Futschik_Controllable_Light_Diffusion_for_Portraits_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.04745-b31b1b.svg)](http://arxiv.org/abs/2305.04745) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=gUY6IT_HOak) |
| InstantAvatar: Learning Avatars From Monocular Video in 60 Seconds | [![GitHub](https://img.shields.io/github/stars/tijiang13/InstantAvatar?style=flat)](https://github.com/tijiang13/InstantAvatar) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jiang_InstantAvatar_Learning_Avatars_From_Monocular_Video_in_60_Seconds_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.10550-b31b1b.svg)](http://arxiv.org/abs/2212.10550) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nixCYsxf1ZE) |
| NeMo: Learning 3D Neural Motion Fields From Multiple Video Instances of the Same Action <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() |[![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sites.google.com/view/nemo-neural-motion-field) <br /> [![GitHub](https://img.shields.io/github/stars/wangkua1/nemo-cvpr2023?style=flat)](https://github.com/wangkua1/nemo-cvpr2023) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_NeMo_Learning_3D_Neural_Motion_Fields_From_Multiple_Video_Instances_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.13660-b31b1b.svg)](https://arxiv.org/abs/2212.13660) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3aoajkG9UGE) |
| Privacy-Preserving Adversarial Facial Features | [![GitHub](https://img.shields.io/github/stars/pterhoer/PrivacyPreservingFaceRecognition?style=flat)](https://github.com/pterhoer/PrivacyPreservingFaceRecognition) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Privacy-Preserving_Adversarial_Facial_Features_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.05391-b31b1b.svg)](http://arxiv.org/abs/2305.05391) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TQRYJ_6humI) |
| Self-Correctable and Adaptable Inference for Generalizable Human Pose Estimation| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kan_Self-Correctable_and_Adaptable_Inference_for_Generalizable_Human_Pose_Estimation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11180-b31b1b.svg)](http://arxiv.org/abs/2303.11180) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=W5cn4W1hBOg) |
| DSFNet: Dual Space Fusion Network for Occlusion-Robust 3D Dense Face Alignment | [![GitHub](https://img.shields.io/github/stars/lhyfst/DSFNet?style=flat)](https://github.com/lhyfst/DSFNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_DSFNet_Dual_Space_Fusion_Network_for_Occlusion-Robust_3D_Dense_Face_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.11522-b31b1b.svg)](https://arxiv.org/abs/2305.11522) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tNcI-1Y9FW8) |
| Clothed Human Performance Capture With a Double-Layer Neural Radiance Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://wangkangkan.github.io/project_pages/ClothedHumanCap/index.html) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Clothed_Human_Performance_Capture_With_a_Double-Layer_Neural_Radiance_Fields_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://m.youtube.com/watch?v=tX3AOOWgpQs) |
| Continuous Landmark Detection With 3D Queries | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chandran_Continuous_Landmark_Detection_With_3D_Queries_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=sK2jsl8jZbQ) |
| Learning a 3D Morphable Face Reflectance Model From Low-Cost Data | [![GitHub](https://img.shields.io/github/stars/yxuhan/ReflectanceMM?style=flat)](https://github.com/yxuhan/ReflectanceMM)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Han_Learning_a_3D_Morphable_Face_Reflectance_Model_From_Low-Cost_Data_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11686-b31b1b.svg)](http://arxiv.org/abs/2303.11686) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=L2pG0RXf0wA) |
| AUNet: Learning Relations Between Action Units for Face Forgery Detection | [![GitHub](https://img.shields.io/github/stars/wmbai/AUNet?style=flat)](https://github.com/wmbai/AUNet)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Bai_AUNet_Learning_Relations_Between_Action_Units_for_Face_Forgery_Detection_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=4fl_A5r4kfA) |
| 3D Human Pose Estimation With Spatio-Temporal Criss-Cross Attention | [![GitHub](https://img.shields.io/github/stars/zhenhuat/STCFormer?style=flat)](https://github.com/zhenhuat/STCFormer)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tang_3D_Human_Pose_Estimation_With_Spatio-Temporal_Criss-Cross_Attention_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VyKbKVZjYqU) |
| Implicit 3D Human Mesh Recovery Using Consistency With Pose and Shape From Unseen-View| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cho_Implicit_3D_Human_Mesh_Recovery_Using_Consistency_With_Pose_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.17651-b31b1b.svg)](https://arxiv.org/abs/2306.17651)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5KJ19dK7PCc) |
| 3D Human Keypoints Estimation From Point Clouds in the Wild Without Human Labels| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Weng_3D_Human_Keypoints_Estimation_From_Point_Clouds_in_the_Wild_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.04745-b31b1b.svg)](https://arxiv.org/abs/2306.04745)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vXGPW2nDHZ4) |
| Multi-Label Compound Expression Recognition: C-EXPR Database & Network | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kollias_Multi-Label_Compound_Expression_Recognition_C-EXPR_Database__Network_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=NgDon2syBbE) |
| FlexNeRF: Photorealistic Free-Viewpoint Rendering of Moving Humans From Sparse Views | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://flex-nerf.github.io/)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jayasundara_FlexNeRF_Photorealistic_Free-Viewpoint_Rendering_of_Moving_Humans_From_Sparse_Views_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14368-b31b1b.svg)](http://arxiv.org/abs/2303.14368) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=MYZtnJVxvo8) |
| Two-Stage Co-Segmentation Network Based on Discriminative Representation for Recovering Human Mesh From Videos | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Two-Stage_Co-Segmentation_Network_Based_on_Discriminative_Representation_for_Recovering_Human_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Co-Speech Gesture Synthesis by Reinforcement Learning With Contrastive Pre-Trained Rewards | [![GitHub](https://img.shields.io/github/stars/RLracer/RACER?style=flat)](https://github.com/RLracer/RACER)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sun_Co-Speech_Gesture_Synthesis_by_Reinforcement_Learning_With_Contrastive_Pre-Trained_Rewards_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-GNRbsbtgZg) |
| FeatER: An Efficient Network for Human Reconstruction via <u>Feat</u>ure Map-based Transform<u>ER</u> | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zczcwh.github.io/feater_page/) <br /> [![GitHub](https://img.shields.io/github/stars/zczcwh/FeatER?style=flat)](https://github.com/zczcwh/FeatER)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zheng_FeatER_An_Efficient_Network_for_Human_Reconstruction_via_Feature_Map-Based_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2205.15448-b31b1b.svg)](http://arxiv.org/abs/2205.15448)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=UErFBdDD7OI) |
