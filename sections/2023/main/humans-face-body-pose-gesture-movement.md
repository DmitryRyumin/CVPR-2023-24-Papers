# CVPR-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/2023/main/image-and-video-synthesis-and-generation.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/2023/main/transfer-meta-low-shot-continual-or-long-tail-learning.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Humans: Face, Body, Pose, Gesture, Movement

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Micron-BERT: BERT-Based Facial Micro-Expression Recognition | [![GitHub](https://img.shields.io/github/stars/uark-cviu/Micron-BERT?style=flat)](https://github.com/uark-cviu/Micron-BERT)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Nguyen_Micron-BERT_BERT-Based_Facial_Micro-Expression_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03195-b31b1b.svg)](https://arxiv.org/abs/2304.03195) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vHN6EYtCo50) |
| NIKI: Neural Inverse Kinematics With Invertible Neural Networks for 3D Human Pose and Shape Estimation | [![GitHub](https://img.shields.io/github/stars/Jeff-sjtu/NIKI?style=flat)](https://github.com/Jeff-sjtu/NIKI)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_NIKI_Neural_Inverse_Kinematics_With_Invertible_Neural_Networks_for_3D_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.08590-b31b1b.svg)](http://arxiv.org/abs/2305.08590) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2tPAYLtat4I) |
| A Characteristic Function-Based Method for Bottom-Up Human Pose Estimation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Qu_A_Characteristic_Function-Based_Method_for_Bottom-Up_Human_Pose_Estimation_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Executing Your Commands via Motion Diffusion in Latent Space | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://chenxin.tech/mld/) <br /> [![GitHub](https://img.shields.io/github/stars/ChenFengYe/motion-latent-diffusion?style=flat)](https://github.com/ChenFengYe/motion-latent-diffusion)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Executing_Your_Commands_via_Motion_Diffusion_in_Latent_Space_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04048-b31b1b.svg)](http://arxiv.org/abs/2212.04048)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Z-nXPfi3p8U) |
| MSINet: Twins Contrastive Search of Multi-Scale Interaction for Object ReID | [![GitHub](https://img.shields.io/github/stars/vimar-gu/MSINet?style=flat)](https://github.com/vimar-gu/MSINet)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gu_MSINet_Twins_Contrastive_Search_of_Multi-Scale_Interaction_for_Object_ReID_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07065-b31b1b.svg)](http://arxiv.org/abs/2303.07065) | :heavy_minus_sign: |
| Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation | [![GitHub](https://img.shields.io/github/stars/Advocate99/DiffGesture?style=flat)](https://github.com/Advocate99/DiffGesture) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhu_Taming_Diffusion_Models_for_Audio-Driven_Co-Speech_Gesture_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09119-b31b1b.svg)](http://arxiv.org/abs/2303.09119) | :heavy_minus_sign: |
| Global-to-Local Modeling for Video-Based 3D Human Pose and Shape Estimation | [![GitHub](https://img.shields.io/github/stars/sxl142/GLoT?style=flat)](https://github.com/sxl142/GLoT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shen_Global-to-Local_Modeling_for_Video-Based_3D_Human_Pose_and_Shape_Estimation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14747-b31b1b.svg)](http://arxiv.org/abs/2303.14747) | :heavy_minus_sign: |
| Dynamic Aggregated Network for Gait Recognition | [![GitHub](https://img.shields.io/github/stars/XKMar/FastGait?style=flat)](https://github.com/XKMar/FastGait) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ma_Dynamic_Aggregated_Network_for_Gait_Recognition_CVPR_2023_paper.pdf) |  [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PiYIo_DZ3I8) |
| Object Pop-Up: Can We Infer 3D Objects and Their Poses From Human Interactions Alone? |  [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://virtualhumans.mpi-inf.mpg.de/object_popup/) <br /> [![GitHub](https://img.shields.io/github/stars/ptrvilya/object-popup?style=flat)](https://github.com/ptrvilya/object-popup) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Petrov_Object_Pop-Up_Can_We_Infer_3D_Objects_and_Their_Poses_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.00777-b31b1b.svg)](https://arxiv.org/abs/2306.00777) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=buEz9ES-R_o) |
| Unsupervised Sampling Promoting for Stochastic Human Trajectory Prediction | [![GitHub](https://img.shields.io/github/stars/viewsetting/Unsupervised_sampling_promoting?style=flat)](https://github.com/viewsetting/Unsupervised_sampling_promoting) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Unsupervised_Sampling_Promoting_for_Stochastic_Human_Trajectory_Prediction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04298-b31b1b.svg)](http://arxiv.org/abs/2304.04298) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hGb2f0zzXs8) |
| ECON: Explicit Clothed Humans Optimized via Normal Integration <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]()  | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://xiuyuliang.cn/econ) <br /> [![GitHub](https://img.shields.io/github/stars/YuliangXiu/ECON?style=flat)](https://github.com/YuliangXiu/ECON)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xiu_ECON_Explicit_Clothed_Humans_Optimized_via_Normal_Integration_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.07422-b31b1b.svg)](http://arxiv.org/abs/2212.07422) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5PEd_p90kS0) |
| Neuron Structure Modeling for Generalizable Remote Physiological Measurement | [![GitHub](https://img.shields.io/github/stars/LuPaoPao/NEST?style=flat)](https://github.com/LuPaoPao/NEST)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lu_Neuron_Structure_Modeling_for_Generalizable_Remote_Physiological_Measurement_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05955-b31b1b.svg)](http://arxiv.org/abs/2303.05955) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=f_4vtE8nTMg) |
| Continuous Sign Language Recognition With Correlation Network | [![GitHub](https://img.shields.io/github/stars/hulianyuyy/CorrNet?style=flat)](https://github.com/hulianyuyy/CorrNet)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hu_Continuous_Sign_Language_Recognition_With_Correlation_Network_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03202-b31b1b.svg)](http://arxiv.org/abs/2303.03202)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kNdEvkPgWhk) |
| Parametric Implicit Face Representation for Audio-Driven Facial Reenactment | [![GitHub](https://img.shields.io/github/stars/JosephPai/Awesome-Talking-Face?style=flat)](https://github.com/JosephPai/Awesome-Talking-Face)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Parametric_Implicit_Face_Representation_for_Audio-Driven_Facial_Reenactment_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.07579-b31b1b.svg)](https://arxiv.org/abs/2306.07579)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OASrT7qm7hc) |
| CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model | [![GitHub](https://img.shields.io/github/stars/dk-liang/CrowdCLIP?style=flat)](https://github.com/dk-liang/CrowdCLIP)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liang_CrowdCLIP_Unsupervised_Crowd_Counting_via_Vision-Language_Model_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04231-b31b1b.svg)](http://arxiv.org/abs/2304.04231) | :heavy_minus_sign: |
| PoseExaminer: Automated Testing of Out-of-Distribution Robustness in Human Pose and Shape Estimation | [![GitHub](https://img.shields.io/github/stars/qihao067/PoseExaminer?style=flat)](https://github.com/qihao067/PoseExaminer)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_PoseExaminer_Automated_Testing_of_Out-of-Distribution_Robustness_in_Human_Pose_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07337-b31b1b.svg)](http://arxiv.org/abs/2303.07337) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PmQ4_VPGLEY) |
| 3D Human Mesh Estimation From Virtual Markers | [![GitHub](https://img.shields.io/github/stars/ShirleyMaxx/VirtualMarker?style=flat)](https://github.com/ShirleyMaxx/VirtualMarker)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ma_3D_Human_Mesh_Estimation_From_Virtual_Markers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11726-b31b1b.svg)](http://arxiv.org/abs/2303.11726)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=w-a0QQi4zjM) |
| 3D Human Pose Estimation via Intuitive Physics | [![GitHub](https://img.shields.io/github/stars/sha2nkt/ipman-r?style=flat)](https://github.com/sha2nkt/ipman-r)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tripathi_3D_Human_Pose_Estimation_via_Intuitive_Physics_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.18246-b31b1b.svg)](https://arxiv.org/abs/2303.18246)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Dufvp_O0ziU) |
| ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation | [![GitHub](https://img.shields.io/github/stars/zc-alexfan/arctic?style=flat)](https://github.com/zc-alexfan/arctic)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Fan_ARCTIC_A_Dataset_for_Dexterous_Bimanual_Hand-Object_Manipulation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2204.13662-b31b1b.svg)](http://arxiv.org/abs/2204.13662)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bvMm8gfFbZ8) |
| Generating Holistic 3D Human Motion From Speech | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://talkshow.is.tue.mpg.de/) <br /> [![GitHub](https://img.shields.io/github/stars/yhw-yhw/TalkSHOW?style=flat)](https://github.com/yhw-yhw/TalkSHOW)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yi_Generating_Holistic_3D_Human_Motion_From_Speech_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04420-b31b1b.svg)](http://arxiv.org/abs/2212.04420) | :heavy_minus_sign: |
| HARP: Personalized Hand Reconstruction From a Monocular RGB Video | [![GitHub](https://img.shields.io/github/stars/korrawe/harp?style=flat)](https://github.com/korrawe/harp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Karunratanakul_HARP_Personalized_Hand_Reconstruction_From_a_Monocular_RGB_Video_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09530-b31b1b.svg)](http://arxiv.org/abs/2212.09530) | :heavy_minus_sign: |
| Learning Locally Editable Virtual Humans | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://custom-humans.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/custom-humans/editable-humans?style=flat)](https://github.com/custom-humans/editable-humans) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ho_Learning_Locally_Editable_Virtual_Humans_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.00121-b31b1b.svg)](http://arxiv.org/abs/2305.00121) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=aT8ql5hB3ZM) |
| Reconstructing Signing Avatars From Video Using Linguistic Priors | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sgnify.is.tue.mpg.de/) <br /> [![GitHub](https://img.shields.io/github/stars/MPForte/SGNify?style=flat)](https://github.com/MPForte/SGNify) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Forte_Reconstructing_Signing_Avatars_From_Video_Using_Linguistic_Priors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10482-b31b1b.svg)](http://arxiv.org/abs/2304.10482) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FLzkGhlsAHw) |
| DrapeNet: Garment Generation and Self-Supervised Draping | [![GitHub](https://img.shields.io/github/stars/liren2515/DrapeNet?style=flat)](https://github.com/liren2515/DrapeNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/De_Luigi_DrapeNet_Garment_Generation_and_Self-Supervised_Draping_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11277-b31b1b.svg)](http://arxiv.org/abs/2211.11277) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vmBITSFNHD0) |
| X-Avatar: Expressive Human Avatars | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://skype-line.github.io/projects/X-Avatar/) <br /> [![GitHub](https://img.shields.io/github/stars/Skype-line/X-Avatar?style=flat)](https://github.com/Skype-line/X-Avatar) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shen_X-Avatar_Expressive_Human_Avatars_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.04805-b31b1b.svg)](https://arxiv.org/abs/2303.04805) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JniKF88aPcs) |
| Hi4D: 4D Instance Segmentation of Close Human Interaction | [![GitHub](https://img.shields.io/github/stars/yifeiyin04/Hi4D?style=flat)](https://github.com/yifeiyin04/Hi4D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yin_Hi4D_4D_Instance_Segmentation_of_Close_Human_Interaction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15380-b31b1b.svg)](http://arxiv.org/abs/2303.15380) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DrvL2XkW7rw) |
| Vid2Avatar: 3D Avatar Reconstruction From Videos in the Wild via Self-Supervised Scene Decomposition | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://moygcc.github.io/vid2avatar/) <br /> [![GitHub](https://img.shields.io/github/stars/MoyGcc/vid2avatar?style=flat)](https://github.com/MoyGcc/vid2avatar) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Guo_Vid2Avatar_3D_Avatar_Reconstruction_From_Videos_in_the_Wild_via_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.11566-b31b1b.svg)](http://arxiv.org/abs/2302.11566) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LXsCGQigteE) |
| CloSET: Modeling Clothed Humans on Continuous Surface With Explicit Template Decomposition | [![GitHub](https://img.shields.io/github/stars/HongwenZhang/CloSET?style=flat)](https://github.com/HongwenZhang/CloSET) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_CloSET_Modeling_Clothed_Humans_on_Continuous_Surface_With_Explicit_Template_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03167-b31b1b.svg)](http://arxiv.org/abs/2304.03167) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=T6h9SBEBS6s) |
| Graphics Capsule: Learning Hierarchical 3D Face Representations From 2D Images | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_Graphics_Capsule_Learning_Hierarchical_3D_Face_Representations_From_2D_Images_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10896-b31b1b.svg)](http://arxiv.org/abs/2303.10896) | :heavy_minus_sign: |
| Rethinking the Learning Paradigm for Dynamic Facial Expression Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Rethinking_the_Learning_Paradigm_for_Dynamic_Facial_Expression_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.15402-b31b1b.svg)](https://arxiv.org/abs/2209.15402) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tANjlVTpsz8) |
| HandNeRF: Neural Radiance Fields for Animatable Interacting Hands | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Guo_HandNeRF_Neural_Radiance_Fields_for_Animatable_Interacting_Hands_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13825-b31b1b.svg)](http://arxiv.org/abs/2303.13825) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=1SMnEyGia9Y) |
| Relightable Neural Human Assets From Multi-View Gradient Illuminations | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://miaoing.github.io/RNHA/)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhou_Relightable_Neural_Human_Assets_From_Multi-View_Gradient_Illuminations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.07648-b31b1b.svg)](http://arxiv.org/abs/2212.07648)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EFcgBDbxJHA) |
| Being Comes From Not-Being: Open-Vocabulary Text-to-Motion Generation With Wordless Training <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/junfanlin/oohmg?style=flat)](junfanlin/oohmg) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_Being_Comes_From_Not-Being_Open-Vocabulary_Text-to-Motion_Generation_With_Wordless_Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.15929-b31b1b.svg)](https://arxiv.org/abs/2210.15929) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=v8MLXgX3R7Y) |
| DeFeeNet: Consecutive 3D Human Motion Prediction With Deviation Feedback | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sun_DeFeeNet_Consecutive_3D_Human_Motion_Prediction_With_Deviation_Feedback_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04496-b31b1b.svg)](http://arxiv.org/abs/2304.04496) | :heavy_minus_sign: |
| BioNet: A Biologically-Inspired Network for Face Recognition | [![GitHub](https://img.shields.io/github/stars/bdevans/BioNet?style=flat)](https://github.com/bdevans/BioNet)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_BioNet_A_Biologically-Inspired_Network_for_Face_Recognition_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_M4qm7rYumo) |
| Boosting Detection in Crowd Analysis via Underutilized Output Features | [![GitHub](https://img.shields.io/github/stars/wskingdom/Crowd-Hat?style=flat)](https://github.com/wskingdom/Crowd-Hat)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Boosting_Detection_in_Crowd_Analysis_via_Underutilized_Output_Features_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.16187-b31b1b.svg)](http://arxiv.org/abs/2308.16187) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6xl2rCVS52I) |
| Learning Analytical Posterior Probability for Human Mesh Recovery | [![GitHub](https://img.shields.io/github/stars/NetEase-GameAI/ProPose?style=flat)](https://github.com/NetEase-GameAI/ProPose)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Fang_Learning_Analytical_Posterior_Probability_for_Human_Mesh_Recovery_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=uS9R82ldkqc) |
| Listening Human Behavior: 3D Human Pose Estimation With Acoustic Signals | [![GitHub](https://img.shields.io/github/stars/YutoShibata07/AcousticPose_Public?style=flat)](https://github.com/YutoShibata07/AcousticPose_Public)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shibata_Listening_Human_Behavior_3D_Human_Pose_Estimation_With_Acoustic_Signals_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=IDvrSUautCI) |
| Detecting and Grounding Multi-Modal Media Manipulation | [![GitHub](https://img.shields.io/github/stars/rshaojimmy/MultiModal-DeepFake?style=flat)](https://github.com/rshaojimmy/MultiModal-DeepFake)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shao_Detecting_and_Grounding_Multi-Modal_Media_Manipulation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02556-b31b1b.svg)](http://arxiv.org/abs/2304.02556) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EortO0cqnGE) |
| RelightableHands: Efficient Neural Relighting of Articulated Hand Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sh8.io/#/relightable_hands) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Iwase_RelightableHands_Efficient_Neural_Relighting_of_Articulated_Hand_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.04866-b31b1b.svg)](http://arxiv.org/abs/2302.04866) | :heavy_minus_sign: |
| MEGANE: Morphable Eyeglass and Avatar Network | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://junxuan-li.github.io/megane/)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_MEGANE_Morphable_Eyeglass_and_Avatar_Network_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.04868-b31b1b.svg)](http://arxiv.org/abs/2302.04868)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xV3sXJN7RxM) |
| SunStage: Portrait Reconstruction and Relighting Using the Sun as a Light Stage | [![GitHub](https://img.shields.io/github/stars/adobe-research/sunstage?style=flat)](https://github.com/adobe-research/sunstage)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_SunStage_Portrait_Reconstruction_and_Relighting_Using_the_Sun_as_a_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2204.03648-b31b1b.svg)](http://arxiv.org/abs/2204.03648) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bc0RTQs5K0E) |
| TryOnDiffusion: A Tale of Two UNets | [![GitHub](https://img.shields.io/github/stars/tryonlabs/tryondiffusion?style=flat)](https://github.com/tryonlabs/tryondiffusion)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhu_TryOnDiffusion_A_Tale_of_Two_UNets_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.08276-b31b1b.svg)](https://arxiv.org/abs/2306.08276)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nMwBVLjRdcc) |
| Semi-Supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhao_Semi-Supervised_Hand_Appearance_Recovery_via_Structure_Disentanglement_and_Dual_Adversarial_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06380-b31b1b.svg)](http://arxiv.org/abs/2303.06380) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PSdDKl8EYS8) |
| POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery | [![GitHub](https://img.shields.io/github/stars/zczcwh/POTTER?style=flat)](https://github.com/zczcwh/POTTER)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zheng_POTTER_Pooling_Attention_Transformer_for_Efficient_Human_Mesh_Recovery_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13357-b31b1b.svg)](http://arxiv.org/abs/2303.13357) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-O4V-yqJmms) |
| Scene-Aware Egocentric 3D Human Pose Estimation | [![GitHub](https://img.shields.io/github/stars/jianwang-mpi/SceneEgo?style=flat)](https://github.com/jianwang-mpi/SceneEgo)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Scene-Aware_Egocentric_3D_Human_Pose_Estimation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.11684-b31b1b.svg)](http://arxiv.org/abs/2212.11684)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bImUBtuZeY8) |
| PSVT: End-to-End Multi-Person 3D Pose and Shape Estimation With Progressive Video Transformers| :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Qiu_PSVT_End-to-End_Multi-Person_3D_Pose_and_Shape_Estimation_With_Progressive_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09187-b31b1b.svg)](http://arxiv.org/abs/2303.09187) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KoOKD1FO_tY) |
| Trajectory-Aware Body Interaction Transformer for Multi-Person Pose Forecasting | [![GitHub](https://img.shields.io/github/stars/xiaogangpeng/TBIFormer?style=flat)](https://github.com/xiaogangpeng/TBIFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Peng_Trajectory-Aware_Body_Interaction_Transformer_for_Multi-Person_Pose_Forecasting_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05095-b31b1b.svg)](http://arxiv.org/abs/2303.05095) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=iyf1kzpRiHs) |
| A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation From a Single RGB Image | [![GitHub](https://img.shields.io/github/stars/ChanglongJiangGit/A2J-Transformer?style=flat)](https://github.com/ChanglongJiangGit/A2J-Transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jiang_A2J-Transformer_Anchor-to-Joint_Transformer_Network_for_3D_Interacting_Hand_Pose_Estimation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03635-b31b1b.svg)](https://arxiv.org/abs/2304.03635) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Q40E1ASRXaQ) |
| TRACE: 5D Temporal Regression of Avatars With Dynamic Cameras in 3D Environments | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://www.yusun.work/TRACE/TRACE.html) <br /> [![GitHub](https://img.shields.io/github/stars/Arthur151/ROMP?style=flat)](https://github.com/Arthur151/ROMP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sun_TRACE_5D_Temporal_Regression_of_Avatars_With_Dynamic_Cameras_in_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.02850-b31b1b.svg)](https://arxiv.org/abs/2306.02850) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=l8aLHDXWQRw) |
| Skinned Motion Retargeting With Residual Perception of Motion Semantics & Geometry | [![GitHub](https://img.shields.io/github/stars/Kebii/R2ET?style=flat)](https://github.com/Kebii/R2ET) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Skinned_Motion_Retargeting_With_Residual_Perception_of_Motion_Semantics__CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08658-b31b1b.svg)](http://arxiv.org/abs/2303.08658) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TzyRLrdNSg8) |
| Generating Human Motion From Textual Descriptions With Discrete Representations | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mael-zys.github.io/T2M-GPT/) <br /> [![GitHub](https://img.shields.io/github/stars/Mael-zys/T2M-GPT?style=flat)](https://github.com/Mael-zys/T2M-GPT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Generating_Human_Motion_From_Textual_Descriptions_With_Discrete_Representations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.06052-b31b1b.svg)](http://arxiv.org/abs/2301.06052) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0PR-cq1x-DU) |
| Learning Human Mesh Recovery in 3D Scenes | [![GitHub](https://img.shields.io/github/stars/zju3dv/SA-HMR?style=flat)](https://github.com/zju3dv/SA-HMR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shen_Learning_Human_Mesh_Recovery_in_3D_Scenes_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.03847-b31b1b.svg)](https://arxiv.org/abs/2306.03847) | :heavy_minus_sign: |
| AVFace: Towards Detailed Audio-Visual 4D Face Reconstruction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://aggelinacha.github.io/AVFace/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chatziagapi_AVFace_Towards_Detailed_Audio-Visual_4D_Face_Reconstruction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.13115-b31b1b.svg)](http://arxiv.org/abs/2304.13115) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-nt2FPFetLg) |
| 3D-Aware Face Swapping | [![GitHub](https://img.shields.io/github/stars/lyx0208/3dSwap?style=flat)](https://github.com/lyx0208/3dSwap) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_3D-Aware_Face_Swapping_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=njeNGZRYVyE) |
| Neural Residual Radiance Fields for Streamably Free-Viewpoint Videos | [![GitHub](https://img.shields.io/github/stars/aoliao12138/ReRF_Dataset?style=flat)](https://github.com/aoliao12138/ReRF_Dataset) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Neural_Residual_Radiance_Fields_for_Streamably_Free-Viewpoint_Videos_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04452-b31b1b.svg)](http://arxiv.org/abs/2304.04452) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lts9AeBjzuU) |
| GFPose: Learning 3D Human Pose Prior With Gradient Fields | [![GitHub](https://img.shields.io/github/stars/Embracing/GFPose?style=flat)](https://github.com/Embracing/GFPose)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ci_GFPose_Learning_3D_Human_Pose_Prior_With_Gradient_Fields_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08641-b31b1b.svg)](http://arxiv.org/abs/2212.08641) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=edAvCKr_bOE) |
| Rethinking Feature-Based Knowledge Distillation for Face Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Rethinking_Feature-Based_Knowledge_Distillation_for_Face_Recognition_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| One-Stage 3D Whole-Body Mesh Recovery With Component Aware Transformer | [![GitHub](https://img.shields.io/github/stars/IDEA-Research/OSX?style=flat)](https://github.com/IDEA-Research/OSX)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_One-Stage_3D_Whole-Body_Mesh_Recovery_With_Component_Aware_Transformer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16160-b31b1b.svg)](http://arxiv.org/abs/2303.16160)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=s0cG3OVXQUo) |
| Towards Stable Human Pose Estimation via Cross-View Fusion and Foot Stabilization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhuo_Towards_Stable_Human_Pose_Estimation_via_Cross-View_Fusion_and_Foot_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CKuAENukmc8) |
| Ego-Body Pose Estimation via Ego-Head Pose Estimation <br/> [![CVPR - Award](https://img.shields.io/badge/CVPR-Award-294A7C)]() | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lijiaman.github.io/projects/egoego/) <br /> [![GitHub](https://img.shields.io/github/stars/lijiaman/egoego_release?style=flat)](https://github.com/lijiaman/egoego_release) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Ego-Body_Pose_Estimation_via_Ego-Head_Pose_Estimation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04636-b31b1b.svg)](http://arxiv.org/abs/2212.04636) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Dg66DY2sGus) |
| TOPLight: Lightweight Neural Networks With Task-Oriented Pretraining for Visible-Infrared Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_TOPLight_Lightweight_Neural_Networks_With_Task-Oriented_Pretraining_for_Visible-Infrared_Recognition_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| StyleIPSB: Identity-Preserving Semantic Basis of StyleGAN for High Fidelity Face Swapping | [![GitHub](https://img.shields.io/github/stars/a686432/StyleIPSB?style=flat)](https://github.com/a686432/StyleIPSB)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jiang_StyleIPSB_Identity-Preserving_Semantic_Basis_of_StyleGAN_for_High_Fidelity_Face_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=R7BH4Y4ca90) |
| Improving Fairness in Facial Albedo Estimation via Visual-Textual Cues <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ren_Improving_Fairness_in_Facial_Albedo_Estimation_via_Visual-Textual_Cues_CVPR_2023_paper.pdf) |  [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2XvqhwWWBsI) |
| FLEX: Full-Body Grasping Without Full-Body Grasps | [![GitHub](https://img.shields.io/github/stars/purvaten/FLEX?style=flat)](https://github.com/purvaten/FLEX)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tendulkar_FLEX_Full-Body_Grasping_Without_Full-Body_Grasps_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11903-b31b1b.svg)](https://arxiv.org/abs/2211.11903) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5Uu9L-eBnX0) |
| EDGE: Editable Dance Generation From Music | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://edge-dance.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/Stanford-TML/EDGE?style=flat)](https://github.com/Stanford-TML/EDGE)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tseng_EDGE_Editable_Dance_Generation_From_Music_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.10658-b31b1b.svg)](http://arxiv.org/abs/2211.10658)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TjOQ_EnKiG0) |
| Complete 3D Human Reconstruction From a Single Incomplete Image | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Complete_3D_Human_Reconstruction_From_a_Single_Incomplete_Image_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6lMbnZp6vNQ) |
| Zero-Shot Pose Transfer for Unrigged Stylized 3D Characters | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jiashunwang.github.io/ZPT/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Zero-Shot_Pose_Transfer_for_Unrigged_Stylized_3D_Characters_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.00200-b31b1b.svg)](http://arxiv.org/abs/2306.00200) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ynT65hp92SE) |
| Hand Avatar: Free-Pose Hand Animation and Rendering From Monocular Video | [![GitHub](https://img.shields.io/github/stars/SeanChenxy/HandAvatar?style=flat)](https://github.com/SeanChenxy/HandAvatar) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Hand_Avatar_Free-Pose_Hand_Animation_and_Rendering_From_Monocular_Video_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12782-b31b1b.svg)](http://arxiv.org/abs/2211.12782) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DQUYYry-NX8) |
| Human-Art: A Versatile Human-Centric Dataset Bridging Natural and Artificial Scenes | [![GitHub](https://img.shields.io/github/stars/IDEA-Research/HumanArt?style=flat)](https://github.com/IDEA-Research/HumanArt) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ju_Human-Art_A_Versatile_Human-Centric_Dataset_Bridging_Natural_and_Artificial_Scenes_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02760-b31b1b.svg)](https://arxiv.org/abs/2303.02760) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=djmTKVlw53E) |
| Learning Neural Proto-Face Field for Disentangled 3D Face Modeling in the Wild | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Learning_Neural_Proto-Face_Field_for_Disentangled_3D_Face_Modeling_in_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| CLAMP: Prompt-Based Contrastive Learning for Connecting Language and Animal Pose | [![GitHub](https://img.shields.io/github/stars/xuzhang1199/CLAMP?style=flat)](https://github.com/xuzhang1199/CLAMP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_CLAMP_Prompt-Based_Contrastive_Learning_for_Connecting_Language_and_Animal_Pose_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.11752-b31b1b.svg)](http://arxiv.org/abs/2206.11752) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bOIl2efyz7E) |
| Invertible Neural Skinning | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yashkant.github.io/invertible-neural-skinning/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kant_Invertible_Neural_Skinning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.09227-b31b1b.svg)](http://arxiv.org/abs/2302.09227) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=il-xVIl6JU4) |
| DiffusionRig: Learning Personalized Priors for Facial Appearance Editing | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://diffusionrig.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/adobe-research/diffusion-rig?style=flat)](https://github.com/adobe-research/diffusion-rig)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ding_DiffusionRig_Learning_Personalized_Priors_for_Facial_Appearance_Editing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06711-b31b1b.svg)](http://arxiv.org/abs/2304.06711) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6ZQbiNiJJEE) |
| Harmonious Feature Learning for Interactive Hand-Object Pose Estimation | [![GitHub](https://img.shields.io/github/stars/lzfff12/HFL-Net?style=flat)](https://github.com/lzfff12/HFL-Net) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_Harmonious_Feature_Learning_for_Interactive_Hand-Object_Pose_Estimation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CRLUkwRr08k) |
| Leapfrog Diffusion Model for Stochastic Trajectory Prediction | [![GitHub](https://img.shields.io/github/stars/MediaBrain-SJTU/LED?style=flat)](https://github.com/MediaBrain-SJTU/LED) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Mao_Leapfrog_Diffusion_Model_for_Stochastic_Trajectory_Prediction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10895-b31b1b.svg)](http://arxiv.org/abs/2303.10895) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=l1gGWVHtBjw) |
| NeuFace: Realistic 3D Neural Face Rendering From Multi-View Images | [![GitHub](https://img.shields.io/github/stars/aejion/NeuFace?style=flat)](https://github.com/aejion/NeuFace)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zheng_NeuFace_Realistic_3D_Neural_Face_Rendering_From_Multi-View_Images_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14092-b31b1b.svg)](http://arxiv.org/abs/2303.14092)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mETP-2hTBQ4) |
| DiffSwap: High-Fidelity and Controllable Face Swapping via 3D-Aware Masked Diffusion | [![GitHub](https://img.shields.io/github/stars/wl-zhao/DiffSwap?style=flat)](https://github.com/wl-zhao/DiffSwap)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhao_DiffSwap_High-Fidelity_and_Controllable_Face_Swapping_via_3D-Aware_Masked_Diffusion_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| GFIE: A Dataset and Baseline for Gaze-Following From 2D to 3D in Indoor Environments | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sites.google.com/view/gfie) <br /> [![GitHub](https://img.shields.io/github/stars/nkuhzx/GFIE?style=flat)](https://github.com/nkuhzx/GFIE)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hu_GFIE_A_Dataset_and_Baseline_for_Gaze-Following_From_2D_to_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9f4NABNrM2g) |
| Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition From Egocentric RGB Videos | [![GitHub](https://img.shields.io/github/stars/fylwen/HTT?style=flat)](https://github.com/fylwen/HTT)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wen_Hierarchical_Temporal_Transformer_for_3D_Hand_Pose_Estimation_and_Action_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.09484-b31b1b.svg)](http://arxiv.org/abs/2209.09484) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lsFjkK-wmMA) |
| Decompose More and Aggregate Better: Two Closer Looks at Frequency Representation Learning for Human Motion Prediction | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_Decompose_More_and_Aggregate_Better_Two_Closer_Looks_at_Frequency_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zNWaywq7cGI) |
| Human Pose As Compositional Tokens | [![GitHub](https://img.shields.io/github/stars/Gengzigang/PCT?style=flat)](https://github.com/Gengzigang/PCT)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Geng_Human_Pose_As_Compositional_Tokens_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11638-b31b1b.svg)](http://arxiv.org/abs/2303.11638)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=eXt_ONASk5E) |
| Normal-Guided Garment UV Prediction for Human Re-Texturing <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jafarian_Normal-Guided_Garment_UV_Prediction_for_Human_Re-Texturing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06504-b31b1b.svg)](http://arxiv.org/abs/2303.06504) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=smXAY890YJQ) |
| Dynamic Graph Learning With Content-Guided Spatial-Frequency Relation Reasoning for Deepfake Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Dynamic_Graph_Learning_With_Content-Guided_Spatial-Frequency_Relation_Reasoning_for_Deepfake_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=W8Hl3p2T50U) |
| VGFlow: Visibility Guided Flow Network for Human Reposing | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jain_VGFlow_Visibility_Guided_Flow_Network_for_Human_Reposing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.08540-b31b1b.svg)](http://arxiv.org/abs/2211.08540) | :heavy_minus_sign: |
| Mutual Information-Based Temporal Difference Learning for Human Pose Estimation in Video | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Feng_Mutual_Information-Based_Temporal_Difference_Learning_for_Human_Pose_Estimation_in_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08475-b31b1b.svg)](http://arxiv.org/abs/2303.08475)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=4VXZMo8N7Ms) |
| PREIM3D: 3D Consistent Precise Image Attribute Editing from a Single Image |  |  |  |
| HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation |  |  |  |
| Implicit Identity Driven Deepfake Face Swapping Detection |  |  |  |
| Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion |  |  |  |
| 3D-Aware Facial Landmark Detection via Multi-View Consistent Training on Synthetic Data |  |  |  |
| SLOPER4D: A Scene-Aware Dataset for Global 4D Human Pose Estimation in Urban Environments |  |  |  |
| Zero-Shot Text-to-Parameter Translation for Game Character Auto-Creation |  |  |  |
| AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation |  |  |  |
| UDE: A Unified Driving Engine for Human Motion Generation |  |  |  |
| CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior |  |  |  |
| Semi-Supervised 2D Human Pose Estimation Driven by Position Inconsistency Pseudo Label Correction Module |  |  |  |
| Learning Personalized High Quality Volumetric Head Avatars from Monocular RGB Videos |  |  |  |
| HOOD: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics |  |  |  |
| ACR: Attention Collaboration-based Regressor for Arbitrary Two-Hand Reconstruction |  |  |  |
| HumanBench: Towards General Human-Centric Perception with Projector Assisted Pretraining |  |  |  |
| CIMI4D: A Large Multimodal Climbing Motion Dataset under Human-Scene Interactions |  |  |  |
| Human Pose Estimation in Extremely Low-Light Conditions |  |  |  |
| DistilPose: Tokenized Pose Regression with Heatmap Distillation |  |  |  |
| Human Body Shape Completion with Implicit Shape and Flow Learning |  |  |  |
| Source-Free Adaptive Gaze Estimation by Uncertainty Reduction |  |  |  |
| Music-Driven Group Choreography |  |  |  |
| Robust Model-based Face Reconstruction through Weakly-Supervised Outlier Segmentation |  |  |  |
| MARLIN: Masked Autoencoder for Facial Video Representation LearnINg |  |  |  |
| Transformer-based Unified Recognition of Two Hands Manipulating Objects |  |  |  |
| Implicit Identity Leakage: The Stumbling Block to Improving Deepfake Detection Generalization |  |  |  |
| ScarceNet: Animal Pose Estimation with Scarce Annotations |  |  |  |
| FFHQ-UV: Normalized Facial UV-Texture Dataset for 3D Face Reconstruction |  |  |  |
| MoDi: Unconditional Motion Synthesis from Diverse Data |  |  |  |
| Feature Representation Learning with Adaptive Displacement Generation and Transformer Fusion for Micro-Expression Recognition |  |  |  |
| MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand Reconstruction |  |  |  |
| Stimulus Verification is a Universal and Effective Sampler in Multi-Modal Human Trajectory Prediction |  |  |  |
| TokenHPE: Learning Orientation Tokens for Efficient Head Pose Estimation via Transformers |  |  |  |
| Handy: Towards a High Fidelity 3D Hand Shape and Appearance Model |  |  |  |
| CIRCLE: Capture in Rich Contextual Environments |  |  |  |
| Gazeformer: Scalable, Effective and Fast Prediction of Goal-Directed Human Attention |  |  |  |
| Implicit Neural Head Synthesis via Controllable Local Deformation Fields |  |  |  |
| Continuous Intermediate Token Learning with Implicit Motion Manifold for Keyframe based Motion Interpolation |  |  |  |
| JRDB-Pose: A Large-Scale Dataset for Multi-Person Pose Estimation and Tracking |  |  |  |
| STAR Loss: Reducing Semantic Ambiguity in Facial Landmark Detection |  |  |  |
| GM-NeRF: Learning Generalizable Model-based Neural Radiance Fields from Multi-View Images |  |  |  |
| Decoupled Multimodal Distilling for Emotion Recognition |  |  |  |
| HaLP: Hallucinating Latent Positives for Skeleton-based Self-Supervised Learning of Actions |  |  |  |
| ReDirTrans: Latent-to-Latent Translation for Gaze and Head Redirection |  |  |  |
| QPGesture: Quantization-based and Phase-guided Motion Matching for Natural Speech-Driven Gesture Generation |  |  |  |
| Multi-Modal Gait Recognition via Effective Spatial-Temporal Feature Fusion |  |  |  |
| Probabilistic Knowledge Distillation of Face Ensembles |  |  |  |
| Learning Semantic-Aware Disentangled Representation for Flexible 3D Human Body Editing |  |  |  |
| Parameter Efficient Local Implicit Image Function Network for Face Segmentation |  |  |  |
| HumanGen: Generating Human Radiance Fields with Explicit Priors |  |  |  |
| Biomechanics-guided Facial Action Unit Detection through Force Modeling |  |  |  |
| Decoupling Human and Camera Motion from Videos in the Wild |  |  |  |
| Overcoming the Trade-Off Between Accuracy and Plausibility in 3D Hand Shape Reconstruction |  |  |  |
| Instant-NVR: Instant Neural Volumetric Rendering for Human-Object Interactions from Monocular RGBD Stream |  |  |  |
| PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation |  |  |  |
| Analyzing and Diagnosing Pose Estimation with Attributions |  |  |  |
| Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning |  |  |  |
| Shape-Erased Feature Learning for Visible-Infrared Person Re-Identification |  |  |  |
| Distilling Cross-Temporal Contexts for Continuous Sign Language Recognition |  |  |  |
| Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model |  |  |  |
| Local Connectivity-based Density Estimation for Face Clustering |  |  |  |
| SelfME: Self-Supervised Motion Learning for Micro-Expression Recognition |  |  |  |
| Detecting Human-Object Contact in Images |  |  |  |
| Controllable Light Diffusion for Portraits |  |  |  |
| InstantAvatar: Learning Avatars from Monocular Video in 60 Seconds |  |  |  |
| <i>NeMo</i>: 3D <i>Ne</i>ural <i>Mo</i>tion Fields from Multiple Video Instances of the Same Action |  |  |  |
| Privacy-Preserving Adversarial Facial Features |  |  |  |
| Self-Correctable and Adaptable Inference for Generalizable Human Pose Estimation |  |  |  |
| DSFNet: Dual Space Fusion Network for Occlusion-Robust 3D Dense Face Alignment |  |  |  |
| Clothed Human Performance Capture with a Double-Layer Neural Radiance Fields |  |  |  |
| Continuous Landmark Detection with 3D Queries |  |  |  |
| Learning a 3D Morphable Face Reflectance Model from Low-Cost Data |  |  |  |
| AUNet: Learning Relations between Action Units for Face Forgery Detection |  |  |  |
| 3D Human Pose Estimation with Spatio-Temporal Criss-Cross Attention |  |  |  |
| Implicit 3D Human Mesh Recovery using Consistency with Pose and Shape from Unseen-View |  |  |  |
| 3D Human Keypoints Estimation from Point Clouds in the Wild without Human Labels |  |  |  |
| Multi-Label Compound Expression Recognition: C-EXPR Database & Network |  |  |  |
| FlexNeRF: Photorealistic Free-Viewpoint Rendering of Moving Humans from Sparse Views |  |  |  |
| Two-Stage Co-Segmentation Network based on Discriminative Representation for Recovering Human Mesh from Videos |  |  |  |
| Co-Speech Gesture Synthesis by Reinforcement Learning with Contrastive Pre-trained Rewards |  |  |  |
| FeatER: An Efficient Network for Human Reconstruction via <u>Feat</u>ure Map-based Transform<u>ER</u> |  |  |  |
