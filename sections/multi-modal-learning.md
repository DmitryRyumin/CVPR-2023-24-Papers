# CVPR-2023-Papers

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/deep-learning-architectures-and-techniques.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/3d-from-single-images.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" />
    </a>
</div>

## Multi-Modal Learning

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Pix2map: Cross-Modal Retrieval for Inferring Street Maps From Images | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://pix2map.github.io/) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Pix2map_Cross-Modal_Retrieval_for_Inferring_Street_Maps_From_Images_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.04224-b31b1b.svg)](http://arxiv.org/abs/2301.04224) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=18VtggvpynY) |
| Audio-Visual Grouping Network for Sound Localization From Mixtures | [![GitHub](https://img.shields.io/github/stars/stoneMo/AVGN)](https://github.com/stoneMo/AVGN) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Mo_Audio-Visual_Grouping_Network_for_Sound_Localization_From_Mixtures_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17056-b31b1b.svg)](http://arxiv.org/abs/2303.17056) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xMA2vOlpaHY) |
| Learning Semantic Relationship Among Instances for Image-Text Matching | [![GitHub](https://img.shields.io/github/stars/CrossmodalGroup/HREM)](https://github.com/CrossmodalGroup/HREM)|[![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Fu_Learning_Semantic_Relationship_Among_Instances_for_Image-Text_Matching_CVPR_2023_paper.pdf)  | :heavy_minus_sign: |
| Identity-Preserving Talking Face Generation With Landmark and Appearance Priors | [![GitHub](https://img.shields.io/github/stars/Weizhi-Zhong/IP_LAP)](https://github.com/Weizhi-Zhong/IP_LAP)|[![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhong_Identity-Preserving_Talking_Face_Generation_With_Landmark_and_Appearance_Priors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.08293-b31b1b.svg)](http://arxiv.org/abs/2305.08293) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jpap8rLXh94) |
| ImageBind: One Embedding Space To Bind Them All <br /> [CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://facebookresearch.github.io/ImageBind) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.05665-b31b1b.svg)](http://arxiv.org/abs/2305.05665) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=X5JCEzhdkW4) |
| Learning To Dub Movies via Hierarchical Prosody Models | [![GitHub](https://img.shields.io/github/stars/GalaxyCong/HPMDubbing)](https://github.com/GalaxyCong/HPMDubbing) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cong_Learning_To_Dub_Movies_via_Hierarchical_Prosody_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04054-b31b1b.svg)](http://arxiv.org/abs/2212.04054) | :heavy_minus_sign: |
| OmniMAE: Single Model Masked Pretraining on Images and Videos |  |  |  |
| CNVid-3.5M: Build, Filter, and Pre-Train the Large-Scale Public Chinese Video-Text Dataset |  |  |  |
| Egocentric Audio-Visual Object Localization |  |  |  |
| Learning Visual Representations via Language-guided Sampling |  |  |  |
| Unite and Conquer: Plug & Play Multi-Modal Synthesis using Diffusion Models |  |  |  |
| iQuery: Instruments as Queries for Audio-Visual Sound Separation |  |  |  |
| Diverse Embedding Expansion Network and Low-Light Cross-Modality Benchmark for Visible-Infrared Person Re-Identification |  |  |  |
| PiMAE: Point Cloud and Image Interactive Masked Autoencoders for 3D Object Detection |  |  |  |
| Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-Shot Learners |  |  |  |
| Non-Contrastive Learning Meets Language-Image Pre-Training |  |  |  |
| Highly Confident Local Structure based Consensus Graph Learning for Incomplete Multi-View Clustering |  |  |  |
| Vision Transformers are Parameter-Efficient Audio-Visual Learners |  |  |  |
| Teaching Structured Vision & Language Concepts to Vision & Language Models |  |  |  |
| Data-Free Sketch-based Image Retrieval |  |  |  |
| Align and Attend: Multimodal Summarization with Dual Contrastive Losses |  |  |  |
| Efficient Multimodal Fusion via Interactive Prompting |  |  |  |
| Multimodal Prompting with Missing Modalities for Visual Recognition |  |  |  |
| Learning Instance-Level Representation for Large-Scale Multi-Modal Pretraining in E-Commerce |  |  |  |
| What Happened 3 Seconds Ago? Inferring the Past with Thermal Imaging |  |  |  |
| MMANet: Margin-Aware Distillation and Modality-Aware Regularization for Incomplete Multimodal Learning |  |  |  |
| Multi-Modal Learning with Missing Modality via Shared-Specific Feature Modelling |  |  |  |
| The ObjectFolder Benchmark: Multisensory Learning with Neural and Real Objects |  |  |  |
| Position-guided Text Prompt for Vision-Language Pre-Training |  |  |  |
| Conditional Generation of Audio from Video via Foley Analogies |  |  |  |
| OSAN: A One-Stage Alignment Network to Unify Multimodal Alignment and Unsupervised Domain Adaptation |  |  |  |
| Self-Supervised Video Forensics by Audio-Visual Anomaly Detection |  |  |  |
| ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding |  |  |  |
| AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR |  |  |  |
| Watch or Listen: Robust Audio-Visual Speech Recognition with Visual Corruption Modeling and Reliability Scoring |  |  |  |
| SceneTrilogy: On Human Scene-Sketch and its Complementarity with Photo and Text |  |  |  |
| Exploring and Exploiting Uncertainty for Incomplete Multi-View Classification |  |  |  |
| EXIF as Language: Learning Cross-Modal Associations between Images and Camera Metadata |  |  |  |
| Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens |  |  |  |
| RONO: Robust Discriminative Learning with Noisy Labels for 2D-3D Cross-Modal Retrieval|  |  |  |
| CASP-Net: Rethinking Video Saliency Prediction from an Audio-Visual Consistency Perceptual Perspective |  |  |  |
| Learning Audio-Visual Source Localization via False Negative Aware Contrastive Learning |  |  |  |
| ReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Regeneration |  |  |  |
| Look, Radiate, and Learn: Self-Supervised Localisation via Radio-Visual Correspondence |  |  |  |
| Learning Emotion Representations from Verbal and Nonverbal Communication |  |  |  |
| Enhanced Multimodal Representation Learning with Cross-Modal KD |  |  |  |
| MELTR: Meta Loss Transformer for Learning to Fine-Tune Video Foundation Models |  |  |  |
| Multilateral Semantic Relations Modeling for Image Text Retrieval |  |  |  |
| GeoVLN: Learning Geometry-enhanced Visual Representation with Slot Attention for Vision-and-Language Navigation |  |  |  |
| Noisy Correspondence Learning with Meta Similarity Correction |  |  |  |
| Improving Cross-Modal Retrieval with Set of Diverse Embeddings |  |  |  |
| Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment |  |  |  |
| MaPLe: Multi-Modal Prompt Learning |  |  |  |
| Fine-grained Image-Text Matching by Cross-Modal Hard Aligning Network |  |  |  |
| Towards Modality-Agnostic Person Re-Identification with Descriptive Query |  |  |  |
| Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos |  |  |  |
| FashionSAP: Symbols and Attributes Prompt for Fine-grained Fashion Vision-Language Pre-Training |  |  |  |
| MAP: Multimodal Uncertainty-Aware Vision-Language Pre-Training Model |  |  |  |
| Egocentric Auditory Attention Localization in Conversations |  |  |  |
| Improving Zero-Shot Generalization and Robustness of Multi-Modal Models |  |  |  |
| Understanding and Constructing Latent Modality Structures in Multi-Modal Representation Learning |  |  |  |
| Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles |  |  |  |
| GCFAgg: Global and Cross-View Feature Aggregation for Multi-View Clustering |  |  |  |
| BiCro: Noisy Correspondence Rectification for Multi-Modality Data via Bi-Directional Cross-Modal Similarity Consistency |  |  |  |
| DisCo-CLIP: A Distributed Contrastive Loss for Memory Efficient CLIP Training |  |  |  |
| Referring Image Matting |  |  |  |
| Leveraging per Image-Token Consistency for Vision-Language Pre-Training |  |  |  |
| Seeing what You Miss: Vision-Language Pre-Training with Semantic Completion Learning |  |  |  |
| Sample-Level Multi-View Graph Clustering |  |  |  |
| SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation |  |  |  |
| On the Effects of Self-Supervision and Contrastive Alignment in Deep Multi-View Clustering |  |  |  |
| SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model |  |  |  |
| Novel-View Acoustic Synthesis |  |  |  |
| MAGVLT: Masked Generative Vision-and-Language Transformer |  |  |  |
| Reproducible Scaling Laws for Contrastive Language-Image Learning |  |  |  |
| PMR: Prototypical Modal Rebalance for Multimodal Learning |  |  |  |
| Language-guided Music Recommendation for Video via Prompt Analogies |  |  |  |
| RA-CLIP: Retrieval Augmented Contrastive Language-Image Pre-Training |  |  |  |
| <i>MMG-Ego4D</i>: <i>M</i>ulti-<i>M</i>odal <i>G</i>eneralization in Egocentric Action Recognition |  |  |  |
| Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning |  |  |  |
| PRISE: Demystifying Deep Lucas-Kanade with Strongly Star-Convex Constraints for Multimodel Image Alignment |  |  |  |
| Masked Autoencoding Does Not Help Natural Language Supervision at Scale |  |  |  |
| CLIPPO: Image-and-Language Understanding from Pixels Only |  |  |  |
| Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations |  |  |  |
| Critical Learning Periods for Multisensory Integration in Deep Networks |  |  |  |
| CLIPPING: Distilling CLIP-based Models with a Student base for Video-Language Retrieval |  |  |  |
| NUWA-LIP: Language-guided Image Inpainting with Defect-Free VQGAN |  |  |  |
| WINNER: Weakly-Supervised hIerarchical DecompositioN and aligNment for spatio-tEmporal Video gRounding |  |  |  |
| Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation |  |  |  |
