# CVPR-2023-Papers

<div align="center">
  <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/deep-learning-architectures-and-techniques.md">
    <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" />
  </a>
  <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
    <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" />
  </a>
  <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/3d-from-single-images.md">
    <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" />
  </a>
</div>

## Multi-Modal Learning

![Section Papers](https://img.shields.io/badge/Section%20Papers-89-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-75-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-60-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-50-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Pix2map: Cross-Modal Retrieval for Inferring Street Maps From Images | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://pix2map.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Pix2map_Cross-Modal_Retrieval_for_Inferring_Street_Maps_From_Images_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.04224-b31b1b.svg)](http://arxiv.org/abs/2301.04224) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=18VtggvpynY) |
| Audio-Visual Grouping Network for Sound Localization From Mixtures | [![GitHub](https://img.shields.io/github/stars/stoneMo/AVGN)](https://github.com/stoneMo/AVGN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Mo_Audio-Visual_Grouping_Network_for_Sound_Localization_From_Mixtures_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17056-b31b1b.svg)](http://arxiv.org/abs/2303.17056) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xMA2vOlpaHY) |
| Learning Semantic Relationship Among Instances for Image-Text Matching | [![GitHub](https://img.shields.io/github/stars/CrossmodalGroup/HREM)](https://github.com/CrossmodalGroup/HREM)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Fu_Learning_Semantic_Relationship_Among_Instances_for_Image-Text_Matching_CVPR_2023_paper.pdf)  | :heavy_minus_sign: |
| Identity-Preserving Talking Face Generation With Landmark and Appearance Priors | [![GitHub](https://img.shields.io/github/stars/Weizhi-Zhong/IP_LAP)](https://github.com/Weizhi-Zhong/IP_LAP)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhong_Identity-Preserving_Talking_Face_Generation_With_Landmark_and_Appearance_Priors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.08293-b31b1b.svg)](http://arxiv.org/abs/2305.08293) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jpap8rLXh94) |
| ImageBind: One Embedding Space To Bind Them All <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://facebookresearch.github.io/ImageBind) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.05665-b31b1b.svg)](http://arxiv.org/abs/2305.05665) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=X5JCEzhdkW4) |
| Learning To Dub Movies via Hierarchical Prosody Models | [![GitHub](https://img.shields.io/github/stars/GalaxyCong/HPMDubbing)](https://github.com/GalaxyCong/HPMDubbing) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cong_Learning_To_Dub_Movies_via_Hierarchical_Prosody_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04054-b31b1b.svg)](http://arxiv.org/abs/2212.04054) | :heavy_minus_sign: |
| OmniMAE: Single Model Masked Pretraining on Images and Videos | [![GitHub](https://img.shields.io/github/stars/facebookresearch/omnivore)](https://github.com/facebookresearch/omnivore)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Girdhar_OmniMAE_Single_Model_Masked_Pretraining_on_Images_and_Videos_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.08356-b31b1b.svg)](http://arxiv.org/abs/2206.08356) | :heavy_minus_sign: |
| CNVid-3.5M: Build, Filter, and Pre-Train the Large-Scale Public Chinese Video-Text Dataset | [![GitHub](https://img.shields.io/github/stars/CNVid/CNVid-3.5M)](https://github.com/CNVid/CNVid-3.5M)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gan_CNVid-3.5M_Build_Filter_and_Pre-Train_the_Large-Scale_Public_Chinese_Video-Text_CVPR_2023_paper.pdf)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ywJfAg4wvr0) |
| Egocentric Audio-Visual Object Localization | [![GitHub](https://img.shields.io/github/stars/WikiChao/Ego-AV-Loc)](https://github.com/WikiChao/Ego-AV-Loc)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Egocentric_Audio-Visual_Object_Localization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13471-b31b1b.svg)](http://arxiv.org/abs/2303.13471)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0-_XJJ1JLmM) |
| Learning Visual Representations via Language-Guided Sampling | [![GitHub](https://img.shields.io/github/stars/mbanani/lgssl)](https://github.com/mbanani/lgssl)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Banani_Learning_Visual_Representations_via_Language-Guided_Sampling_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.12248-b31b1b.svg)](http://arxiv.org/abs/2302.12248)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=95I8DmUoJ2s) |
| Unite and Conquer: Plug & Play Multi-Modal Synthesis Using Diffusion Models | [![GitHub](https://img.shields.io/github/stars/Nithin-GK/UniteandConquer)](https://github.com/Nithin-GK/UniteandConquer)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Nair_Unite_and_Conquer_Plug__Play_Multi-Modal_Synthesis_Using_Diffusion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.00793-b31b1b.svg)](http://arxiv.org/abs/2212.00793) | :heavy_minus_sign: |
| iQuery: Instruments As Queries for Audio-Visual Sound Separation | [![GitHub](https://img.shields.io/github/stars/JiabenChen/iQuery)](https://github.com/JiabenChen/iQuery)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_iQuery_Instruments_As_Queries_for_Audio-Visual_Sound_Separation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03814-b31b1b.svg)](http://arxiv.org/abs/2212.03814) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EZ9CgknV9Z4) |
| Diverse Embedding Expansion Network and Low-Light Cross-Modality Benchmark for Visible-Infrared Person Re-Identification | [![GitHub](https://img.shields.io/github/stars/ZYK100/LLCM)](https://github.com/ZYK100/LLCM)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Diverse_Embedding_Expansion_Network_and_Low-Light_Cross-Modality_Benchmark_for_Visible-Infrared_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14481-b31b1b.svg)](http://arxiv.org/abs/2303.14481) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=oMIRqc-Fq5c) |
| PiMAE: Point Cloud and Image Interactive Masked Autoencoders for 3D Object Detection | [![GitHub](https://img.shields.io/github/stars/BLVLab/PiMAE)](https://github.com/BLVLab/PiMAE)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_PiMAE_Point_Cloud_and_Image_Interactive_Masked_Autoencoders_for_3D_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08129-b31b1b.svg)](http://arxiv.org/abs/2303.08129) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rcs8DYAwugQ) |
| Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners | [![GitHub](https://img.shields.io/github/stars/ZrrSkywalker/CaFo)](https://github.com/ZrrSkywalker/CaFo)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Prompt_Generate_Then_Cache_Cascade_of_Foundation_Models_Makes_Strong_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02151-b31b1b.svg)](http://arxiv.org/abs/2303.02151) | :heavy_minus_sign: |
| Non-Contrastive Learning Meets Language-Image Pre-Training | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhou_Non-Contrastive_Learning_Meets_Language-Image_Pre-Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.09304-b31b1b.svg)](http://arxiv.org/abs/2210.09304) | :heavy_minus_sign: |
| Highly Confident Local Structure Based Consensus Graph Learning for Incomplete Multi-View Clustering | [![GitHub](https://img.shields.io/github/stars/ckghostwj/cvpr2023_code)](https://github.com/ckghostwj/cvpr2023_code)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wen_Highly_Confident_Local_Structure_Based_Consensus_Graph_Learning_for_Incomplete_CVPR_2023_paper.pdf)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8yaouwWf4ko) |
| Vision Transformers Are Parameter-Efficient Audio-Visual Learners | [![GitHub](https://img.shields.io/github/stars/GenjiB/LAVISH)](https://github.com/GenjiB/LAVISH)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_Vision_Transformers_Are_Parameter-Efficient_Audio-Visual_Learners_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.07983-b31b1b.svg)](http://arxiv.org/abs/2212.07983) | :heavy_minus_sign: |
| Teaching Structured Vision & Language Concepts to Vision & Language Models | [![GitHub](https://img.shields.io/github/stars/SivanDoveh/TSVLC)](https://github.com/SivanDoveh/TSVLC)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Doveh_Teaching_Structured_Vision__Language_Concepts_to_Vision__Language_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11733-b31b1b.svg)](https://arxiv.org/abs/2211.11733)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=s_rr1RbX1Iw) |
| Data-Free Sketch-Based Image Retrieval | [![GitHub](https://img.shields.io/github/stars/abhrac/data-free-sbir)](https://github.com/abhrac/data-free-sbir)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chaudhuri_Data-Free_Sketch-Based_Image_Retrieval_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07775-b31b1b.svg)](http://arxiv.org/abs/2303.07775) | :heavy_minus_sign: |
| Align and Attend: Multimodal Summarization With Dual Contrastive Losses | [![GitHub](https://img.shields.io/github/stars/boheumd/A2Summ)](https://github.com/boheumd/A2Summ)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/He_Align_and_Attend_Multimodal_Summarization_With_Dual_Contrastive_Losses_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07284-b31b1b.svg)](http://arxiv.org/abs/2303.07284) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7EfdJx6G9rI) |
| Efficient Multimodal Fusion via Interactive Prompting | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Efficient_Multimodal_Fusion_via_Interactive_Prompting_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06306-b31b1b.svg)](http://arxiv.org/abs/2304.06306)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=MK_HJkgGnJ4) |
| Multimodal Prompting With Missing Modalities for Visual Recognition | [![GitHub](https://img.shields.io/github/stars/YiLunLee/missing_aware_prompts)](https://github.com/YiLunLee/missing_aware_prompts)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lee_Multimodal_Prompting_With_Missing_Modalities_for_Visual_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03369-b31b1b.svg)](http://arxiv.org/abs/2303.03369) | :heavy_minus_sign: |
| Learning Instance-Level Representation for Large-Scale Multi-Modal Pretraining in E-Commerce | :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jin_Learning_Instance-Level_Representation_for_Large-Scale_Multi-Modal_Pretraining_in_E-Commerce_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02853-b31b1b.svg)](https://arxiv.org/abs/2304.02853) | :heavy_minus_sign: |
| What Happened 3 Seconds Ago? Inferring the Past With Thermal Imaging | [![GitHub](https://img.shields.io/github/stars/ZitianTang/Thermal-IM)](https://github.com/ZitianTang/Thermal-IM)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tang_What_Happened_3_Seconds_Ago_Inferring_the_Past_With_Thermal_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.13651-b31b1b.svg)](http://arxiv.org/abs/2304.13651) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=RKptl-mUYQw) |
| MMANet: Margin-Aware Distillation and Modality-Aware Regularization for Incomplete Multimodal Learning | [![GitHub](https://img.shields.io/github/stars/shicaiwei123/MMANet)](https://github.com/shicaiwei123/MMANet)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wei_MMANet_Margin-Aware_Distillation_and_Modality-Aware_Regularization_for_Incomplete_Multimodal_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08028-b31b1b.svg)](http://arxiv.org/abs/2304.08028) | :heavy_minus_sign: |
| Multi-Modal Learning With Missing Modality via Shared-Specific Feature Modelling | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Multi-Modal_Learning_With_Missing_Modality_via_Shared-Specific_Feature_Modelling_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.14126-b31b1b.svg)](https://arxiv.org/abs/2307.14126)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wkYFpWOZaeg) |
| The ObjectFolder Benchmark: Multisensory Learning With Neural and Real Objects | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://objectfolder.stanford.edu/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_The_ObjectFolder_Benchmark_Multisensory_Learning_With_Neural_and_Real_Objects_CVPR_2023_paper.pdf) |[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VhXDempUYgE) |
| Position-Guided Text Prompt for Vision-Language Pre-Training | [![GitHub](https://img.shields.io/github/stars/sail-sg/ptp)](https://github.com/sail-sg/ptp)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Position-Guided_Text_Prompt_for_Vision-Language_Pre-Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09737-b31b1b.svg)](http://arxiv.org/abs/2212.09737) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6FNXUSMefIQ) |
| Conditional Generation of Audio From Video via Foley Analogies | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://xypb.github.io/CondFoleyGen/) <br /> [![GitHub](https://img.shields.io/github/stars/XYPB/CondFoleyGen)](https://github.com/XYPB/CondFoleyGen)  |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Du_Conditional_Generation_of_Audio_From_Video_via_Foley_Analogies_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08490-b31b1b.svg)](http://arxiv.org/abs/2304.08490) | :heavy_minus_sign: |
| OSAN: A One-Stage Alignment Network To Unify Multimodal Alignment and Unsupervised Domain Adaptation | :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_OSAN_A_One-Stage_Alignment_Network_To_Unify_Multimodal_Alignment_and_CVPR_2023_paper.pdf)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=84H9dQZD3DY) |
| Self-Supervised Video Forensics by Audio-Visual Anomaly Detection <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cfeng16.github.io/audio-visual-forensics/) <br /> [![GitHub](https://img.shields.io/github/stars/cfeng16/audio-visual-forensics)](https://github.com/cfeng16/audio-visual-forensics)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.01767-b31b1b.svg)](http://arxiv.org/abs/2301.01767) | :heavy_minus_sign: |
| ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding | [![GitHub](https://img.shields.io/github/stars/salesforce/ULIP)](https://github.com/salesforce/ULIP)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_ULIP_Learning_a_Unified_Representation_of_Language_Images_and_Point_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.05171-b31b1b.svg)](https://arxiv.org/abs/2212.05171) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dgFYBVmeilk) |
| AVFormer: Injecting Vision Into Frozen Speech Models for Zero-Shot AV-ASR | :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Seo_AVFormer_Injecting_Vision_Into_Frozen_Speech_Models_for_Zero-Shot_AV-ASR_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16501-b31b1b.svg)](http://arxiv.org/abs/2303.16501) | :heavy_minus_sign: |
| Watch or Listen: Robust Audio-Visual Speech Recognition With Visual Corruption Modeling and Reliability Scoring | [![GitHub](https://img.shields.io/github/stars/joannahong/AV-RelScore)](https://github.com/joannahong/AV-RelScore)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hong_Watch_or_Listen_Robust_Audio-Visual_Speech_Recognition_With_Visual_Corruption_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08536-b31b1b.svg)](http://arxiv.org/abs/2303.08536) | :heavy_minus_sign: |
| SceneTrilogy: On Human Scene-Sketch and Its Complementarity With Photo and Text | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.pinakinathc.me/scenetrilogy/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chowdhury_SceneTrilogy_On_Human_Scene-Sketch_and_Its_Complementarity_With_Photo_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2204.11964-b31b1b.svg)](http://arxiv.org/abs/2204.11964) | :heavy_minus_sign: |
| Exploring and Exploiting Uncertainty for Incomplete Multi-View Classification | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xie_Exploring_and_Exploiting_Uncertainty_for_Incomplete_Multi-View_Classification_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05165-b31b1b.svg)](http://arxiv.org/abs/2304.05165) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=d124-SGH6bw) |
| EXIF As Language: Learning Cross-Modal Associations Between Images and Camera Metadata <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() |[![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hellomuffin.github.io/exif-as-language/) <br /> [![GitHub](https://img.shields.io/github/stars/hellomuffin/exif-as-language)](https://github.com/hellomuffin/exif-as-language) |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zheng_EXIF_As_Language_Learning_Cross-Modal_Associations_Between_Images_and_Camera_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.04647-b31b1b.svg)](http://arxiv.org/abs/2301.04647) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dPHdNgxUXd4) |
| Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens | [![GitHub](https://img.shields.io/github/stars/yuxiaochen1103/FDT)](https://github.com/yuxiaochen1103/FDT)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Revisiting_Multimodal_Representation_in_Contrastive_Learning_From_Patch_and_Token_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14865-b31b1b.svg)](http://arxiv.org/abs/2303.14865)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PT1InnOlfmg) |
| RONO: Robust Discriminative Learning With Noisy Labels for 2D-3D Cross-Modal Retrieval | [![GitHub](https://img.shields.io/github/stars/penghu-cs/RONO)](https://github.com/penghu-cs/RONO)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Feng_RONO_Robust_Discriminative_Learning_With_Noisy_Labels_for_2D-3D_Cross-Modal_CVPR_2023_paper.pdf)  | :heavy_minus_sign: |
| CASP-Net: Rethinking Video Saliency Prediction From an Audio-Visual Consistency Perceptual Perspective | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://junwenxiong.github.io/CASP-Net/index.html) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xiong_CASP-Net_Rethinking_Video_Saliency_Prediction_From_an_Audio-Visual_Consistency_Perceptual_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06357-b31b1b.svg)](https://arxiv.org/abs/2303.06357) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=keeAINxmhAM) |
| Learning Audio-Visual Source Localization via False Negative Aware Contrastive Learning | [![GitHub](https://img.shields.io/github/stars/OpenNLPLab/FNAC_AVL)](https://github.com/OpenNLPLab/FNAC_AVL)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sun_Learning_Audio-Visual_Source_Localization_via_False_Negative_Aware_Contrastive_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11302-b31b1b.svg)](http://arxiv.org/abs/2303.11302)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LZprwXIn-2U) |
| ReVISE: Self-Supervised Speech Resynthesis With Visual Input for Universal and Generalized Speech Regeneration | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://wnhsu.github.io/ReVISE/) <br /> [![GitHub](https://img.shields.io/github/stars/facebookresearch/av_hubert)](https://github.com/facebookresearch/av_hubert)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Hsu_ReVISE_Self-Supervised_Speech_Resynthesis_With_Visual_Input_for_Universal_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.11377-b31b1b.svg)](https://arxiv.org/abs/2212.11377) |  :heavy_minus_sign: |
| Look, Radiate, and Learn: Self-Supervised Localisation via Radio-Visual Correspondence |  :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Alloulah_Look_Radiate_and_Learn_Self-Supervised_Localisation_via_Radio-Visual_Correspondence_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.06424-b31b1b.svg)](http://arxiv.org/abs/2206.06424) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=NUV0hm03slY) |
| Learning Emotion Representations From Verbal and Nonverbal Communication | [![GitHub](https://img.shields.io/github/stars/Xeaver/EmotionCLIP)](https://github.com/Xeaver/EmotionCLIP)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Learning_Emotion_Representations_From_Verbal_and_Nonverbal_Communication_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.13500-b31b1b.svg)](https://arxiv.org/abs/2305.13500) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9Mcw_xf3Dk4) |
| Enhanced Multimodal Representation Learning With Cross-Modal KD |  :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Enhanced_Multimodal_Representation_Learning_With_Cross-Modal_KD_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.07646-b31b1b.svg)](https://arxiv.org/abs/2306.07646) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZTELXRN5W0M) |
| MELTR: Meta Loss Transformer for Learning To Fine-Tune Video Foundation Models | [![GitHub](https://img.shields.io/github/stars/mlvlab/MELTR)](https://github.com/mlvlab/MELTR)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ko_MELTR_Meta_Loss_Transformer_for_Learning_To_Fine-Tune_Video_Foundation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13009-b31b1b.svg)](http://arxiv.org/abs/2303.13009)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nDIKwYRf-NE) |
| Multilateral Semantic Relations Modeling for Image Text Retrieval |  :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Multilateral_Semantic_Relations_Modeling_for_Image_Text_Retrieval_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| GeoVLN: Learning Geometry-Enhanced Visual Representation With Slot Attention for Vision-and-Language Navigation | [![GitHub](https://img.shields.io/github/stars/jingyanghuo/GeoVLN)](https://github.com/jingyanghuo/GeoVLN)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huo_GeoVLN_Learning_Geometry-Enhanced_Visual_Representation_With_Slot_Attention_for_Vision-and-Language_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.17102-b31b1b.svg)](https://arxiv.org/abs/2305.17102) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=i-WjmcNMnQI) |
| Noisy Correspondence Learning With Meta Similarity Correction | [![GitHub](https://img.shields.io/github/stars/hhc1997/MSCN)](https://github.com/hhc1997/MSCN)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Han_Noisy_Correspondence_Learning_With_Meta_Similarity_Correction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06275-b31b1b.svg)](http://arxiv.org/abs/2304.06275) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DspzqtqDgmo) |
| Improving Cross-Modal Retrieval With Set of Diverse Embeddings <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://cvlab.postech.ac.kr/research/DivE/)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kim_Improving_Cross-Modal_Retrieval_With_Set_of_Diverse_Embeddings_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.16761-b31b1b.svg)](http://arxiv.org/abs/2211.16761) | :heavy_minus_sign: |
| Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sound2scene.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/postech-ami/Sound2Scene)](https://github.com/postech-ami/Sound2Scene) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Sung-Bin_Sound_to_Visual_Scene_Generation_by_Audio-to-Visual_Latent_Alignment_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17490-b31b1b.svg)](http://arxiv.org/abs/2303.17490) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=XETgkM22n6s) |
| MaPLe: Multi-Modal Prompt Learning | [![GitHub](https://img.shields.io/github/stars/muzairkhattak/multimodal-prompt-learning)](https://github.com/muzairkhattak/multimodal-prompt-learning)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Khattak_MaPLe_Multi-Modal_Prompt_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.03117-b31b1b.svg)](http://arxiv.org/abs/2210.03117) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=fmULeaqAzfg) |
| Fine-Grained Image-Text Matching by Cross-Modal Hard Aligning Network | [![GitHub](https://img.shields.io/github/stars/ppanzx/CHAN)](https://github.com/ppanzx/CHAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Pan_Fine-Grained_Image-Text_Matching_by_Cross-Modal_Hard_Aligning_Network_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DwvSv_FATjg) |
| Towards Modality-Agnostic Person Re-Identification With Descriptive Query | [![GitHub](https://img.shields.io/github/stars/ccq195/UNIReID)](https://github.com/ccq195/UNIReID) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Towards_Modality-Agnostic_Person_Re-Identification_With_Descriptive_Query_CVPR_2023_paper.pdf)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_nQANSa7GyA) |
| Physics-Driven Diffusion Models for Impact Sound Synthesis From Videos | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sukun1045.github.io/video-physics-sound-diffusion/) <br /> [![GitHub](https://img.shields.io/github/stars/sukun1045/video-physics-sound-diffusion)](https://github.com/sukun1045/video-physics-sound-diffusion)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Su_Physics-Driven_Diffusion_Models_for_Impact_Sound_Synthesis_From_Videos_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16897-b31b1b.svg)](http://arxiv.org/abs/2303.16897) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8IDssk5bWmg) |
| FashionSAP: Symbols and Attributes Prompt for Fine-Grained Fashion Vision-Language Pre-Training | [![GitHub](https://img.shields.io/github/stars/hssip/FashionSAP)](https://github.com/hssip/FashionSAP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Han_FashionSAP_Symbols_and_Attributes_Prompt_for_Fine-Grained_Fashion_Vision-Language_Pre-Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05051-b31b1b.svg)](http://arxiv.org/abs/2304.05051) | :heavy_minus_sign: |
| MAP: Multimodal Uncertainty-Aware Vision-Language Pre-Training Model | [![GitHub](https://img.shields.io/github/stars/IIGROUP/MAP)](https://github.com/IIGROUP/MAP) |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ji_MAP_Multimodal_Uncertainty-Aware_Vision-Language_Pre-Training_Model_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.05335-b31b1b.svg)](http://arxiv.org/abs/2210.05335) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=XZqZDqe9MYo) |
| Egocentric Auditory Attention Localization in Conversations | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://fkryan.github.io/saal) |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ryan_Egocentric_Auditory_Attention_Localization_in_Conversations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16024-b31b1b.svg)](http://arxiv.org/abs/2303.16024) | :heavy_minus_sign: |
| Improving Zero-Shot Generalization and Robustness of Multi-Modal Models | [![GitHub](https://img.shields.io/github/stars/gyhandy/Hierarchy-CLIP)](https://github.com/gyhandy/Hierarchy-CLIP)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ge_Improving_Zero-Shot_Generalization_and_Robustness_of_Multi-Modal_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.01758-b31b1b.svg)](http://arxiv.org/abs/2212.01758) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6nirYCh2xA0) |
| Understanding and Constructing Latent Modality Structures in Multi-Modal Representation Learning | :heavy_minus_sign:|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jiang_Understanding_and_Constructing_Latent_Modality_Structures_in_Multi-Modal_Representation_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05952-b31b1b.svg)](http://arxiv.org/abs/2303.05952) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kEDr2zmhe3Q) |
| Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() |[![GitHub](https://img.shields.io/github/stars/pleaseconnectwifi/DANCE)](https://github.com/pleaseconnectwifi/DANCE)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ye_Improving_Commonsense_in_Vision-Language_Models_via_Knowledge_Graph_Riddles_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.16504-b31b1b.svg)](http://arxiv.org/abs/2211.16504) | :heavy_minus_sign: |
| GCFAgg: Global and Cross-View Feature Aggregation for Multi-View Clustering|[![GitHub](https://img.shields.io/github/stars/Galaxy922/GCFAggMVC)](https://github.com/Galaxy922/GCFAggMVC)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yan_GCFAgg_Global_and_Cross-View_Feature_Aggregation_for_Multi-View_Clustering_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.06799-b31b1b.svg)](http://arxiv.org/abs/2305.06799) | :heavy_minus_sign: |
| BiCro: Noisy Correspondence Rectification for Multi-Modality Data via Bi-Directional Cross-Modal Similarity Consistency|[![GitHub](https://img.shields.io/github/stars/xu5zhao/BiCro)](https://github.com/xu5zhao/BiCro)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yang_BiCro_Noisy_Correspondence_Rectification_for_Multi-Modality_Data_via_Bi-Directional_Cross-Modal_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12419-b31b1b.svg)](http://arxiv.org/abs/2303.12419) | :heavy_minus_sign: |
| DisCo-CLIP: A Distributed Contrastive Loss for Memory Efficient CLIP Training <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() |[![GitHub](https://img.shields.io/github/stars/IDEA-Research/DisCo-CLIP)](https://github.com/IDEA-Research/DisCo-CLIP)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_DisCo-CLIP_A_Distributed_Contrastive_Loss_for_Memory_Efficient_CLIP_Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08480-b31b1b.svg)](https://arxiv.org/abs/2304.08480) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=1MTUKnqRwC0) |
| Referring Image Matting | [![GitHub](https://img.shields.io/github/stars/JizhiziLi/RIM)](https://github.com/JizhiziLi/RIM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Referring_Image_Matting_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.05149-b31b1b.svg)](http://arxiv.org/abs/2206.05149)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0d9J5DD7vAY) |
| Leveraging per Image-Token Consistency for Vision-Language Pre-Training | [![GitHub](https://img.shields.io/github/stars/gyhdog99/epic)](https://github.com/gyhdog99/epic) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gou_Leveraging_per_Image-Token_Consistency_for_Vision-Language_Pre-Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.15398-b31b1b.svg)](http://arxiv.org/abs/2211.15398) | :heavy_minus_sign: |
| Seeing What You Miss: Vision-Language Pre-Training With Semantic Completion Learning | [![GitHub](https://img.shields.io/github/stars/iigroup/scl)](https://github.com/iigroup/scl)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ji_Seeing_What_You_Miss_Vision-Language_Pre-Training_With_Semantic_Completion_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13437-b31b1b.svg)](http://arxiv.org/abs/2211.13437) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KXqcGfozrQw) |
| Sample-Level Multi-View Graph Clustering |  :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tan_Sample-Level_Multi-View_Graph_Clustering_CVPR_2023_paper.pdf) | :heavy_minus_sign: | 
| SmallCap: Lightweight Image Captioning Prompted With Retrieval Augmentation | [![GitHub](https://img.shields.io/github/stars/RitaRamo/smallcap)](https://github.com/RitaRamo/smallcap)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ramos_SmallCap_Lightweight_Image_Captioning_Prompted_With_Retrieval_Augmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.15323-b31b1b.svg)](http://arxiv.org/abs/2209.15323)| :heavy_minus_sign: |
| On the Effects of Self-Supervision and Contrastive Alignment in Deep Multi-View Clustering <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() |  [![GitHub](https://img.shields.io/github/stars/DanielTrosten/DeepMVC)](https://github.com/DanielTrosten/DeepMVC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Trosten_On_the_Effects_of_Self-Supervision_and_Contrastive_Alignment_in_Deep_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09877-b31b1b.svg)](https://arxiv.org/abs/2303.09877) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=YRnOo3qs28A) |
| SmartBrush: Text and Shape Guided Object Inpainting With Diffusion Model |  :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xie_SmartBrush_Text_and_Shape_Guided_Object_Inpainting_With_Diffusion_Model_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.05034-b31b1b.svg)](http://arxiv.org/abs/2212.05034)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kzrfcKi-XCI) |
| Novel-View Acoustic Synthesis | [![GitHub](https://img.shields.io/github/stars/facebookresearch/novel-view-acoustic-synthesis)](https://github.com/facebookresearch/novel-view-acoustic-synthesis) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Novel-View_Acoustic_Synthesis_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.08730-b31b1b.svg)](http://arxiv.org/abs/2301.08730) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5X8JdLYUA8w) |
| MAGVLT: Masked Generative Vision-and-Language Transformer | [![GitHub](https://img.shields.io/github/stars/kakaobrain/magvlt)](https://github.com/kakaobrain/magvlt) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kim_MAGVLT_Masked_Generative_Vision-and-Language_Transformer_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12208-b31b1b.svg)](http://arxiv.org/abs/2303.12208) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8dpW-V7N3-0) |
| Reproducible Scaling Laws for Contrastive Language-Image Learning | [![GitHub](https://img.shields.io/github/stars/LAION-AI/scaling-laws-openclip)](https://github.com/LAION-AI/scaling-laws-openclip) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.07143-b31b1b.svg)](http://arxiv.org/abs/2212.07143)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=QFNt52y6IK4) |
| PMR: Prototypical Modal Rebalance for Multimodal Learning | [![GitHub](https://img.shields.io/github/stars/fanyunfeng-bit/Modal-Imbalance-PMR)](https://github.com/fanyunfeng-bit/Modal-Imbalance-PMR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Fan_PMR_Prototypical_Modal_Rebalance_for_Multimodal_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.07089-b31b1b.svg)](http://arxiv.org/abs/2211.07089) | :heavy_minus_sign: |
| Language-Guided Music Recommendation for Video via Prompt Analogies <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.danielbmckee.com/language-guided-music-for-video/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/McKee_Language-Guided_Music_Recommendation_for_Video_via_Prompt_Analogies_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.09327-b31b1b.svg)](https://arxiv.org/abs/2306.09327) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=z6DBTAiLKzY) |
| RA-CLIP: Retrieval Augmented Contrastive Language-Image Pre-Training |  :heavy_minus_sign:  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xie_RA-CLIP_Retrieval_Augmented_Contrastive_Language-Image_Pre-Training_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| MMG-Ego4D: Multimodal Generalization in Egocentric Action Recognition |  :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gong_MMG-Ego4D_Multimodal_Generalization_in_Egocentric_Action_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.07214-b31b1b.svg)](https://arxiv.org/abs/2305.07214) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rZoMIkIk6Gw) |
| Open Vocabulary Semantic Segmentation With Patch Aligned Contrastive Learning <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Mukhoti_Open_Vocabulary_Semantic_Segmentation_With_Patch_Aligned_Contrastive_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04994-b31b1b.svg)](http://arxiv.org/abs/2212.04994) | :heavy_minus_sign: |
| PRISE: Demystifying Deep Lucas-Kanade With Strongly Star-Convex Constraints for Multimodel Image Alignment <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub](https://img.shields.io/github/stars/Zhang-VISLab/CVPR2023-PRISE)](https://github.com/Zhang-VISLab/CVPR2023-PRISE)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_PRISE_Demystifying_Deep_Lucas-Kanade_With_Strongly_Star-Convex_Constraints_for_Multimodel_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11526-b31b1b.svg)](http://arxiv.org/abs/2303.11526) | :heavy_minus_sign: |
| Masked Autoencoding Does Not Help Natural Language Supervision at Scale | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Weers_Masked_Autoencoding_Does_Not_Help_Natural_Language_Supervision_at_Scale_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.07836-b31b1b.svg)](http://arxiv.org/abs/2301.07836) | :heavy_minus_sign: |
| CLIPPO: Image-and-Language Understanding From Pixels Only | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/clippo/README.md) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tschannen_CLIPPO_Image-and-Language_Understanding_From_Pixels_Only_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08045-b31b1b.svg)](http://arxiv.org/abs/2212.08045)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3tTbm4_EsFU) |
| Chat2Map: Efficient Scene Mapping From Multi-Ego Conversations | [![GitHub](https://img.shields.io/github/stars/facebookresearch/chat2map-official)](https://github.com/facebookresearch/chat2map-official) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Majumder_Chat2Map_Efficient_Scene_Mapping_From_Multi-Ego_Conversations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.02184-b31b1b.svg)](http://arxiv.org/abs/2301.02184) | :heavy_minus_sign: |
| Critical Learning Periods for Multisensory Integration in Deep Networks <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kleinman_Critical_Learning_Periods_for_Multisensory_Integration_in_Deep_Networks_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.04643-b31b1b.svg)](http://arxiv.org/abs/2210.04643) | :heavy_minus_sign: |
| CLIPPING: Distilling CLIP-Based Models With a Student Base for Video-Language Retrieval |  :heavy_minus_sign:  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Pei_CLIPPING_Distilling_CLIP-Based_Models_With_a_Student_Base_for_Video-Language_CVPR_2023_paper.pdf)  | :heavy_minus_sign: |
| NUWA-LIP: Language-Guided Image Inpainting With Defect-Free VQGAN | [![GitHub](https://img.shields.io/github/stars/kodenii/NUWA-LIP)](https://github.com/kodenii/NUWA-LIP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ni_NUWA-LIP_Language-Guided_Image_Inpainting_With_Defect-Free_VQGAN_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2202.05009-b31b1b.svg)](https://arxiv.org/abs/2202.05009) | :heavy_minus_sign: |
| WINNER: Weakly-Supervised hIerarchical decompositioN and aligNment for Spatio-tEmporal Video gRounding |  :heavy_minus_sign:  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_WINNER_Weakly-Supervised_hIerarchical_decompositioN_and_aligNment_for_Spatio-tEmporal_Video_gRounding_CVPR_2023_paper.pdf)  | :heavy_minus_sign: |
| Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation|[![GitHub](https://img.shields.io/github/stars/feiyuchen7/M3NET)](https://github.com/feiyuchen7/M3NET) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Multivariate_Multi-Frequency_and_Multimodal_Rethinking_Graph_Neural_Networks_for_Emotion_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
