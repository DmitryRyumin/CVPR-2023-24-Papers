# CVPR-2023-Papers

<div align="center">
  <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/deep-learning-architectures-and-techniques.md">
    <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" />
  </a>
  <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
    <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" />
  </a>
  <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/3d-from-single-images.md">
    <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" />
  </a>
</div>

## Multi-Modal Learning

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Pix2map: Cross-Modal Retrieval for Inferring Street Maps From Images | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://pix2map.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_Pix2map_Cross-Modal_Retrieval_for_Inferring_Street_Maps_From_Images_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.04224-b31b1b.svg)](http://arxiv.org/abs/2301.04224) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=18VtggvpynY) |
| Audio-Visual Grouping Network for Sound Localization From Mixtures | [![GitHub](https://img.shields.io/github/stars/stoneMo/AVGN)](https://github.com/stoneMo/AVGN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Mo_Audio-Visual_Grouping_Network_for_Sound_Localization_From_Mixtures_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17056-b31b1b.svg)](http://arxiv.org/abs/2303.17056) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xMA2vOlpaHY) |
| Learning Semantic Relationship Among Instances for Image-Text Matching | [![GitHub](https://img.shields.io/github/stars/CrossmodalGroup/HREM)](https://github.com/CrossmodalGroup/HREM)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Fu_Learning_Semantic_Relationship_Among_Instances_for_Image-Text_Matching_CVPR_2023_paper.pdf)  | :heavy_minus_sign: |
| Identity-Preserving Talking Face Generation With Landmark and Appearance Priors | [![GitHub](https://img.shields.io/github/stars/Weizhi-Zhong/IP_LAP)](https://github.com/Weizhi-Zhong/IP_LAP)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhong_Identity-Preserving_Talking_Face_Generation_With_Landmark_and_Appearance_Priors_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.08293-b31b1b.svg)](http://arxiv.org/abs/2305.08293) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jpap8rLXh94) |
| ImageBind: One Embedding Space To Bind Them All <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://facebookresearch.github.io/ImageBind) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.05665-b31b1b.svg)](http://arxiv.org/abs/2305.05665) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=X5JCEzhdkW4) |
| Learning To Dub Movies via Hierarchical Prosody Models | [![GitHub](https://img.shields.io/github/stars/GalaxyCong/HPMDubbing)](https://github.com/GalaxyCong/HPMDubbing) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cong_Learning_To_Dub_Movies_via_Hierarchical_Prosody_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04054-b31b1b.svg)](http://arxiv.org/abs/2212.04054) | :heavy_minus_sign: |
| OmniMAE: Single Model Masked Pretraining on Images and Videos | [![GitHub](https://img.shields.io/github/stars/facebookresearch/omnivore)](https://github.com/facebookresearch/omnivore)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Girdhar_OmniMAE_Single_Model_Masked_Pretraining_on_Images_and_Videos_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.08356-b31b1b.svg)](http://arxiv.org/abs/2206.08356) | :heavy_minus_sign: |
| CNVid-3.5M: Build, Filter, and Pre-Train the Large-Scale Public Chinese Video-Text Dataset | [![GitHub](https://img.shields.io/github/stars/CNVid/CNVid-3.5M)](https://github.com/CNVid/CNVid-3.5M)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gan_CNVid-3.5M_Build_Filter_and_Pre-Train_the_Large-Scale_Public_Chinese_Video-Text_CVPR_2023_paper.pdf)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ywJfAg4wvr0) |
| Egocentric Audio-Visual Object Localization | [![GitHub](https://img.shields.io/github/stars/WikiChao/Ego-AV-Loc)](https://github.com/WikiChao/Ego-AV-Loc)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Huang_Egocentric_Audio-Visual_Object_Localization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13471-b31b1b.svg)](http://arxiv.org/abs/2303.13471)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0-_XJJ1JLmM) |
| Learning Visual Representations via Language-Guided Sampling | [![GitHub](https://img.shields.io/github/stars/mbanani/lgssl)](https://github.com/mbanani/lgssl)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Banani_Learning_Visual_Representations_via_Language-Guided_Sampling_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.12248-b31b1b.svg)](http://arxiv.org/abs/2302.12248)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=95I8DmUoJ2s) |
| Unite and Conquer: Plug & Play Multi-Modal Synthesis Using Diffusion Models | [![GitHub](https://img.shields.io/github/stars/Nithin-GK/UniteandConquer)](https://github.com/Nithin-GK/UniteandConquer)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Nair_Unite_and_Conquer_Plug__Play_Multi-Modal_Synthesis_Using_Diffusion_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.00793-b31b1b.svg)](http://arxiv.org/abs/2212.00793) | :heavy_minus_sign: |
| iQuery: Instruments As Queries for Audio-Visual Sound Separation | [![GitHub](https://img.shields.io/github/stars/JiabenChen/iQuery)](https://github.com/JiabenChen/iQuery)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_iQuery_Instruments_As_Queries_for_Audio-Visual_Sound_Separation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03814-b31b1b.svg)](http://arxiv.org/abs/2212.03814) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EZ9CgknV9Z4) |
| Diverse Embedding Expansion Network and Low-Light Cross-Modality Benchmark for Visible-Infrared Person Re-Identification | [![GitHub](https://img.shields.io/github/stars/ZYK100/LLCM)](https://github.com/ZYK100/LLCM)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Diverse_Embedding_Expansion_Network_and_Low-Light_Cross-Modality_Benchmark_for_Visible-Infrared_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14481-b31b1b.svg)](http://arxiv.org/abs/2303.14481) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=oMIRqc-Fq5c) |
| PiMAE: Point Cloud and Image Interactive Masked Autoencoders for 3D Object Detection | [![GitHub](https://img.shields.io/github/stars/BLVLab/PiMAE)](https://github.com/BLVLab/PiMAE)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_PiMAE_Point_Cloud_and_Image_Interactive_Masked_Autoencoders_for_3D_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08129-b31b1b.svg)](http://arxiv.org/abs/2303.08129) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rcs8DYAwugQ) |
| Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners | [![GitHub](https://img.shields.io/github/stars/ZrrSkywalker/CaFo)](https://github.com/ZrrSkywalker/CaFo)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Prompt_Generate_Then_Cache_Cascade_of_Foundation_Models_Makes_Strong_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02151-b31b1b.svg)](http://arxiv.org/abs/2303.02151) | :heavy_minus_sign: |
| Non-Contrastive Learning Meets Language-Image Pre-Training | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhou_Non-Contrastive_Learning_Meets_Language-Image_Pre-Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.09304-b31b1b.svg)](http://arxiv.org/abs/2210.09304) | :heavy_minus_sign: |
| Highly Confident Local Structure Based Consensus Graph Learning for Incomplete Multi-View Clustering | [![GitHub](https://img.shields.io/github/stars/ckghostwj/cvpr2023_code)](https://github.com/ckghostwj/cvpr2023_code)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wen_Highly_Confident_Local_Structure_Based_Consensus_Graph_Learning_for_Incomplete_CVPR_2023_paper.pdf)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8yaouwWf4ko) |
| Vision Transformers Are Parameter-Efficient Audio-Visual Learners | [![GitHub](https://img.shields.io/github/stars/GenjiB/LAVISH)](https://github.com/GenjiB/LAVISH)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_Vision_Transformers_Are_Parameter-Efficient_Audio-Visual_Learners_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.07983-b31b1b.svg)](http://arxiv.org/abs/2212.07983) | :heavy_minus_sign: |
| Teaching Structured Vision & Language Concepts to Vision & Language Models | [![GitHub](https://img.shields.io/github/stars/SivanDoveh/TSVLC)](https://github.com/SivanDoveh/TSVLC)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Doveh_Teaching_Structured_Vision__Language_Concepts_to_Vision__Language_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11733-b31b1b.svg)](https://arxiv.org/abs/2211.11733)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=s_rr1RbX1Iw) |
| Data-Free Sketch-Based Image Retrieval | [![GitHub](https://img.shields.io/github/stars/abhrac/data-free-sbir)](https://github.com/abhrac/data-free-sbir)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chaudhuri_Data-Free_Sketch-Based_Image_Retrieval_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07775-b31b1b.svg)](http://arxiv.org/abs/2303.07775) | :heavy_minus_sign: |
| Align and Attend: Multimodal Summarization With Dual Contrastive Losses | [![GitHub](https://img.shields.io/github/stars/boheumd/A2Summ)](https://github.com/boheumd/A2Summ)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/He_Align_and_Attend_Multimodal_Summarization_With_Dual_Contrastive_Losses_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07284-b31b1b.svg)](http://arxiv.org/abs/2303.07284) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7EfdJx6G9rI) |
| Efficient Multimodal Fusion via Interactive Prompting | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Efficient_Multimodal_Fusion_via_Interactive_Prompting_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06306-b31b1b.svg)](http://arxiv.org/abs/2304.06306)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=MK_HJkgGnJ4) |
| Multimodal Prompting With Missing Modalities for Visual Recognition | [![GitHub](https://img.shields.io/github/stars/YiLunLee/missing_aware_prompts)](https://github.com/YiLunLee/missing_aware_prompts)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lee_Multimodal_Prompting_With_Missing_Modalities_for_Visual_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03369-b31b1b.svg)](http://arxiv.org/abs/2303.03369) | :heavy_minus_sign: |
| Learning Instance-Level Representation for Large-Scale Multi-Modal Pretraining in E-Commerce | :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Jin_Learning_Instance-Level_Representation_for_Large-Scale_Multi-Modal_Pretraining_in_E-Commerce_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02853-b31b1b.svg)](https://arxiv.org/abs/2304.02853) | :heavy_minus_sign: |
| What Happened 3 Seconds Ago? Inferring the Past With Thermal Imaging | [![GitHub](https://img.shields.io/github/stars/ZitianTang/Thermal-IM)](https://github.com/ZitianTang/Thermal-IM)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Tang_What_Happened_3_Seconds_Ago_Inferring_the_Past_With_Thermal_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.13651-b31b1b.svg)](http://arxiv.org/abs/2304.13651) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=RKptl-mUYQw) |
| MMANet: Margin-Aware Distillation and Modality-Aware Regularization for Incomplete Multimodal Learning | [![GitHub](https://img.shields.io/github/stars/shicaiwei123/MMANet)](https://github.com/shicaiwei123/MMANet)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wei_MMANet_Margin-Aware_Distillation_and_Modality-Aware_Regularization_for_Incomplete_Multimodal_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08028-b31b1b.svg)](http://arxiv.org/abs/2304.08028) | :heavy_minus_sign: |
| Multi-Modal Learning With Missing Modality via Shared-Specific Feature Modelling | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Multi-Modal_Learning_With_Missing_Modality_via_Shared-Specific_Feature_Modelling_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.14126-b31b1b.svg)](https://arxiv.org/abs/2307.14126)|[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wkYFpWOZaeg) |
| The ObjectFolder Benchmark: Multisensory Learning With Neural and Real Objects | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://objectfolder.stanford.edu/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_The_ObjectFolder_Benchmark_Multisensory_Learning_With_Neural_and_Real_Objects_CVPR_2023_paper.pdf) |[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VhXDempUYgE) |
| Position-Guided Text Prompt for Vision-Language Pre-Training | [![GitHub](https://img.shields.io/github/stars/sail-sg/ptp)](https://github.com/sail-sg/ptp)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_Position-Guided_Text_Prompt_for_Vision-Language_Pre-Training_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09737-b31b1b.svg)](http://arxiv.org/abs/2212.09737) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6FNXUSMefIQ) |
| Conditional Generation of Audio From Video via Foley Analogies | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://xypb.github.io/CondFoleyGen/) <br /> [![GitHub](https://img.shields.io/github/stars/XYPB/CondFoleyGen)](https://github.com/XYPB/CondFoleyGen)  |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Du_Conditional_Generation_of_Audio_From_Video_via_Foley_Analogies_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08490-b31b1b.svg)](http://arxiv.org/abs/2304.08490) | :heavy_minus_sign: |
| OSAN: A One-Stage Alignment Network To Unify Multimodal Alignment and Unsupervised Domain Adaptation | :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_OSAN_A_One-Stage_Alignment_Network_To_Unify_Multimodal_Alignment_and_CVPR_2023_paper.pdf)  | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=84H9dQZD3DY) |
| Self-Supervised Video Forensics by Audio-Visual Anomaly Detection <br/> [![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00)]() | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cfeng16.github.io/audio-visual-forensics/) <br /> [![GitHub](https://img.shields.io/github/stars/cfeng16/audio-visual-forensics)](https://github.com/cfeng16/audio-visual-forensics)  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.01767-b31b1b.svg)](http://arxiv.org/abs/2301.01767) | :heavy_minus_sign: |
| ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding | [![GitHub](https://img.shields.io/github/stars/salesforce/ULIP)](https://github.com/salesforce/ULIP)|[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_ULIP_Learning_a_Unified_Representation_of_Language_Images_and_Point_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.05171-b31b1b.svg)](https://arxiv.org/abs/2212.05171) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dgFYBVmeilk) |
| AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR |  |  |  |
| Watch or Listen: Robust Audio-Visual Speech Recognition with Visual Corruption Modeling and Reliability Scoring |  |  |  |
| SceneTrilogy: On Human Scene-Sketch and its Complementarity with Photo and Text |  |  |  |
| Exploring and Exploiting Uncertainty for Incomplete Multi-View Classification |  |  |  |
| EXIF as Language: Learning Cross-Modal Associations between Images and Camera Metadata |  |  |  |
| Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens |  |  |  |
| RONO: Robust Discriminative Learning with Noisy Labels for 2D-3D Cross-Modal Retrieval|  |  |  |
| CASP-Net: Rethinking Video Saliency Prediction from an Audio-Visual Consistency Perceptual Perspective |  |  |  |
| Learning Audio-Visual Source Localization via False Negative Aware Contrastive Learning |  |  |  |
| ReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Regeneration |  |  |  |
| Look, Radiate, and Learn: Self-Supervised Localisation via Radio-Visual Correspondence |  |  |  |
| Learning Emotion Representations from Verbal and Nonverbal Communication |  |  |  |
| Enhanced Multimodal Representation Learning with Cross-Modal KD |  |  |  |
| MELTR: Meta Loss Transformer for Learning to Fine-Tune Video Foundation Models |  |  |  |
| Multilateral Semantic Relations Modeling for Image Text Retrieval |  |  |  |
| GeoVLN: Learning Geometry-enhanced Visual Representation with Slot Attention for Vision-and-Language Navigation |  |  |  |
| Noisy Correspondence Learning with Meta Similarity Correction |  |  |  |
| Improving Cross-Modal Retrieval with Set of Diverse Embeddings |  |  |  |
| Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment |  |  |  |
| MaPLe: Multi-Modal Prompt Learning |  |  |  |
| Fine-grained Image-Text Matching by Cross-Modal Hard Aligning Network |  |  |  |
| Towards Modality-Agnostic Person Re-Identification with Descriptive Query |  |  |  |
| Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos |  |  |  |
| FashionSAP: Symbols and Attributes Prompt for Fine-grained Fashion Vision-Language Pre-Training |  |  |  |
| MAP: Multimodal Uncertainty-Aware Vision-Language Pre-Training Model |  |  |  |
| Egocentric Auditory Attention Localization in Conversations |  |  |  |
| Improving Zero-Shot Generalization and Robustness of Multi-Modal Models |  |  |  |
| Understanding and Constructing Latent Modality Structures in Multi-Modal Representation Learning |  |  |  |
| Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles |  |  |  |
| GCFAgg: Global and Cross-View Feature Aggregation for Multi-View Clustering |  |  |  |
| BiCro: Noisy Correspondence Rectification for Multi-Modality Data via Bi-Directional Cross-Modal Similarity Consistency |  |  |  |
| DisCo-CLIP: A Distributed Contrastive Loss for Memory Efficient CLIP Training |  |  |  |
| Referring Image Matting |  |  |  |
| Leveraging per Image-Token Consistency for Vision-Language Pre-Training |  |  |  |
| Seeing what You Miss: Vision-Language Pre-Training with Semantic Completion Learning |  |  |  |
| Sample-Level Multi-View Graph Clustering |  |  |  |
| SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation |  |  |  |
| On the Effects of Self-Supervision and Contrastive Alignment in Deep Multi-View Clustering |  |  |  |
| SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model |  |  |  |
| Novel-View Acoustic Synthesis |  |  |  |
| MAGVLT: Masked Generative Vision-and-Language Transformer |  |  |  |
| Reproducible Scaling Laws for Contrastive Language-Image Learning |  |  |  |
| PMR: Prototypical Modal Rebalance for Multimodal Learning |  |  |  |
| Language-guided Music Recommendation for Video via Prompt Analogies |  |  |  |
| RA-CLIP: Retrieval Augmented Contrastive Language-Image Pre-Training |  |  |  |
| <i>MMG-Ego4D</i>: <i>M</i>ulti-<i>M</i>odal <i>G</i>eneralization in Egocentric Action Recognition |  |  |  |
| Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning |  |  |  |
| PRISE: Demystifying Deep Lucas-Kanade with Strongly Star-Convex Constraints for Multimodel Image Alignment |  |  |  |
| Masked Autoencoding Does Not Help Natural Language Supervision at Scale |  |  |  |
| CLIPPO: Image-and-Language Understanding from Pixels Only |  |  |  |
| Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations |  |  |  |
| Critical Learning Periods for Multisensory Integration in Deep Networks |  |  |  |
| CLIPPING: Distilling CLIP-based Models with a Student base for Video-Language Retrieval |  |  |  |
| NUWA-LIP: Language-guided Image Inpainting with Defect-Free VQGAN |  |  |  |
| WINNER: Weakly-Supervised hIerarchical DecompositioN and aligNment for spatio-tEmporal Video gRounding |  |  |  |
| Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation |  |  |  |
