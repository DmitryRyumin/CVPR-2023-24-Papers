# CVPR-2023-Papers

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/self-supervised-or-unsupervised-representation-learning.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/scene-analysis-and-understanding.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Datasets and Evaluation

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Large-Scale Training Data Search for Object Re-Identification | [![GitHub](https://img.shields.io/github/stars/yorkeyao/SnP)](https://github.com/yorkeyao/SnP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Large-Scale_Training_Data_Search_for_Object_Re-Identification_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16186-b31b1b.svg)](http://arxiv.org/abs/2303.16186) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OAZ0Pka2mKE) |
| Class Prototypes based Contrastive Learning for Classifying Multi-Label and Fine-grained Educational Videos | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Class_Prototypes_Based_Contrastive_Learning_for_Classifying_Multi-Label_and_Fine-Grained_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EhIeZtBB8bk) |
| V2X-Seq: A Large-Scale Sequential Dataset for Vehicle-Infrastructure Cooperative Perception and Forecasting | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://thudair.baai.ac.cn/index) <br /> [![GitHub](https://img.shields.io/github/stars/AIR-THU/DAIR-V2X-Seq)](https://github.com/AIR-THU/DAIR-V2X-Seq) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_V2X-Seq_A_Large-Scale_Sequential_Dataset_for_Vehicle-Infrastructure_Cooperative_Perception_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.05938-b31b1b.svg)](http://arxiv.org/abs/2305.05938) | :heavy_minus_sign: |
| NewsNet: A Novel Dataset for Hierarchical Temporal Segmentation | [![GitHub](https://img.shields.io/github/stars/NewsNet-Benchmark/NewsNet)](https://github.com/NewsNet-Benchmark/NewsNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_NewsNet_A_Novel_Dataset_for_Hierarchical_Temporal_Segmentation_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dEGco30TBCk) |
| CLOTH4D: A Dataset for Clothed Human Reconstruction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/AemikaChow/CLOTH4D) <br /> [![GitHub](https://img.shields.io/github/stars/NewsNet-Benchmark/NewsNet)](https://github.com/AemikaChow/AiDLab-fAshIon-Data) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zou_CLOTH4D_A_Dataset_for_Clothed_Human_Reconstruction_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8Cc_kl55bFo) |
| Accelerating Dataset Distillation via Model Augmentation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Accelerating_Dataset_Distillation_via_Model_Augmentation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.06152-b31b1b.svg)](http://arxiv.org/abs/2212.06152) | :heavy_minus_sign: |
| ImageNet-E: Benchmarking Neural Network Robustness via Attribute Editing | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/alibaba/easyrobust/tree/main/benchmarks/imagenet-e) <br /> [![GitHub](https://img.shields.io/github/stars/alibaba/easyrobust)](https://github.com/alibaba/easyrobust) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_ImageNet-E_Benchmarking_Neural_Network_Robustness_via_Attribute_Editing_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17096-b31b1b.svg)](http://arxiv.org/abs/2303.17096) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=66Be-LDVHbc) |
| Visual Atoms: Pre-Training Vision Transformers with Sinusoidal Waves | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://masora1030.github.io/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves/) <br /> [![GitHub](https://img.shields.io/github/stars/masora1030/CVPR2023-FDSL-on-VisualAtom)](https://github.com/masora1030/CVPR2023-FDSL-on-VisualAtom) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Takashima_Visual_Atoms_Pre-Training_Vision_Transformers_With_Sinusoidal_Waves_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.01112-b31b1b.svg)](http://arxiv.org/abs/2303.01112) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2reoDrFf0OA) |
| Infinite Photorealistic Worlds using Procedural Generation | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://infinigen.org/) <br /> [![GitHub](https://img.shields.io/github/stars/princeton-vl/infinigen)](https://github.com/princeton-vl/infinigen) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Raistrick_Infinite_Photorealistic_Worlds_Using_Procedural_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.09310-b31b1b.svg)](http://arxiv.org/abs/2306.09310) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6tgspeI-GHY) |
| CelebV-Text: A Large-Scale Facial Text-Video Dataset | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://celebv-text.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/celebv-text/CelebV-Text)](https://github.com/celebv-text/CelebV-Text) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_CelebV-Text_A_Large-Scale_Facial_Text-Video_Dataset_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14717-b31b1b.svg)](http://arxiv.org/abs/2303.14717) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0TS1hQwjNWw) |
| Spring: A High-Resolution High-Detail Dataset and Benchmark for Scene Flow, Optical Flow and Stereo | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://spring-benchmark.org/) <br /> [![GitHub](https://img.shields.io/github/stars/cv-stuttgart/sceneflow_from_blender)](https://github.com/cv-stuttgart/sceneflow_from_blender) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Mehl_Spring_A_High-Resolution_High-Detail_Dataset_and_Benchmark_for_Scene_Flow_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.01943-b31b1b.svg)](http://arxiv.org/abs/2303.01943) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=omcntkTrFTg) |
| Connecting Vision and Language with Video Localized Narratives | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://google.github.io/video-localized-narratives/) <br /> [![GitHub](https://img.shields.io/github/stars/google/video-localized-narratives)](https://github.com/google/video-localized-narratives) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Voigtlaender_Connecting_Vision_and_Language_With_Video_Localized_Narratives_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.11217-b31b1b.svg)](http://arxiv.org/abs/2302.11217) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=j1LUa-Cd4L8) |
| Towards Artistic Image Aesthetics Assessment: A Large-Scale Dataset and a New Method | [![GitHub](https://img.shields.io/github/stars/Dreemurr-T/BAID)](https://github.com/Dreemurr-T/BAID) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_Towards_Artistic_Image_Aesthetics_Assessment_A_Large-Scale_Dataset_and_a_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15166-b31b1b.svg)](http://arxiv.org/abs/2303.15166) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=U1h4S7J2xnw) |
| MD-VQA: Multi-Dimensional Quality Assessment for UGC Live Videos | [![GitHub](https://img.shields.io/github/stars/zzc-1998/MD-VQA)](https://github.com/zzc-1998/MD-VQA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_MD-VQA_Multi-Dimensional_Quality_Assessment_for_UGC_Live_Videos_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14933-b31b1b.svg)](http://arxiv.org/abs/2303.14933) | :heavy_minus_sign: |
| Toward RAW Object Detection: A New Benchmark and a New Model | [![Gitee Page](https://img.shields.io/badge/Gitee-Page-303643.svg)](https://gitee.com//mindspore/models/tree/master/research/cv/RAOD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Toward_RAW_Object_Detection_A_New_Benchmark_and_a_New_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dyudIByvYKc) |
| Objaverse: A Universe of Annotated 3D Objects | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://objaverse.allenai.org/) <br /> [![GitHub](https://img.shields.io/github/stars/allenai/objaverse-xl)](https://github.com/allenai/objaverse-xl) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Deitke_Objaverse_A_Universe_of_Annotated_3D_Objects_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08051-b31b1b.svg)](http://arxiv.org/abs/2212.08051) | :heavy_minus_sign: |
| Habitat-Matterport 3D Semantics Dataset | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://aihabitat.org/datasets/hm3d-semantics/) <br /> [![GitHub](https://img.shields.io/github/stars/matterport/habitat-matterport-3dresearch)](https://github.com/matterport/habitat-matterport-3dresearch) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yadav_Habitat-Matterport_3D_Semantics_Dataset_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.05633-b31b1b.svg)](http://arxiv.org/abs/2210.05633) | :heavy_minus_sign: |
| Similarity Metric Learning for RGB-Infrared Group Re-Identification | [![GitHub](https://img.shields.io/github/stars/WhollyOat/CM-Group)](https://github.com/WhollyOat/CM-Group) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_Similarity_Metric_Learning_for_RGB-Infrared_Group_Re-Identification_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VUWEkJYvDO0) |
| MISC210K: A Large-Scale Dataset for Multi-Instance Semantic Correspondence | [![GitHub](https://img.shields.io/github/stars/YXSUNMADMAX/MISC210K)](https://github.com/YXSUNMADMAX/MISC210K) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_MISC210K_A_Large-Scale_Dataset_for_Multi-Instance_Semantic_Correspondence_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| WeatherStream: Light Transport Automation of Single Image Deweathering | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://visual.ee.ucla.edu/wstream.htm/) <br /> [![GitHub](https://img.shields.io/github/stars/UCLA-VMG/WeatherStream)](https://github.com/UCLA-VMG/WeatherStream) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_WeatherStream_Light_Transport_Automation_of_Single_Image_Deweathering_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=y2cQdAmegaY) |
| MobileBrick: Building LEGO for 3D Reconstruction on Mobile Devices | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://code.active.vision/MobileBrick/) <br /> [![GitHub](https://img.shields.io/github/stars/ActiveVisionLab/MobileBrick)](https://github.com/ActiveVisionLab/MobileBrick) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_MobileBrick_Building_LEGO_for_3D_Reconstruction_on_Mobile_Devices_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.01932-b31b1b.svg)](http://arxiv.org/abs/2303.01932) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0NzsB3rdXlY) |
| GeoNet: Benchmarking Unsupervised Adaptation Across Geographies | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tarun005.github.io/GeoNet/) <br /> [![GitHub](https://img.shields.io/github/stars/ViLab-UCSD/GeoNet)](https://github.com/ViLab-UCSD/GeoNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kalluri_GeoNet_Benchmarking_Unsupervised_Adaptation_Across_Geographies_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15443-b31b1b.svg)](http://arxiv.org/abs/2303.15443) | :heavy_minus_sign: |
| Logical Consistency and Greater Descriptive Power for Facial Hair Attribute Learning | [![GitHub](https://img.shields.io/github/stars/HaiyuWu/LogicalConsistency)](https://github.com/HaiyuWu/LogicalConsistency) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Logical_Consistency_and_Greater_Descriptive_Power_for_Facial_Hair_Attribute_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.11102-b31b1b.svg)](http://arxiv.org/abs/2302.11102) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Pmpe3jJu8zQ) |
| PACO: Parts and Attributes of Common Objects | [![GitHub](https://img.shields.io/github/stars/facebookresearch/paco)](https://github.com/facebookresearch/paco) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ramanathan_PACO_Parts_and_Attributes_of_Common_Objects_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.01795-b31b1b.svg)](http://arxiv.org/abs/2301.01795) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=sIDX_2W9Wc8) |
| Understanding Deep Generative Models with Generalized Empirical Likelihoods | [![GitHub](https://img.shields.io/github/stars/google-deepmind/understanding_deep_generative_models_with_generalized_empirical_likelihood)](https://github.com/google-deepmind/understanding_deep_generative_models_with_generalized_empirical_likelihood) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ravuri_Understanding_Deep_Generative_Models_With_Generalized_Empirical_Likelihoods_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.09780-b31b1b.svg)](http://arxiv.org/abs/2306.09780) | :heavy_minus_sign: |
| BEDLAM: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://bedlam.is.tue.mpg.de/) <br /> [![GitHub](https://img.shields.io/github/stars/pixelite1201/BEDLAM)](https://github.com/pixelite1201/BEDLAM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Black_BEDLAM_A_Synthetic_Dataset_of_Bodies_Exhibiting_Detailed_Lifelike_Animated_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.16940-b31b1b.svg)](http://arxiv.org/abs/2306.16940) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OBttHFwdtfI) |
| Unicode Analogies: An Anti-Objectivist Visual Reasoning Challenge | [![GitHub](https://img.shields.io/github/stars/SvenShade/UnicodeAnalogies)](https://github.com/SvenShade/UnicodeAnalogies) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Spratley_Unicode_Analogies_An_Anti-Objectivist_Visual_Reasoning_Challenge_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| A New Comprehensive Benchmark for Semi-Supervised Video Anomaly Detection and Anticipation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://campusvad.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/zugexiaodui/campus_vad_code)](https://github.com/zugexiaodui/campus_vad_code) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_A_New_Comprehensive_Benchmark_for_Semi-Supervised_Video_Anomaly_Detection_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.13611-b31b1b.svg)](http://arxiv.org/abs/2305.13611) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kdLapd4rBCc) |
| An In-Depth Exploration of Person Re-Identification and Gait Recognition in Cloth-Changing Conditions | [![GitHub](https://img.shields.io/github/stars/BNU-IVC/CCPG)](https://github.com/BNU-IVC/CCPG) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_An_In-Depth_Exploration_of_Person_Re-Identification_and_Gait_Recognition_in_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Otani_Toward_Verifiable_and_Reproducible_Human_Evaluation_for_Text-to-Image_Generation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01816-b31b1b.svg)](http://arxiv.org/abs/2304.01816) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=65QMukifH60) |
| BiasBed - Rigorous Texture Bias Evaluation | [![GitHub](https://img.shields.io/github/stars/D1noFuzi/BiasBed)](https://github.com/D1noFuzi/BiasBed) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kalischek_BiasBed_-_Rigorous_Texture_Bias_Evaluation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13190-b31b1b.svg)](http://arxiv.org/abs/2211.13190) | :heavy_minus_sign: |
| A Large-Scale Homography Benchmark | [![GitHub](https://img.shields.io/github/stars/danini/homography-benchmark)](https://github.com/danini/homography-benchmark) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Barath_A_Large-Scale_Homography_Benchmark_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.09997-b31b1b.svg)](http://arxiv.org/abs/2302.09997) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8RmTlekfKGY) |
| Exploring and Utilizing Pattern Imbalance | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Mei_Exploring_and_Utilizing_Pattern_Imbalance_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=w6AUguS5eQQ) |
| Full or Weak Annotations? An Adaptive Strategy for Budget-constrained Annotation Campaigns | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tejero_Full_or_Weak_Annotations_An_Adaptive_Strategy_for_Budget-Constrained_Annotation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11678-b31b1b.svg)](http://arxiv.org/abs/2303.11678) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PixUJ9Xl5_8) |
| ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of Real World Objects |  |  |  |
| Open-Vocabulary Attribute Detection |  |  |  |
| Visual DNA: Representing and Comparing Images using Distributions of Neuron Activations |  |  |  |
| Blind Image Quality Assessment via Vision-Language Correspondence: A Multitask Learning Perspective |  |  |  |
| An Image Quality Assessment Dataset for Portraits |  |  |  |
| Multi-Sensor Large-Scale Dataset for Multi-View 3D Reconstruction |  |  |  |
| 3D-POP - An Automated Annotation Approach to Facilitate Markerless 2D-3D Tracking of Freely Moving Birds with Marker-based Motion Capture |  |  |  |
| Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation |  |  |  |
| Visual Localization using Imperfect 3D Models from the Internet |  |  |  |
| <i>Fantastic Breaks</i>: A Dataset of Paired 3D Scans of Real-World Broken Objects and Their Complete Counterparts |  |  |  |
| StarCraftImage: A Dataset for Prototyping Spatial Reasoning Methods for Multi-Agent Environments |  |  |  |
| MammalNet: A Large-Scale Video Benchmark for Mammal Recognition and Behavior Understanding |  |  |  |
| A Large-Scale Robustness Analysis of Video Action Recognition Models |  |  |  |
| Affection: Learning Affective Explanations for Real-World Visual Data |  |  |  |
| ShapeTalk: A Language Dataset and Framework for 3D Shape Edits and Deformations |  |  |  |
| Deep Depth Estimation from Thermal Image |  |  |  |
| DF-Platter: Multi-Face Heterogeneous Deepfake Dataset |  |  |  |
| A New Dataset based on Images Taken by Blind People for Testing the Robustness of Image Classification Models Trained for ImageNet Categories |  |  |  |
| RealImpact: A Dataset of Impact Sound Fields for Real Objects |  |  |  |
| NICO<sup>++</sup>: Towards Better Benchmarking for Domain Generalization |  |  |  |
