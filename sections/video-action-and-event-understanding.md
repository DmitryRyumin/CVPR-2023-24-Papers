# CVPR-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/medical-and-biological-vision-cell-microscopy.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/CVPR-2023-Papers/blob/main/sections/autonomous-driving.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Video: Action and Event Understanding

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Open Set Action Recognition via Multi-Label Evidential Learning | [![GitHub](https://img.shields.io/github/stars/charliezhaoyinpeng/mule?style=flat)](https://github.com/charliezhaoyinpeng/mule) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Open_Set_Action_Recognition_via_Multi-Label_Evidential_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12698-b31b1b.svg)](http://arxiv.org/abs/2303.12698) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=S185R1vT2Qk) |
| FLAG3D: A 3D Fitness Activity Dataset with Language Instruction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://andytang15.github.io/FLAG3D/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_FLAG3D_A_3D_Fitness_Activity_Dataset_With_Language_Instruction_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04638-b31b1b.svg)](http://arxiv.org/abs/2212.04638) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CgdRmk0BVvM) |
| MoLo: Motion-augmented Long-Short Contrastive Learning for Few-Shot Action Recognition | [![GitHub](https://img.shields.io/github/stars/alibaba-mmai-research/MoLo?style=flat)](https://github.com/alibaba-mmai-research/MoLo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MoLo_Motion-Augmented_Long-Short_Contrastive_Learning_for_Few-Shot_Action_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00946-b31b1b.svg)](http://arxiv.org/abs/2304.00946) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Ig1HGTrrA54) |
| The Wisdom of Crowds: Temporal Progressive Attention for Early Action Prediction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://alexandrosstergiou.github.io/project_pages/TemPr/index.html) <br /> [![GitHub](https://img.shields.io/github/stars/alexandrosstergiou/progressive-action-prediction?style=flat)](https://github.com/alexandrosstergiou/progressive-action-prediction) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Stergiou_The_Wisdom_of_Crowds_Temporal_Progressive_Attention_for_Early_Action_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2204.13340-b31b1b.svg)](http://arxiv.org/abs/2204.13340) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dcmd8U47BT8) |
| Use Your Head: Improving Long-Tail Video Recognition | [![GitHub](https://img.shields.io/github/stars/tobyperrett/lmr?style=flat)](https://github.com/tobyperrett/lmr) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Perrett_Use_Your_Head_Improving_Long-Tail_Video_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01143-b31b1b.svg)](http://arxiv.org/abs/2304.01143) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TXEMh99Ukmg) |
| Decomposed Cross-Modal Distillation for RGB-based Temporal Action Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Decomposed_Cross-Modal_Distillation_for_RGB-Based_Temporal_Action_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17285-b31b1b.svg)](http://arxiv.org/abs/2303.17285) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2upFWX7NVqc) |
| Video Test-Time Adaptation for Action Recognition | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://wlin-at.github.io/vitta) <br /> [![GitHub](https://img.shields.io/github/stars/wlin-at/ViTTA?style=flat)](https://github.com/wlin-at/ViTTA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Video_Test-Time_Adaptation_for_Action_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.15393-b31b1b.svg)](http://arxiv.org/abs/2211.15393) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=RzdYgE1hN2o) |
| How Can Objects Help Action Recognition? | [![GitHub](https://img.shields.io/github/stars/google-research/scenic?style=flat)](https://github.com/google-research/scenic) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_How_Can_Objects_Help_Action_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.11726-b31b1b.svg)](http://arxiv.org/abs/2306.11726) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=4E_X1hCj4yU) |
| Text-Visual Prompting for Efficient 2D Temporal Video Grounding | [![GitHub](https://img.shields.io/github/stars/intel/TVP?style=flat)](https://github.com/intel/TVP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Text-Visual_Prompting_for_Efficient_2D_Temporal_Video_Grounding_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.04995-b31b1b.svg)](http://arxiv.org/abs/2303.04995) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zj2s_G3066s) |
| Enlarging Instance-Specific and Class-Specific Information for Open-Set Action Recognition | [![GitHub](https://img.shields.io/github/stars/Jun-CEN/PSL?style=flat)](https://github.com/Jun-CEN/PSL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Cen_Enlarging_Instance-Specific_and_Class-Specific_Information_for_Open-Set_Action_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15467-b31b1b.svg)](http://arxiv.org/abs/2303.15467) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=SofkzNeymP4) |
| TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition | [![GitHub](https://img.shields.io/github/stars/DAVEISHAN/TimeBalance?style=flat)](https://github.com/DAVEISHAN/TimeBalance) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Dave_TimeBalance_Temporally-Invariant_and_Temporally-Distinctive_Video_Representations_for_Semi-Supervised_Action_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16268-b31b1b.svg)](http://arxiv.org/abs/2303.16268) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2c5LM6YqPKQ) |
| Learning Video Representations from Large Language Models <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://facebookresearch.github.io/LaViLa/) <br /> [![GitHub](https://img.shields.io/github/stars/facebookresearch/LaViLa?style=flat)](https://github.com/facebookresearch/LaViLa) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Learning_Video_Representations_From_Large_Language_Models_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04501-b31b1b.svg)](http://arxiv.org/abs/2212.04501) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tbQaP07xQ4c) |
| Fine-tuned CLIP Models are Efficient Video Learners | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://muzairkhattak.github.io/ViFi-CLIP/) <br /> [![GitHub](https://img.shields.io/github/stars/muzairkhattak/ViFi-CLIP?style=flat)](https://github.com/muzairkhattak/ViFi-CLIP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Rasheed_Fine-Tuned_CLIP_Models_Are_Efficient_Video_Learners_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03640-b31b1b.svg)](http://arxiv.org/abs/2212.03640) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=uqPLPIyWBb0) |
| Efficient Movie Scene Detection Using State-Space Transformers | [![GitHub](https://img.shields.io/github/stars/md-mohaiminul/TranS4mer?style=flat)](https://github.com/md-mohaiminul/TranS4mer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Islam_Efficient_Movie_Scene_Detection_Using_State-Space_Transformers_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.14427-b31b1b.svg)](http://arxiv.org/abs/2212.14427) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EOmVAByPQbE) |
| AdamsFormer for Spatial Action Localization in the Future | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chi_AdamsFormer_for_Spatial_Action_Localization_in_the_Future_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PK0O-ynPgr0) |
| A Light Weight Model for Active Speaker Detection | [![GitHub](https://img.shields.io/github/stars/Junhua-Liao/Light-ASD?style=flat)](https://github.com/Junhua-Liao/Light-ASD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liao_A_Light_Weight_Model_for_Active_Speaker_Detection_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.04439-b31b1b.svg)](http://arxiv.org/abs/2303.04439) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=spGacmYdvYs) |
| System-Status-Aware Adaptive Network for Online Streaming Video Understanding | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Foo_System-Status-Aware_Adaptive_Network_for_Online_Streaming_Video_Understanding_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15742-b31b1b.svg)](http://arxiv.org/abs/2303.15742) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8DrTkS247xs) |
| STMixer: A One-Stage Sparse Action Detector | [![GitHub](https://img.shields.io/github/stars/MCG-NJU/STMixer?style=flat)](https://github.com/MCG-NJU/STMixer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wu_STMixer_A_One-Stage_Sparse_Action_Detector_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15879-b31b1b.svg)](http://arxiv.org/abs/2303.15879) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Sy4jozsQLM0) |
| Revisiting Temporal Modeling for CLIP-Based Image-to-Video Knowledge Transferring | [![GitHub](https://img.shields.io/github/stars/farewellthree/STAN?style=flat)](https://github.com/farewellthree/STAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_Revisiting_Temporal_Modeling_for_CLIP-Based_Image-to-Video_Knowledge_Transferring_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.11116-b31b1b.svg)](http://arxiv.org/abs/2301.11116) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kaDItcB1iFw) |
| Distilling Vision-Language Pre-Training To Collaborate With Weakly-Supervised Temporal Action Localization | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://voide1220.github.io/distillation_collaboration/) <br /> [![GitHub](https://img.shields.io/github/stars/ju-chen/Efficient-Prompt?style=flat)](https://github.com/ju-chen/Efficient-Prompt) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ju_Distilling_Vision-Language_Pre-Training_To_Collaborate_With_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09335-b31b1b.svg)](http://arxiv.org/abs/2212.09335) | :heavy_minus_sign: |
| Real-Time Multi-Person Eyeblink Detection in the Wild for Untrimmed Video | [![GitHub](https://img.shields.io/github/stars/wenzhengzeng/MPEblink?style=flat)](https://github.com/wenzhengzeng/MPEblink) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zeng_Real-Time_Multi-Person_Eyeblink_Detection_in_the_Wild_for_Untrimmed_Video_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16053-b31b1b.svg)](http://arxiv.org/abs/2303.16053) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ngME7dym0Uk) |
| Modeling Video As Stochastic Processes for Fine-Grained Video Representation Learning <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub](https://img.shields.io/github/stars/hengRUC/VSP?style=flat)](https://github.com/hengRUC/VSP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Modeling_Video_As_Stochastic_Processes_for_Fine-Grained_Video_Representation_Learning_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Re<sup>2</sup>TAL: <u>Re</u>wiring Pretrained Video Backbones for <u>Re</u>versible <u>T</u>emporal <u>A</u>ction <u>L</u>ocalization | [![GitHub](https://img.shields.io/github/stars/coolbay/Re2TAL?style=flat)](https://github.com/coolbay/Re2TAL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhao_Re2TAL_Rewiring_Pretrained_Video_Backbones_for_Reversible_Temporal_Action_Localization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14053-b31b1b.svg)](https://arxiv.org/abs/2211.14053) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Oa29cFo_nMY) |
| Learning Discriminative Representations for Skeleton Based Action Recognition | [![GitHub](https://img.shields.io/github/stars/zhysora/FR-Head?style=flat)](https://github.com/zhysora/FR-Head) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhou_Learning_Discriminative_Representations_for_Skeleton_Based_Action_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03729-b31b1b.svg)](http://arxiv.org/abs/2303.03729) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ix6rADaCjNs) |
| Learning Procedure-Aware Video Representation From Instructional Videos and Their Narrations | [![GitHub](https://img.shields.io/github/stars/facebookresearch/ProcedureVRL?style=flat)](https://github.com/facebookresearch/ProcedureVRL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Yu_Learning_Procedure-Aware_Video_Representation_From_Instructional_Videos_and_Their_Narrations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17839-b31b1b.svg)](http://arxiv.org/abs/2303.17839) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=YPq-rziL8Jo) |
| Collecting Cross-Modal Presence-Absence Evidence for Weakly-Supervised Audio-Visual Event Perception | [![GitHub](https://img.shields.io/github/stars/MengyuanChen21/CVPR2023-CMPAE?style=flat)](https://github.com/MengyuanChen21/CVPR2023-CMPAE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Gao_Collecting_Cross-Modal_Presence-Absence_Evidence_for_Weakly-Supervised_Audio-Visual_Event_Perception_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| PivoTAL: Prior-Driven Supervision for Weakly-Supervised Temporal Action Localization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Rizve_PivoTAL_Prior-Driven_Supervision_for_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6kAoQjXfzio) |
| Cascade Evidential Learning for Open-World Weakly-Supervised Temporal Action Localization | [![GitHub](https://img.shields.io/github/stars/zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation?style=flat)](https://github.com/zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Chen_Cascade_Evidential_Learning_for_Open-World_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Soft-Landing Strategy for Alleviating the Task Discrepancy Problem in Temporal Action Localization Tasks | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Kang_Soft-Landing_Strategy_for_Alleviating_the_Task_Discrepancy_Problem_in_Temporal_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.06023-b31b1b.svg)](http://arxiv.org/abs/2211.06023) | :heavy_minus_sign: |
| SVFormer: Semi-Supervised Video Transformer for Action Recognition | [![GitHub](https://img.shields.io/github/stars/ChenHsing/SVFormer?style=flat)](https://github.com/ChenHsing/SVFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xing_SVFormer_Semi-Supervised_Video_Transformer_for_Action_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13222-b31b1b.svg)](http://arxiv.org/abs/2211.13222) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6kAoQjXfzio) |
| AutoAD: Movie Description in Context <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.robots.ox.ac.uk/~vgg/research/autoad/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Han_AutoAD_Movie_Description_in_Context_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.16899-b31b1b.svg)](http://arxiv.org/abs/2303.16899) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=gMQSoib6lSI) |
| STMT: A Spatial-Temporal Mesh Transformer for MoCap-based Action Recognition | [![GitHub](https://img.shields.io/github/stars/zgzxy001/STMT?style=flat)](https://github.com/zgzxy001/STMT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhu_STMT_A_Spatial-Temporal_Mesh_Transformer_for_MoCap-Based_Action_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.18177-b31b1b.svg)](http://arxiv.org/abs/2303.18177) | :heavy_minus_sign: |
| Boosting Weakly-Supervised Temporal Action Localization with Text Information | [![GitHub](https://img.shields.io/github/stars/lgzlIlIlI/Boosting-WTAL?style=flat)](https://github.com/lgzlIlIlI/Boosting-WTAL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Li_Boosting_Weakly-Supervised_Temporal_Action_Localization_With_Text_Information_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.00607-b31b1b.svg)](http://arxiv.org/abs/2305.00607) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=n8p4ZU85LXM) |
| Aligning Step-by-Step Instructional Diagrams to Video Demonstrations| [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://academic.davidz.cn/en/publication/zhang-cvpr-2023/) <br /> [![GitHub](https://img.shields.io/github/stars/DavidZhang73/AssemblyVideoManualAlignment?style=flat)](https://github.com/DavidZhang73/AssemblyVideoManualAlignment) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_Aligning_Step-by-Step_Instructional_Diagrams_to_Video_Demonstrations_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13800-b31b1b.svg)](http://arxiv.org/abs/2303.13800) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8iC5QyP8U6o) |
| Improving Weakly Supervised Temporal Action Localization by Bridging Train-Test Gap in Pseudo Labels | [![GitHub](https://img.shields.io/github/stars/zhou745/GauFuse_WSTAL?style=flat)](https://github.com/zhou745/GauFuse_WSTAL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhou_Improving_Weakly_Supervised_Temporal_Action_Localization_by_Bridging_Train-Test_Gap_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.07978-b31b1b.svg)](http://arxiv.org/abs/2304.07978) | :heavy_minus_sign: |
| Weakly Supervised Video Representation Learning with Unaligned Text for Sequential Videos | [![GitHub](https://img.shields.io/github/stars/svip-lab/WeakSVR?style=flat)](https://github.com/svip-lab/WeakSVR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Dong_Weakly_Supervised_Video_Representation_Learning_With_Unaligned_Text_for_Sequential_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12370-b31b1b.svg)](http://arxiv.org/abs/2303.12370) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=AqozSRYP7Pc) |
| Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale Benchmark and Baseline | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://unav100.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/ttgeng233/UnAV?style=flat)](https://github.com/ttgeng233/UnAV) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Geng_Dense-Localizing_Audio-Visual_Events_in_Untrimmed_Videos_A_Large-Scale_Benchmark_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12930-b31b1b.svg)](http://arxiv.org/abs/2303.12930) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PPDywLMn1Js) |
| LOGO: A Long-Form Video Dataset for Group Action Quality Assessment | [![GitHub](https://img.shields.io/github/stars/shiyi-zh0408/LOGO?style=flat)](https://github.com/shiyi-zh0408/LOGO) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhang_LOGO_A_Long-Form_Video_Dataset_for_Group_Action_Quality_Assessment_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Search-Map-Search: A Frame Selection Paradigm for Action Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Zhao_Search-Map-Search_A_Frame_Selection_Paradigm_for_Action_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10316-b31b1b.svg)](https://arxiv.org/abs/2304.10316) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Ywdf6di2QWo) |
| 3Mformer: Multi-Order Multi-Mode Transformer for Skeletal Action Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_3Mformer_Multi-Order_Multi-Mode_Transformer_for_Skeletal_Action_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14474-b31b1b.svg)](http://arxiv.org/abs/2303.14474) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_LzrzFIuaNU) |
| ProTeGe: Untrimmed Pretraining for Video Temporal Grounding by Video Temporal Grounding | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Wang_ProTeGe_Untrimmed_Pretraining_for_Video_Temporal_Grounding_by_Video_Temporal_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Egocentric Video Task Translation <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://vision.cs.utexas.edu/projects/egot2/) <br /> [![GitHub](https://img.shields.io/github/stars/facebookresearch/EgoT2?style=flat)](https://github.com/facebookresearch/EgoT2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Xue_Egocentric_Video_Task_Translation_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.06301-b31b1b.svg)](https://arxiv.org/abs/2212.06301) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=HHWLMFIZ5ow) |
| Look Around for Anomalies: Weakly-Supervised Anomaly Detection via Context-Motion Relational Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Cho_Look_Around_for_Anomalies_Weakly-Supervised_Anomaly_Detection_via_Context-Motion_Relational_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=31ccYdwGDG8) |
| Proposal-based Multiple Instance Learning for Weakly-Supervised Temporal Action Localization | [![GitHub](https://img.shields.io/github/stars/RenHuan1999/CVPR2023_P-MIL?style=flat)](https://github.com/RenHuan1999/CVPR2023_P-MIL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Ren_Proposal-Based_Multiple_Instance_Learning_for_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.17861-b31b1b.svg)](https://arxiv.org/abs/2305.17861)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hfHGlKyOQ68) |
| TriDet: Temporal Action Detection with Relative Boundary Modeling | [![GitHub](https://img.shields.io/github/stars/dingfengshi/TriDet?style=flat)](https://github.com/dingfengshi/TriDet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Shi_TriDet_Temporal_Action_Detection_With_Relative_Boundary_Modeling_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07347-b31b1b.svg)](http://arxiv.org/abs/2303.07347)| [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=f1gJkUI6rA4) |
| Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-based Action Recognition <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://langlandslin.github.io/projects/ActCLR/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Lin_Actionlet-Dependent_Contrastive_Learning_for_Unsupervised_Skeleton-Based_Action_Recognition_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10904-b31b1b.svg)](http://arxiv.org/abs/2303.10904) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jwX0Zc8s10w) |
| EVAL: Explainable Video Anomaly Localization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Singh_EVAL_Explainable_Video_Anomaly_Localization_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.07900-b31b1b.svg)](http://arxiv.org/abs/2212.07900) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6x8GUDWkN68) |
| Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning | [![GitHub](https://img.shields.io/github/stars/daniel-code/TubeViT?style=flat)](https://github.com/daniel-code/TubeViT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Piergiovanni_Rethinking_Video_ViTs_Sparse_Video_Tubes_for_Joint_Image_and_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03229-b31b1b.svg)](http://arxiv.org/abs/2212.03229) | :heavy_minus_sign: |
| Weakly Supervised Temporal Sentence Grounding with Uncertainty-guided Self-Training | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Weakly_Supervised_Temporal_Sentence_Grounding_With_Uncertainty-Guided_Self-Training_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KyXVUlBjiZ4) |
| Leveraging Temporal Context in Low Representational Power Regimes | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://camilofosco.com/etm_website/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Fosco_Leveraging_Temporal_Context_in_Low_Representational_Power_Regimes_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=a5-gWsUMVXw) |
| PIVOT: Prompting for Video Continual Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Villa_PIVOT_Prompting_for_Video_Continual_Learning_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04842-b31b1b.svg)](http://arxiv.org/abs/2212.04842) | :heavy_minus_sign: |
| On the Benefits of 3D Pose and Tracking for Human Action Recognition | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://people.eecs.berkeley.edu/~jathushan/LART/) <br /> [![GitHub](https://img.shields.io/github/stars/brjathu/LART?style=flat)](https://github.com/brjathu/LART) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Rajasegaran_On_the_Benefits_of_3D_Pose_and_Tracking_for_Human_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01199-b31b1b.svg)](http://arxiv.org/abs/2304.01199) | :heavy_minus_sign: |
| NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://vision.cs.utexas.edu/projects/naq/) <br /> [![GitHub](https://img.shields.io/github/stars/srama2512/NaQ?style=flat)](https://github.com/srama2512/NaQ) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ramakrishnan_NaQ_Leveraging_Narrations_As_Queries_To_Supervise_Episodic_Memory_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.00746-b31b1b.svg)](http://arxiv.org/abs/2301.00746) | :heavy_minus_sign: |
| Selective Structured State-Spaces for Long-Form Video Understanding | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Selective_Structured_State-Spaces_for_Long-Form_Video_Understanding_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14526-b31b1b.svg)](http://arxiv.org/abs/2303.14526) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CcQHyaZLIpQ) |
| Frame Flexible Network | [![GitHub](https://img.shields.io/github/stars/BeSpontaneous/FFN-pytorch?style=flat)](https://github.com/BeSpontaneous/FFN-pytorch) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Frame_Flexible_Network_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14817-b31b1b.svg)](http://arxiv.org/abs/2303.14817) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JFHqyw20T8M) |
| ASPnet: Action Segmentation with Shared-Private Representation of Multiple Data Sources | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/van_Amsterdam_ASPnet_Action_Segmentation_With_Shared-Private_Representation_of_Multiple_Data_Sources_CVPR_2023_paper.pdf) | :heavy_minus_sign: |
| Unified Keypoint-based Action Recognition Framework via Structured Keypoint Pooling | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Hachiuma_Unified_Keypoint-Based_Action_Recognition_Framework_via_Structured_Keypoint_Pooling_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15270-b31b1b.svg)](http://arxiv.org/abs/2303.15270) | :heavy_minus_sign: |
| Learning Transferable Spatiotemporal Representations from Natural Script Knowledge | [![GitHub](https://img.shields.io/github/stars/TencentARC/TVTS?style=flat)](https://github.com/TencentARC/TVTS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_Learning_Transferable_Spatiotemporal_Representations_From_Natural_Script_Knowledge_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.15280-b31b1b.svg)](http://arxiv.org/abs/2209.15280) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Jlpe75bsYUg) |
| Masked Video Distillation: Rethinking Masked Feature Modeling for Self-Supervised Video Representation Learning | [![GitHub](https://img.shields.io/github/stars/ruiwang2021/mvd?style=flat)](https://github.com/ruiwang2021/mvd) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Masked_Video_Distillation_Rethinking_Masked_Feature_Modeling_for_Self-Supervised_Video_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04500-b31b1b.svg)](http://arxiv.org/abs/2212.04500) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=btxcb-5NfWo) |
| Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained Vision-Language Models | [![GitHub](https://img.shields.io/github/stars/whwu95/BIKE?style=flat)](https://github.com/whwu95/BIKE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Bidirectional_Cross-Modal_Knowledge_Exploration_for_Video_Recognition_With_Pre-Trained_Vision-Language_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.00182-b31b1b.svg)](http://arxiv.org/abs/2301.00182) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=cjEvOcN-bmI) |
| Procedure-Aware Pretraining for Instructional Video Understanding | [![GitHub](https://img.shields.io/github/stars/salesforce/paprika?style=flat)](https://github.com/salesforce/paprika) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Procedure-Aware_Pretraining_for_Instructional_Video_Understanding_CVPR_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.18230-b31b1b.svg)](http://arxiv.org/abs/2303.18230) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=NvuPQbhC3Qo) |
| Latency Matters: Real-Time Action Forecasting Transformer <br /> ![CVPR - Highlight](https://img.shields.io/badge/CVPR-Highlight-FFFF00) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://karttikeya.github.io/publication/RAFTformer/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/CVPR2023/papers/Girase_Latency_Matters_Real-Time_Action_Forecasting_Transformer_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0tbK36hzSM0) |
| Generating Anomalies for Video Anomaly Detection with Prompt-based Feature Mapping | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com//content/CVPR2023/papers/Liu_Generating_Anomalies_for_Video_Anomaly_Detection_With_Prompt-Based_Feature_Mapping_CVPR_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=iYGfdd3t4ZE) |
| HierVL: Learning Hierarchical Video-Language Embeddings |  |  |  |
| Two-Stream Networks for Weakly-Supervised Temporal Action Localization with Semantic-Aware Mechanisms |  |  |  |
| Hybrid Active Learning via Deep Clustering for Video Action Detection |  |  |  |
| Prompt-guided Zero-Shot Anomaly Action Recognition using Pretrained Deep Skeleton Features |  |  |  |
| Unbiased Multiple Instance Learning for Weakly Supervised Video Anomaly Detection |  |  |  |
| VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking |  |  |  |
| PDPP: Projected Diffusion for Procedure Planning in Instructional Videos |  |  |  |
| Learning Action Changes by Measuring Verb-Adverb Textual Relationships |  |  |  |
| Reducing the Label Bias for Timestamp Supervised Temporal Action Segmentation |  |  |  |
| Video Event Restoration based on Keyframes for Video Anomaly Detection |  |  |  |
| Active Exploration of Multimodal Complementarity for Few-Shot Action Recognition |  |  |  |
| Vita-CLIP: Video and Text Adaptive CLIP via Multimodal Prompting |  |  |  |
| Post-Processing Temporal Action Detection |  |  |  |
| Relational Space-Time Query in Long-Form Videos |  |  |  |
| Therbligs in Action: Video Understanding through Motion Primitives |  |  |  |
| Dual-Path Adaptation from Image to Video Transformers |  |  |  |
| Hierarchical Semantic Contrast for Scene-Aware Video Anomaly Detection |  |  |  |
| Exploiting Completeness and Uncertainty of Pseudo Labels for Weakly Supervised Video Anomaly Detection |  |  |  |
| Unbiased Scene Graph Generation in Videos |  |  |  |
